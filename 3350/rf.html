<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Random Forest Classifier Explained</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;600;700&display=swap" rel="stylesheet">
  <script>
    // MathJax v3 config
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$', '$']],
        processEscapes: true
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" defer></script>

  <style>
    *{margin:0;padding:0;box-sizing:border-box}
    body{font-family:'Roboto',Arial,sans-serif;background:#17191a;min-height:100vh;color:#ff79c6;padding:20px}
    section{margin:0 auto 55px auto;background:#222025;border-radius:18px;box-shadow:0 4px 30px rgba(255,121,198,.09);padding:36px 32px 22px 32px;border-left:7px solid #ff79c6;position:relative;max-width:1400px}
    h1{color:#ffb3de;margin:0 0 8px 0;text-align:center;font-size:2.5em}
    h2{margin:0 0 20px 0;color:#ffb3de;font-weight:700;font-size:1.8em}
    h3{font-size:1.18em;color:#ffb3de;margin-bottom:15px;font-weight:600}
    h4{font-size:1.05em;color:#ffd7f0;margin-bottom:10px;font-weight:600;margin-top:18px}
    p{color:#ffd7f0;line-height:1.8;margin-bottom:12px}
    .subtitle{text-align:center;color:#ffd7f0;opacity:.9;font-size:1.1em;margin-bottom:20px}
    .pill{display:inline-block;background:#19121a;border:1px solid #ff79c6;border-radius:999px;padding:4px 10px;margin:2px 8px 2px 0;font-size:.9em}
    .info-section{background:#2a252b;border-radius:12px;padding:25px;margin-top:30px;border:1px solid rgba(255,121,198,0.2)}
    .info-section h3{color:#ffb3de;margin-bottom:15px;font-size:1.3em;border-bottom:1px solid #ff79c6;padding-bottom:8px}
    .card{background:#2a252b;border:1px solid rgba(255,121,198,0.25);border-radius:10px;padding:16px;margin-top:12px}
    ul{margin-left:20px}
    li{margin:8px 0;color:#ffd7f0;line-height:1.7}
    ol{margin-left:20px}
    ol li{margin:10px 0}
    table{width:100%;border-collapse:collapse;color:#ffd7f0;margin-top:15px}
    th,td{border:1px solid rgba(255,121,198,0.2);padding:12px;text-align:left}
    th{color:#ffb3de;background:#1e1a20;font-weight:700}
    td:first-child{font-weight:600;color:#ffb3de;width:25%}
    .highlight{background:rgba(255,121,198,0.1);border-left:3px solid #ff79c6;padding:15px;margin:15px 0;border-radius:6px}
    .step{background:#19121a;border-radius:8px;padding:15px;margin:12px 0;border-left:4px solid #ff79c6}
    .step-title{color:#ffb3de;font-weight:700;margin-bottom:8px}
    b{color:#ffb3de}
    a{color:#8be9fd;text-decoration:underline;text-underline-offset:3px}
    a:hover{color:#ffb3de}
  </style>
</head>
<body>

<section>
  <center><h1><span class="pill">Random Forest Classifier</span></h1></center>
  <p class="subtitle">A comprehensive explanation of Random Forest - an ensemble learning method</p>
</section>

<section>
  <div class="info-section">
    <h3>What is Random Forest?</h3>

    <p>Random Forest is a <b>supervised machine learning algorithm</b> that builds multiple decision trees and combines their predictions to make a final decision. It's called "Random" because it uses randomness in two key ways: random data sampling and random feature selection.</p>

    <p>Think of it like asking 100 different experts to make a prediction and then taking the majority vote - the final answer is usually better than any single expert's opinion!</p>

    <div class="card">
      <b>Simple Definition:</b><br>
      Random Forest = Multiple Decision Trees + Bootstrap Sampling + Random Feature Selection + Voting/Averaging
    </div>
  </div>

</section>

<section>
  <h2>How Random Forest Works: Step-by-Step</h2>

  <div class="step">
    <div class="step-title">Step 1: Bootstrap Sampling (Creating Diverse Datasets)</div>
    <p>Instead of training all trees on the same dataset, Random Forest creates multiple random subsets of the training data by <b>sampling with replacement</b>. Each tree gets a slightly different dataset.</p>
    <p><b>Example:</b> If you have 1000 training samples, you randomly pick 1000 samples with replacement multiple times, creating different combinations for each tree.</p>
  </div>

  <div class="step">
    <div class="step-title">Step 2: Build Multiple Decision Trees</div>
    <p>Each tree is trained on its own bootstrapped dataset. However, unlike a single decision tree, there's an important difference: <b>not all features are considered at each split</b>.</p>
    <p>Instead, at each node, only a <b>random subset of features</b> is evaluated to find the best split. This adds more randomness and prevents correlation between trees.</p>
  </div>

  <div class="step">
    <div class="step-title">Step 3: Make Predictions from Each Tree</div>
    <p>Once all trees are trained, each tree makes its own prediction for a new sample.</p>
    <p><b>For Classification:</b> Each tree votes (predicts a class)</p>
    <p><b>For Regression:</b> Each tree predicts a numeric value</p>
  </div>

  <div class="step">
    <div class="step-title">Step 4: Aggregate Predictions (Voting/Averaging)</div>
    <p><b>Classification:</b> The final prediction is the <b>majority vote</b> - the class that most trees predicted</p>
    <p><b>Regression:</b> The final prediction is the <b>average</b> of all tree predictions</p>
    <p><b>Example:</b> If you have 10 trees and 6 predict "Yes" and 4 predict "No", the final prediction is "Yes".</p>
  </div>

  <div class="card" style="margin-top:20px">
    <b>Classification Formula:</b><br>
    $\text{Final Prediction} = \text{argmax}(\text{count of votes for each class})$<br><br>
    <b>Regression Formula:</b><br>
    $\text{Final Prediction} = \frac{1}{N} \sum_{i=1}^{N} \text{Tree}_i \text{(prediction)}$<br>
    <span style="font-size:0.9em;color:#ffd7f0">where N is the number of trees</span>
  </div>
</section>

<section>
  <h2>Key Concepts</h2>

  <div class="info-section">
    <h3>1. Bootstrap Aggregating (Bagging)</h3>
    <p>Bootstrap Aggregating, or "Bagging," is the technique of:</p>
    <ol>
      <li>Creating multiple random subsets of the training data (with replacement)</li>
      <li>Training a model on each subset</li>
      <li>Combining predictions from all models</li>
    </ol>
    <p><b>Why it works:</b> By training on different data samples, each tree learns different patterns. When combined, these diverse trees produce more robust predictions than a single tree.</p>
  </div>

  <div class="info-section">
    <h3>2. Random Feature Selection</h3>
    <p>At each node of a tree, Random Forest doesn't consider all features. Instead, it randomly selects a subset of features and chooses the best split among only those features.</p>
    <p><b>Typical subset size:</b></p>
    <ul>
      <li><b>Classification:</b> âˆš(total number of features)</li>
      <li><b>Regression:</b> total features / 3</li>
    </ul>
    <p><b>Why it works:</b> This reduces correlation between trees and makes the ensemble more effective. Without this, all trees might make similar mistakes.</p>
  </div>

  <div class="info-section">
    <h3>3. Out-of-Bag (OOB) Error</h3>
    <p>Since bootstrap sampling creates subsets <b>with replacement</b>, approximately 1/3 of the original data is not included in each bootstrap sample. This data is called "Out-of-Bag" data.</p>
    <p>Random Forest can use this OOB data to estimate model performance without needing a separate test set!</p>
  </div>
</section>

<section>
  <h2>Random Forest vs Decision Tree</h2>

  <table>
    <thead>
      <tr>
        <th>Aspect</th>
        <th>Decision Tree</th>
        <th>Random Forest</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Model Type</td>
        <td>Single tree</td>
        <td>Multiple trees (ensemble)</td>
      </tr>
      <tr>
        <td>Training Data</td>
        <td>Entire dataset</td>
        <td>Bootstrap samples of dataset</td>
      </tr>
      <tr>
        <td>Features per Split</td>
        <td>All features considered</td>
        <td>Random subset of features</td>
      </tr>
      <tr>
        <td>Overfitting Risk</td>
        <td>High (easily overfits)</td>
        <td>Low (ensemble reduces overfitting)</td>
      </tr>
      <tr>
        <td>Accuracy</td>
        <td>Good but can vary</td>
        <td>Generally higher and more stable</td>
      </tr>
      <tr>
        <td>Interpretability</td>
        <td>High (easy to visualize)</td>
        <td>Low (100s of trees hard to interpret)</td>
      </tr>
      <tr>
        <td>Training Speed</td>
        <td>Fast</td>
        <td>Slower (trains multiple trees)</td>
      </tr>
      <tr>
        <td>Prediction Speed</td>
        <td>Very Fast</td>
        <td>Slower (predicts with all trees)</td>
      </tr>
    </tbody>
  </table>
</section>


<section>
  <h2>Important Hyperparameters</h2>

  <table>
    <thead>
      <tr>
        <th>Parameter</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>n_estimators</td>
        <td>Number of trees in the forest. More trees generally improve performance but increase computation time</td>
      </tr>
      <tr>
        <td>max_depth</td>
        <td>Maximum depth of each tree. Lower values reduce overfitting</td>
      </tr>
      <tr>
        <td>min_samples_split</td>
        <td>Minimum samples required to split a node. Higher values reduce overfitting</td>
      </tr>
      <tr>
        <td>min_samples_leaf</td>
        <td>Minimum samples required at a leaf node. Prevents very small leaves</td>
      </tr>
      <tr>
        <td>max_features</td>
        <td>Number of features to consider at each split. Options: 'sqrt', 'log2', or a number</td>
      </tr>
      <tr>
        <td>bootstrap</td>
        <td>Whether to use bootstrap sampling. Default is True</td>
      </tr>
      <tr>
        <td>random_state</td>
        <td>Seed for reproducibility</td>
      </tr>
    </tbody>
  </table>
</section>

<section>
  <h2>When to Use Random Forest</h2>

  <div class="highlight">
    <b>Good for Random Forest:</b>
    <ul style="margin-top:10px">
      <li>Non-linear relationships in data</li>
      <li>Large datasets with many features</li>
      <li>When accuracy is more important than interpretability</li>
      <li>Problems with mixed feature types (numerical and categorical)</li>
      <li>When you want automatic feature importance ranking</li>
      <li>Binary and multi-class classification problems</li>
    </ul>
  </div>

  <div class="highlight">
    <b>Not ideal for Random Forest:</b>
    <ul style="margin-top:10px">
      <li>When model interpretability is critical (use decision tree instead)</li>
      <li>Very small datasets (overfitting risk for single tree still applies)</li>
      <li>Real-time prediction requirements (needs multiple tree evaluations)</li>
      <li>When memory is severely limited</li>
      <li>Simple linear relationships (faster methods like logistic regression may suffice)</li>
    </ul>
  </div>
</section>

<section>
  <h2>Learning Resources</h2>

  <ul>
    <li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" target="_blank">Scikit-learn Random Forest Documentation</a></li>
  </ul>
</section>

</body>
</html>
