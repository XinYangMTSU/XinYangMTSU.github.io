<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Worked Example: OLS vs Ridge vs Lasso (High-Dimensional Breast Cancer)</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <style>
   :root{
      --bg: #17191a;
      --card: #222025;
      --ink: #ff79c6;
      --ink-soft: #ffb3de;
      --ink-dim: #caa0c9;
      --chip: #21111b;
   }
   *{box-sizing:border-box}
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      background: var(--bg);
      min-height: 100vh;
      color: var(--ink);
      padding: 20px;
    }
    section {
      margin-bottom: 55px;
      background: var(--card);
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid var(--ink);
      position: relative;
      max-width: 980px;
      margin-left: auto;
      margin-right: auto;
    }
    section:hover { box-shadow: 0 8px 60px rgba(255,121,198,.15); }

    h1, h2, h3, h4 {
      margin: 0 0 10px 0;
      color: var(--ink-soft);
      font-weight: 700;
    }
    h1{font-size: 1.9rem}
    .subtitle{color: var(--ink-dim); margin-top: 6px; font-size: .98rem}

    section h4 {
      font-size: 1.12em;
      color: var(--ink-soft);
      margin-bottom: 8px;
      margin-top: 28px;
      font-weight: 700;
      border-bottom: 1px solid var(--ink-soft);
      padding-bottom: 6px;
    }

    .pill {
      display:inline-block;
      padding: 6px 10px;
      border: 1px solid var(--ink);
      background: var(--chip);
      color: var(--ink-soft);
      border-radius: 999px;
      font-size: .85rem;
      margin-right: 6px;
      margin-top: 6px;
    }

    pre {
      background: #19121a;
      color: #ffd3ee;
      padding: 16px 18px 18px 18px;
      border-radius: 10px;
      font-size: 0.98em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
      position: relative;
      max-width: 100%;
      border: 1px solid rgba(255,121,198,.25);
    }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; }

    .copy {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 6px 10px;
      background: var(--chip);
      border: 1px solid var(--ink);
      border-radius: 8px;
      color: var(--ink-soft);
      font-size: 0.82em;
      cursor: pointer;
    }
    .copy:hover { background: var(--ink); color: #111; }

    ul, ol { margin-top: 6px; margin-bottom: 4px; }
    li { margin: 6px 0; }

    .note{
      background:#19121a; border:1px dashed rgba(255,121,198,.35);
      padding:10px 12px; border-radius:10px; color:#ffd3ee; font-size:.96rem
    }

    .grid{
      display:grid; grid-template-columns: repeat(auto-fit,minmax(220px,1fr));
      gap:10px; margin-top:10px
    }
    .chip{
      padding:10px 12px; border-radius:12px; background:#1b141c; border:1px solid rgba(255,121,198,.25);
      color:#ffd3ee; font-size:.95rem
    }
  </style>
</head>

<body>

<section>
  <center>
    <h1>Worked Example: OLS vs Ridge vs Lasso</h1>
    <div class="subtitle">High-Dimensional Breast Cancer Data (Polynomial Expansion, degree = 3)</div>
  </center>

  <h4> Method: OLS vs Ridge vs Lasso with PolynomialFeatures (degree = 3) </h4>
  
  <pre><button class="copy">Copy</button><code>
# ========================================================
# Breast Cancer Data — OLS vs Ridge vs Lasso (High-dimensional demo)
# Polynomial feature expansion, train/test split, compare
# ========================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load dataset
data = load_breast_cancer()
X, y = data.data, data.target   # binary labels (0/1)
n_features = X.shape[1]

# 2) Train / test split (before poly expansion!)
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=RANDOM_STATE
)

# 3) Polynomial feature expansion (degree=3)
# Includes x, x^2, x^3 and all cross-terms --> high dimensional
poly = PolynomialFeatures(degree=3)
X_train = poly.fit_transform(X_train_raw)  # (fit only on training data)
X_test  = poly.transform(X_test_raw)

print(f"Original features: {n_features} → After expansion: {X_train.shape[1]}")

# 4) Helper function to evaluate models
def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)

    # --- Train ---
    y_pred_train = model.predict(X_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    rmse_train = np.sqrt(mse_train)
    r2_train = r2_score(y_train, y_pred_train)
    sse_train = np.sum((y_train - y_pred_train) ** 2)

    # --- Test ---
    y_pred_test = model.predict(X_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    rmse_test = np.sqrt(mse_test)
    r2_test = r2_score(y_test, y_pred_test)
    sse_test = np.sum((y_test - y_pred_test) ** 2)

    # Collect results
    results = pd.DataFrame([
        {"Model": name, "Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Model": name, "Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ])
    return results

# 5) Define models
ols = Pipeline([
    ("scaler", StandardScaler()),
    ("linreg", LinearRegression())
])

ridge = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", RidgeCV(alphas=np.logspace(-3, 3, 30), cv=5))
])

lasso = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", LassoCV(alphas=np.logspace(-3, 3, 60), cv=5, max_iter=20000, random_state=RANDOM_STATE))
])

# 6) Evaluate models
results_df = pd.DataFrame(
    pd.concat([
        evaluate_model("OLS",   ols,   X_train, y_train, X_test, y_test),
        evaluate_model("Ridge", ridge, X_train, y_train, X_test, y_test),
        evaluate_model("Lasso", lasso, X_train, y_train, X_test, y_test),
    ])
).set_index(["Model", "Split"])

# 7) Display results + chosen alphas
print("\n=== Train/Test Metrics (high-dimensional case) ===")
print(results_df)

print("\nRidge alpha chosen:", ridge.named_steps["ridge"].alpha_)
print("Lasso alpha chosen:", lasso.named_steps["lasso"].alpha_)

# 8) Coefficient magnitudes
coef_ols   = ols.named_steps["linreg"].coef_
coef_ridge = ridge.named_steps["ridge"].coef_
coef_lasso = lasso.named_steps["lasso"].coef_

plt.figure(figsize=(9, 4.8))
idx = np.arange(len(coef_ols))
plt.semilogy(idx, np.abs(coef_ols),   "o", markersize=3, label="OLS")
plt.semilogy(idx, np.abs(coef_ridge), "o", markersize=3, label="Ridge")
plt.semilogy(idx, np.abs(coef_lasso), "o", markersize=3, label="Lasso")
plt.title("Coefficient Magnitudes (OLS vs Ridge vs Lasso)")
plt.xlabel("Feature index")
plt.ylabel("Absolute coefficient (log scale)")
plt.legend()
plt.tight_layout()
plt.show()

# 9) Lasso sparsity report
nz = np.count_nonzero(coef_lasso)
print(f"\nLasso nonzero coefficients: {nz} / {len(coef_lasso)} "
      f"({nz/len(coef_lasso):.2%} nonzero)")

# 10 ) Discussion

# OLS: Overfits completely --> useless for test data.
# Ridge: Best test performance, smooth shrinkage.
# Lasso: Almost as good as Ridge, but with sparsity (helps interpretability).
  </code></pre>


  <h4 id="interpret">Interpreting Results</h4>
  <div class="grid">
    <div class="chip"><b>OLS:</b> Trains to near-perfect fit (R²≈1) but generalizes poorly (often negative test R²) due to massive feature space → classic overfitting.</div>
    <div class="chip"><b>Ridge (ℓ₂):</b> Shrinks all coefficients; typically best test R² here, with stable generalization. High α means strong shrinkage.</div>
    <div class="chip"><b>Lasso (ℓ₁):</b> Shrinks and zeroes many coefficients → sparse, interpretable model with similar test performance to Ridge.</div>
  </div>

  <h4 id="notes">Notes & Tips</h4>
  <ul>
    <li><b>Data leakage:</b> Fit the polynomial transformer on <i>training</i> only (as done). Same principle for scalers, encoders, selectors.</li>
    <li><b>Alpha grids:</b> For Lasso stability on many features, try a wider grid (e.g., <code>np.logspace(-4,2,80)</code>) and/or <code>tol=1e-4</code>.</li>
    <li><b>Log plot:</b> We use <code>semilogy</code> so you can see both tiny and large coefficients clearly.</li>
    <li><b>Binary labels with linear regression:</b> OK for showing shrinkage; for classification metrics use LogisticRegression / ROC-AUC.</li>
  </ul>

</section>

<script>
  // Copy buttons
  document.querySelectorAll("pre .copy").forEach(button => {
    button.addEventListener("click", () => {
      const code = button.nextElementSibling.innerText;
      navigator.clipboard.writeText(code).then(() => {
        const old = button.textContent;
        button.textContent = "Copied!";
        setTimeout(() => button.textContent = old, 1500);
      });
    });
  });
</script>

</body>
</html>
