
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 7: Neural Networks</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }

    /* SVG and plot styles */
    svg {
      display: block;
      margin: 20px auto;
      background: #19121a;
      border-radius: 12px;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }

    .legend {
      margin-top: 20px;
      color: #ffb3de;
      font-size: 0.95em;
      line-height: 1.8;
    }

    .legend-item {
      display: flex;
      align-items: center;
      margin: 8px 0;
    }

    .legend-color {
      width: 20px;
      height: 20px;
      border-radius: 50%;
      margin-right: 12px;
    }

    .input-color {
      background: #ff79c6;
    }

    .hidden-color {
      background: #ffb3de;
    }

    .output-color {
      background: #fff52e;
    }

    .connection-color {
      background: #50fa7b;
    }

    .label {
      color: #ffb3de;
      font-size: 0.9em;
      font-weight: 500;
      text-align: center;
      margin-top: 15px;
    }
  </style>

  <!-- MathJax (once per page) -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-are-nn"><i class="fas fa-brain"></i>1. What are Artificial Neural Networks?</a></li>

    <li><a href="#neuron"><i class="fas fa-circle"></i>2. Understanding Neurons</a></li>

    <!--
    <li>
      <a href="#perceptron"><i class="fas fa-node"></i>2. The Perceptron</a>
      <div class="subsections">
        <a href="#perceptron-model">2.1 Perceptron Model</a>
        <a href="#perceptron-learning">2.2 Perceptron Learning Rule</a>
      </div>
    </li>
  -->

    <li>
      <a href="#mlp"><i class="fas fa-layer-group"></i>3. Multi-Layer Perceptron (MLP)</a>
      <div class="subsections">
        <a href="#mlp-structure">3.1 Network Structure</a>
        <a href="#forward-pass">3.2 Forward Propagation</a>
        <a href="#backprop">3.3 Backpropagation</a>
      </div>
    </li>

    <li>
      <a href="#gd"><i class="fas fa-arrow-down"></i>4. Gradient Descent</a>
      <div class="subsections">
        <a href="#gd-concept">4.1 The Concept</a>
        <a href="#gd-variants">4.2 Variants</a>
      </div>
    </li>

    <li>
      <a href="#activation"><i class="fas fa-zap"></i>5. Activation Functions</a>
      <div class="subsections">
        <a href="#common-activations">5.1 Common Functions</a>
      </div>
    </li>

    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 7: Artificial Neural Networks</h1>

  <section id="what-are-nn">
    <h3>1. What Is Artificial Neural Network?</h3>
    <p>
       Artificial neural network is a computational model inspired by how the human brain works — a system made up of
    simple processing units called <b>neurons</b> that work together to recognize patterns, make predictions,
    and learn from data. They form the foundation of modern deep learning.
    </p>

    <h4>Biological Inspiration:</h4>
    <p>In the brain:</p>
    <ul>
      <li>A neuron receives signals from other neurons.</li>
      <li>If the combined signal is strong enough, it fires and sends a new signal onward.</li>
      <li>Over time, connections that lead to correct "decisions" become stronger (learning).</li>
    </ul>

    <p>
      Artificial neurons mimic this process: receiving inputs, weighting them,
      summing, and applying an activation function.
    </p>

    <p>In an artificial neural network:</p>
    <ul>
      <li>Each neuron receives inputs (numbers, features).</li>
      <li>It multiplies each input by a weight (how important that input is).</li>
      <li>It adds them all up and applies an activation function to decide whether to "fire" (output a value).</li>
      <li>These outputs then feed into the next layer of neurons.</li>
    </ul>

    <h4>Artificial Neural Network Architecture</h4>

    <ul>
        <li><b>Input Layer:</b> Takes feature values (e.g., pixels, data attributes) </li>
        <li><b>Hidden Layers:</b> Transform features through weighted sums + activations</li>
        <li><b>Output Layer:</b> Produces final prediction (e.g., probability or label)</li>
    </ul>

    <svg width="750" height="520" viewBox="0 0 750 520">
       <!-- Title for each layer -->
       <text x="100" y="25" font-size="14" font-weight="bold" fill="#ff79c6" text-anchor="middle">Input Layer</text>
       <text x="400" y="25" font-size="14" font-weight="bold" fill="#ff79c6" text-anchor="middle">Hidden Layer</text>
       <text x="680" y="25" font-size="14" font-weight="bold" fill="#ff79c6" text-anchor="middle">Output Layer</text>

       <!-- Input Layer Neurons -->
       <circle cx="100" cy="80" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
       <text x="100" y="85" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₁</text>

       <circle cx="100" cy="170" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
       <text x="100" y="175" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₂</text>

       <circle cx="100" cy="260" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
       <text x="100" y="265" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₃</text>

       <circle cx="100" cy="350" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
       <text x="100" y="355" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₄</text>

       <!-- Hidden Layer Neurons -->
       <circle cx="400" cy="110" r="15" fill="#ffb3de" stroke="#ff79c6" stroke-width="2"/>
       <text x="400" y="115" font-size="11" fill="white" text-anchor="middle" font-weight="bold">h₁</text>

       <circle cx="400" cy="220" r="15" fill="#ffb3de" stroke="#ff79c6" stroke-width="2"/>
       <text x="400" y="225" font-size="11" fill="white" text-anchor="middle" font-weight="bold">h₂</text>

       <circle cx="400" cy="330" r="15" fill="#ffb3de" stroke="#ff79c6" stroke-width="2"/>
       <text x="400" y="335" font-size="11" fill="white" text-anchor="middle" font-weight="bold">h₃</text>

       <!-- Output Layer Neurons -->
       <circle cx="680" cy="150" r="15" fill="#fff52e" stroke="#ffb3de" stroke-width="2"/>
       <text x="680" y="155" font-size="11" fill="black" text-anchor="middle" font-weight="bold">y₁</text>

       <circle cx="680" cy="290" r="15" fill="#fff52e" stroke="#ffb3de" stroke-width="2"/>
       <text x="680" y="295" font-size="11" fill="black" text-anchor="middle" font-weight="bold">y₂</text>

       <!-- Connections from Input to Hidden Layer (with weight labels) -->
       <line x1="115" y1="80" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="90" font-size="9" fill="#50fa7b" font-weight="bold">w₁₁</text>

       <line x1="115" y1="80" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="140" font-size="9" fill="#50fa7b" font-weight="bold">w₁₂</text>

       <line x1="115" y1="80" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="190" font-size="9" fill="#50fa7b" font-weight="bold">w₁₃</text>

       <line x1="115" y1="170" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="135" font-size="9" fill="#50fa7b" font-weight="bold">w₂₁</text>

       <line x1="115" y1="170" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="190" font-size="9" fill="#50fa7b" font-weight="bold">w₂₂</text>

       <line x1="115" y1="170" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="245" font-size="9" fill="#50fa7b" font-weight="bold">w₂₃</text>

       <line x1="115" y1="260" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="180" font-size="9" fill="#50fa7b" font-weight="bold">w₃₁</text>

       <line x1="115" y1="260" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="235" font-size="9" fill="#50fa7b" font-weight="bold">w₃₂</text>

       <line x1="115" y1="260" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="290" font-size="9" fill="#50fa7b" font-weight="bold">w₃₃</text>

       <line x1="115" y1="350" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="225" font-size="9" fill="#50fa7b" font-weight="bold">w₄₁</text>

       <line x1="115" y1="350" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="280" font-size="9" fill="#50fa7b" font-weight="bold">w₄₂</text>

       <line x1="115" y1="350" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="240" y="335" font-size="9" fill="#50fa7b" font-weight="bold">w₄₃</text>

       <!-- Connections from Hidden Layer to Output Layer -->
       <line x1="415" y1="110" x2="665" y2="150" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="535" y="125" font-size="9" fill="#50fa7b" font-weight="bold">w₁</text>

       <line x1="415" y1="110" x2="665" y2="290" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="535" y="195" font-size="9" fill="#50fa7b" font-weight="bold">w₂</text>

       <line x1="415" y1="220" x2="665" y2="150" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="535" y="180" font-size="9" fill="#50fa7b" font-weight="bold">w₃</text>

       <line x1="415" y1="220" x2="665" y2="290" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="535" y="250" font-size="9" fill="#50fa7b" font-weight="bold">w₄</text>

       <line x1="415" y1="330" x2="665" y2="150" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="535" y="235" font-size="9" fill="#50fa7b" font-weight="bold">w₅</text>

       <line x1="415" y1="330" x2="665" y2="290" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
       <text x="535" y="305" font-size="9" fill="#50fa7b" font-weight="bold">w₆</text>

       <!-- Bias nodes -->
       <circle cx="100" cy="430" r="10" fill="#888888" stroke="#666666" stroke-width="2"/>
       <text x="100" y="434" font-size="9" fill="white" text-anchor="middle" font-weight="bold">b⁰</text>

       <circle cx="400" cy="430" r="10" fill="#888888" stroke="#666666" stroke-width="2"/>
       <text x="400" y="434" font-size="9" fill="white" text-anchor="middle" font-weight="bold">b¹</text>

       <circle cx="680" cy="430" r="10" fill="#888888" stroke="#666666" stroke-width="2"/>
       <text x="680" y="434" font-size="9" fill="white" text-anchor="middle" font-weight="bold">b²</text>

       <!-- Lines from bias to layers -->
       <line x1="100" y1="420" x2="100" y2="360" stroke="#888888" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
       <line x1="400" y1="420" x2="400" y2="345" stroke="#888888" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
       <line x1="680" y1="420" x2="680" y2="305" stroke="#888888" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
     </svg>

     <div class="label"><strong>Architecture:</strong> 4 inputs → 3 hidden neurons (with ReLU) → 2 outputs</div>


    <div class="legend">
      <strong style="color: #ff79c6;">Legend:</strong>
      <div class="legend-item">
        <div class="legend-color input-color"></div>
        <span><b>Input Neurons:</b> Receive feature values (x₁, x₂, x₃, x₄)</span>
      </div>
      <div class="legend-item">
        <div class="legend-color hidden-color"></div>
        <span><b>Hidden Neurons:</b> Learn abstract representations (h₁, h₂, h₃)</span>
      </div>
      <div class="legend-item">
        <div class="legend-color output-color"></div>
        <span><b>Output Neurons:</b> Produce final predictions (y₁, y₂)</span>
      </div>
      <div class="legend-item">
        <div class="legend-color connection-color"></div>
        <span><b>Connections (weights):</b> Green lines represent learned parameters connecting neurons</span>
      </div>
      <div class="legend-item">
        <div class="legend-color" style="background: #888888;"></div>
        <span><b>Bias nodes:</b> Help shift activation function; one per layer</span>
      </div>
    </div>
  </section>



  <section id="neuron">

    <h3>2. Understanding Neurons in Artificial Neural Networks</h3>
    <p>
      A neuron in artificial neural network is a <b>computational unit</b> that mimics the behavior of biological neurons.
      A neuron takes multiple inputs, processes them, and produces a single output.
    </p>

    <ul>
      <li>Receives inputs</li>
      <li>Weights each input</li>
      <li>Adds bias</li>
      <li>Applies activation function</li>
      <li>Outputs result</li>
    </ul>

    <br>

    <p>Each neuron (in hidden and output layers) computes this function:</p>

    <p style="text-align:center;">
      $$
      y = \sigma(w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b)
      $$
    </p>

    <table style="width: 100%; border-collapse: collapse;">
      <thead>
        <tr style="background: #2a1e28; color: #ff79c6;">
          <th style="border: 1px solid #ff79c6; padding: 12px; text-align: left; font-weight: 700;">Symbol</th>
          <th style="border: 1px solid #ff79c6; padding: 12px; text-align: left; font-weight: 700;">Meaning</th>
          <th style="border: 1px solid #ff79c6; padding: 12px; text-align: left; font-weight: 700;">Analogy</th>
        </tr>
      </thead>
      <tbody>
        <tr style="background: #222025;">
          <td style="border: 1px solid #ffb3de; padding: 12px;">\( x_1, x_2, \dots, x_n \)</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Inputs (features)</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Signals received by the neuron</td>
        </tr>
        <tr style="background: #1f1a24;">
          <td style="border: 1px solid #ffb3de; padding: 12px;">\( w_1, w_2, \dots, w_n \)</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Weights</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Strength of each connection</td>
        </tr>
        <tr style="background: #222025;">
          <td style="border: 1px solid #ffb3de; padding: 12px;">\( b \)</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Bias</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Helps the neuron make decisions</td>
        </tr>
        <tr style="background: #1f1a24;">
          <td style="border: 1px solid #ffb3de; padding: 12px;">\( \sigma(\cdot) \)</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Activation function</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Converts raw input into output (e.g., sigmoid, ReLU)</td>
        </tr>
        <tr style="background: #222025;">
          <td style="border: 1px solid #ffb3de; padding: 12px;">\( y \)</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">Output</td>
          <td style="border: 1px solid #ffb3de; padding: 12px;">The neuron's response — sent to the next layer</td>
        </tr>
      </tbody>
    </table>


    <!-- Input Layer Computation -->
<div class="computation-box">
  <h4>What Happens in Input Neurons?</h4>

  <p><strong>The input layer is the simplest:</strong></p>
  <ol>
    <li><strong>Receives raw data:</strong> Takes feature values directly from your dataset</li>
    <li><strong>No computation:</strong> Just passes values through to hidden layer</li>
    <li><strong>No activation function:</strong> Raw values are sent as-is</li>
  </ol>

  <p><strong>Key Point:</strong> Input neurons are just "placeholder" neurons that hold your raw features.
  The real computation happens in the next layers!</p>

</div>


<!-- Hidden Layer Computation -->
<div class="computation-box">
  <h4>What Happens in Hidden Neurons?</h4>
  <p><strong>Each hidden neuron does 3 steps:</strong></p>
  <ol>
    <li><strong>Weighted Sum:</strong> Multiply each input by its weight and add bias</li>
    <li><strong>Activation Function:</strong> Apply ReLU (or sigmoid, tanh)</li>
    <li><strong>Pass Forward:</strong> Send output to next layer</li>
  </ol>

  <p><strong>For neuron \(h_1\):</strong></p>
  <div class="code-block">
$$ z_1 = w_{11} \times x_1 + w_{21} \times x_2 + w_{31} \times x_3 + w_{41} \times x_4 + b_1 $$
$$ h_1 = \text{ReLU}(z_1) = max(0, z_1) $$
  </div>

  <table style="border: 2px solid black; border-collapse: collapse; text-align: center;">
  <tr>
    <th style="border: 1px solid black; padding: 6px;">Neuron</th>
    <th style="border: 1px solid black; padding: 6px;">Computation</th>
    <th style="border: 1px solid black; padding: 6px;">Activation</th>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 6px;">\(h_1\)</td>
    <td style="border: 1px solid black; padding: 6px;">\(z_1 = w_{11}x_1 + w_{21}x_2 + w_{31}x_3 + w_{41}x_4 + b_1\)</td>
    <td style="border: 1px solid black; padding: 6px;">\(h_1 = \text{ReLU}(z_1)\)</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 6px;">\(h_2\)</td>
    <td style="border: 1px solid black; padding: 6px;">\(z_2 = w_{12}x_1 + w_{22}x_2 + w_{32}x_3 + w_{42}x_4 + b_2\)</td>
    <td style="border: 1px solid black; padding: 6px;">\(h_2 = \text{ReLU}(z_2)\)</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 6px;">\(h_3\)</td>
    <td style="border: 1px solid black; padding: 6px;">\(z_3 = w_{13}x_1 + w_{23}x_2 + w_{33}x_3 + w_{43}x_4 + b_3\)</td>
    <td style="border: 1px solid black; padding: 6px;">\(h_3 = \text{ReLU}(z_3)\)</td>
  </tr>
</table>
</div>

<!-- Output Layer Computation -->
<div class="computation-box">
  <h4>What Happens in Output Neurons?</h4>
  <p><strong>Similar process, but the output layer typically uses a different activation:</strong></p>

  <p>For <strong>binary classification</strong>, use Sigmoid:</p>
  <div class="code-block">
    \(
    z_{1}^{out} = w_{1}h_{1} + w_{3}h_{2} + w_{5}h_{3} + b_{2}
    \)<br>
    \(
    y_{1} = \sigma(z_{1}^{out}) = \frac{1}{1 + e^{-z_{1}^{out}}}
    \) (outputs probability between 0 and 1)
  </div>

  <p>For <strong>regression</strong>, use Linear (no activation):</p>
  <div class="code-block">
    \(
    y_{1} = w_{1}h_{1} + w_{3}h_{2} + w_{5}h_{3} + b_{2}
    \) (outputs any real number)
  </div>
</div>

  </section>



<!--
  <section id="perceptron">
    <h3>2. The Perceptron</h3>
    <p>
      The <b>perceptron</b> is the simplest neural network model — a single neuron that performs
      <b><font color="red">binary classification</font></b>. It forms the building block of more complex networks.
    </p>

    <h4 id="perceptron-model">2.1 Perceptron Model</h4>
    <p>
      A perceptron takes \(d\) inputs \(\mathbf{x} = (x_1, x_2, \ldots, x_d)\) and computes:
    </p>
    <p style="text-align:center;">
      $$
      z = \mathbf{w}^T \mathbf{x} + b = \sum_{i=1}^{d} w_i x_i + b
      $$
    </p>
    <p>
      Then applies a <b>threshold activation function</b>:
    </p>
    <p style="text-align:center;">
      $$
      \hat{y} = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}
      $$
    </p>

    <p><strong>Where:</strong></p>
    <ul>
      <li>\(\mathbf{w}\) = weight vector; \(b\) = bias term.</li>
      <li>\(\mathbf{w}^T \mathbf{x}\) = dot product (weighted sum).</li>
      <li>Output is binary: 0 or 1 (or ±1 depending on formulation).</li>
    </ul>

    <h4 id="perceptron-learning">2.2 Perceptron Learning Rule</h4>
    <p>
      The perceptron learns by adjusting weights to minimize classification errors. The learning rule is:
    </p>
    <p style="text-align:center;">
      $$
      \mathbf{w} \leftarrow \mathbf{w} + \eta (y - \hat{y}) \mathbf{x}, \quad b \leftarrow b + \eta (y - \hat{y})
      $$
    </p>

    <p><strong>Where:</strong></p>
    <ul>
      <li>\(\eta\) = learning rate (typically small, e.g., 0.01).</li>
      <li>\(y\) = true label; \(\hat{y}\) = predicted label.</li>
      <li>Update occurs only when \(y \neq \hat{y}\) (error).</li>
    </ul>

    <h4>Algorithm: Perceptron Training</h4>
    <ol>
      <li>Initialize \(\mathbf{w}\) and \(b\) to small random values (or zero).</li>
      <li>For each training example \((\mathbf{x}, y)\):</li>
      <ul>
        <li>Compute prediction: \(\hat{y} = \text{sign}(\mathbf{w}^T \mathbf{x} + b)\)</li>
        <li>If \(y \neq \hat{y}\), update: \(\mathbf{w} \leftarrow \mathbf{w} + \eta (y - \hat{y}) \mathbf{x}\) and \(b \leftarrow b + \eta (y - \hat{y})\)</li>
      </ul>
      <li>Repeat for multiple epochs until convergence.</li>
    </ol>

    <h4>Limitations:</h4>
    <ul>
      <li>Can only solve <b>linearly separable</b> problems.</li>
      <li>Cannot learn XOR (exclusive OR) function.</li>
      <li>Single layer prevents learning complex patterns.</li>
    </ul>
  </section>
-->

<section id="mlp">
  <h3>3. Multi-Layer Perceptron (MLP)</h3>
  <p>
    A Multi-Layer Perceptron (MLP) is a type of feedforward neural network that has one or more hidden layers between
    the input and output layers.
    Each neuron in one layer connects to every neuron in the next layer — that’s why it’s called a fully connected network.
    It can learn <b><font color="red">non-linear decision boundaries</font></b>.
  </p>

  <h4 id="mlp-structure">3.1 Network Structure</h4>
  <p>An MLP consists of three types of layers:</p>
  <ul>
    <li><b><font color="red">Input Layer:</font></b> Receives feature vectors \(\mathbf{x}\); no activation.</li>
    <br>
    <li><b><font color="red">Hidden Layers:</font></b> Perform non-linear transformations; learn abstract representations. Apply nonlinear activations (e.g., ReLU, sigmoid, tanh).</li>
    $$ h_j = \sigma(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + b) $$
    $$ \text{where:} \quad \sigma : \text{hidden activation (e.g., ReLU, sigmoid, tanh)} $$
    <br>

    <li><b><font color="red">Output Layer:</font></b> Produces final predictions; often uses softmax (multi-class) or sigmoid (binary).</li>
    $$ y_1 = \phi(w_1h_1 + w_3h_2 + w_5h_3 + c_1) $$
    $$ y_2 = \phi(w_2h_1 + w_4h_2 + w_6h_3 + c_2) $$
    $$ \phi : \text{output activation (e.g., Sigmoid or Softmax)} $$
  </ul>


  <svg width="750" height="520" viewBox="0 0 750 520">
     <!-- Title for each layer -->
     <text x="100" y="25" font-size="14" font-weight="bold" fill="#ff79c6" text-anchor="middle">Input Layer</text>
     <text x="400" y="25" font-size="14" font-weight="bold" fill="#ff79c6" text-anchor="middle">Hidden Layer</text>
     <text x="680" y="25" font-size="14" font-weight="bold" fill="#ff79c6" text-anchor="middle">Output Layer</text>

     <!-- Input Layer Neurons -->
     <circle cx="100" cy="80" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
     <text x="100" y="85" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₁</text>

     <circle cx="100" cy="170" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
     <text x="100" y="175" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₂</text>

     <circle cx="100" cy="260" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
     <text x="100" y="265" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₃</text>

     <circle cx="100" cy="350" r="15" fill="#ff79c6" stroke="#ffb3de" stroke-width="2"/>
     <text x="100" y="355" font-size="11" fill="white" text-anchor="middle" font-weight="bold">x₄</text>

     <!-- Hidden Layer Neurons -->
     <circle cx="400" cy="110" r="15" fill="#ffb3de" stroke="#ff79c6" stroke-width="2"/>
     <text x="400" y="115" font-size="11" fill="white" text-anchor="middle" font-weight="bold">h₁</text>

     <circle cx="400" cy="220" r="15" fill="#ffb3de" stroke="#ff79c6" stroke-width="2"/>
     <text x="400" y="225" font-size="11" fill="white" text-anchor="middle" font-weight="bold">h₂</text>

     <circle cx="400" cy="330" r="15" fill="#ffb3de" stroke="#ff79c6" stroke-width="2"/>
     <text x="400" y="335" font-size="11" fill="white" text-anchor="middle" font-weight="bold">h₃</text>

     <!-- Output Layer Neurons -->
     <circle cx="680" cy="150" r="15" fill="#fff52e" stroke="#ffb3de" stroke-width="2"/>
     <text x="680" y="155" font-size="11" fill="black" text-anchor="middle" font-weight="bold">y₁</text>

     <circle cx="680" cy="290" r="15" fill="#fff52e" stroke="#ffb3de" stroke-width="2"/>
     <text x="680" y="295" font-size="11" fill="black" text-anchor="middle" font-weight="bold">y₂</text>

     <!-- Connections from Input to Hidden Layer (with weight labels) -->
     <line x1="115" y1="80" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="90" font-size="9" fill="#50fa7b" font-weight="bold">w₁₁</text>

     <line x1="115" y1="80" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="140" font-size="9" fill="#50fa7b" font-weight="bold">w₁₂</text>

     <line x1="115" y1="80" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="190" font-size="9" fill="#50fa7b" font-weight="bold">w₁₃</text>

     <line x1="115" y1="170" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="135" font-size="9" fill="#50fa7b" font-weight="bold">w₂₁</text>

     <line x1="115" y1="170" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="190" font-size="9" fill="#50fa7b" font-weight="bold">w₂₂</text>

     <line x1="115" y1="170" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="245" font-size="9" fill="#50fa7b" font-weight="bold">w₂₃</text>

     <line x1="115" y1="260" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="180" font-size="9" fill="#50fa7b" font-weight="bold">w₃₁</text>

     <line x1="115" y1="260" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="235" font-size="9" fill="#50fa7b" font-weight="bold">w₃₂</text>

     <line x1="115" y1="260" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="290" font-size="9" fill="#50fa7b" font-weight="bold">w₃₃</text>

     <line x1="115" y1="350" x2="385" y2="110" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="225" font-size="9" fill="#50fa7b" font-weight="bold">w₄₁</text>

     <line x1="115" y1="350" x2="385" y2="220" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="280" font-size="9" fill="#50fa7b" font-weight="bold">w₄₂</text>

     <line x1="115" y1="350" x2="385" y2="330" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="240" y="335" font-size="9" fill="#50fa7b" font-weight="bold">w₄₃</text>

     <!-- Connections from Hidden Layer to Output Layer -->
     <line x1="415" y1="110" x2="665" y2="150" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="535" y="125" font-size="9" fill="#50fa7b" font-weight="bold">w₁</text>

     <line x1="415" y1="110" x2="665" y2="290" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="535" y="195" font-size="9" fill="#50fa7b" font-weight="bold">w₂</text>

     <line x1="415" y1="220" x2="665" y2="150" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="535" y="180" font-size="9" fill="#50fa7b" font-weight="bold">w₃</text>

     <line x1="415" y1="220" x2="665" y2="290" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="535" y="250" font-size="9" fill="#50fa7b" font-weight="bold">w₄</text>

     <line x1="415" y1="330" x2="665" y2="150" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="535" y="235" font-size="9" fill="#50fa7b" font-weight="bold">w₅</text>

     <line x1="415" y1="330" x2="665" y2="290" stroke="#50fa7b" stroke-width="2" opacity="0.7"/>
     <text x="535" y="305" font-size="9" fill="#50fa7b" font-weight="bold">w₆</text>

     <!-- Bias nodes -->
     <circle cx="100" cy="430" r="10" fill="#888888" stroke="#666666" stroke-width="2"/>
     <text x="100" y="434" font-size="9" fill="white" text-anchor="middle" font-weight="bold">b⁰</text>

     <circle cx="400" cy="430" r="10" fill="#888888" stroke="#666666" stroke-width="2"/>
     <text x="400" y="434" font-size="9" fill="white" text-anchor="middle" font-weight="bold">b¹</text>

     <circle cx="680" cy="430" r="10" fill="#888888" stroke="#666666" stroke-width="2"/>
     <text x="680" y="434" font-size="9" fill="white" text-anchor="middle" font-weight="bold">b²</text>

     <!-- Lines from bias to layers -->
     <line x1="100" y1="420" x2="100" y2="360" stroke="#888888" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
     <line x1="400" y1="420" x2="400" y2="345" stroke="#888888" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
     <line x1="680" y1="420" x2="680" y2="305" stroke="#888888" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
   </svg>

   <div class="label"><strong>Architecture:</strong> 4 inputs → 3 hidden neurons (with ReLU) → 2 outputs</div>

   <h4>How It Works</h4>

   <ul>
        <li>Forward propagation: Data flows from inputs → hidden → output layers.</li>
        <li>Activation functions introduce nonlinearity, allowing the model to learn complex patterns.</li>
        <li>Backpropagation: The model adjusts weights and biases based on errors (using gradient descent).</li>
   </ul>

  <h4 id="forward-pass">3.2 Forward Propagation</h4>
  <!--  -->

  <p>During <b>forward propagation</b>, data moves through the network layer by layer — from the <b>input</b> to the <b>output</b>. Each neuron computes a weighted sum of its inputs, adds a bias, and applies an activation function.</p>

  <ol>
    <li><b><font color="red">Step 1 – Input to Hidden Layer:</font></b><br>
        The input layer passes feature values directly to the hidden layer (no computation or activation).<br><br>
        Each hidden neuron \(h_j\) computes:
        $$ h_j = \sigma(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + w_{4j}x_4 + b_j) $$
        $$ \sigma : \text{hidden activation (e.g., ReLU, sigmoid, tanh)} $$
    </li>
    <br>

    <li><b><font color="red">Step 2 – Hidden to Output Layer:</font></b><br>
        The outputs from hidden neurons are then combined and passed to the output layer:
        $$ y_k = \phi(v_{1k}h_1 + v_{2k}h_2 + v_{3k}h_3 + c_k) $$
        where \( k = 1, 2, ... , m \) (number of output neurons).<br><br>
        $$ \phi : \text{output activation (e.g., Sigmoid for binary, Softmax for multi-class)} $$
    </li>
    <br>

    <li><b><font color="red">Step 3 – Output Generation:</font></b><br>
        The output layer produces final predictions — probabilities for classification or numeric values for regression.
    </li>
  </ol>

  <div class="computation-box" style="background:#1e1e2f; color:#fff; padding:15px; border-radius:8px; margin-top:15px;">
    <p><b>Summary of Forward Propagation:</b></p>
    <p style="margin-left:20px;">
      Input Features → Weighted Sum → Activation → Output Prediction
    </p>
    <p style="text-align:center;">
      $$ \mathbf{x} \xrightarrow[]{W^{(1)}, b^{(1)}} \mathbf{h} = \sigma(W^{(1)}\mathbf{x} + b^{(1)})
         \xrightarrow[]{W^{(2)}, b^{(2)}} \mathbf{y} = \phi(W^{(2)}\mathbf{h} + b^{(2)}) $$
    </p>
  </div>
  <!-- -->


  <h4 id="backprop">3.3 Backpropagation</h4>

  <p><b>Backpropagation</b> is the algorithm for training MLPs. It computes gradients of the loss with respect to all weights and biases, then updates them to minimize error.</p>

  <p>
    After forward propagation produces an output, the network compares it to the true target and calculates an error.
    <b>Backpropagation</b> is the process of sending that error backward through the network to adjust the weights and biases
    — so the model learns to make better predictions next time.
  </p>

  <ol>
    <li><b><font color="red">Step 1 – Compute the Loss:</font></b><br>
        The loss function measures how far the prediction \( \hat{y} \) is from the true label \( y \).<br>
        Common loss functions:
        <ul>
          <li>Mean Squared Error (Regression):  \( L = \frac{1}{2}(y - \hat{y})^2 \)</li>
          <li>Cross-Entropy (Classification): \( L = -\sum y_i \log(\hat{y}_i) \)</li>
        </ul>
    </li>
    <br>

    <li><b><font color="red">Step 2 – Compute the Output Layer Gradient:</font></b><br>
        We find how much each output neuron contributed to the total error by computing partial derivatives:
        $$ \frac{\partial L}{\partial w_{jk}^{(2)}} = \delta_k^{(2)} \, h_j $$
        $$ \text{where: } \delta_k^{(2)} = ( \hat{y}_k - y_k ) \, \phi'(z_k^{(2)}) $$
        Here \( \phi'(z_k^{(2)}) \) is the derivative of the output activation function.
    </li>
    <br>

    <li><b><font color="red">Step 3 – Propagate Error to Hidden Layer:</font></b><br>
        The hidden layer errors are found by propagating output errors backward:
        $$ \delta_j^{(1)} = \sigma'(z_j^{(1)}) \sum_k \delta_k^{(2)} w_{jk}^{(2)} $$
        $$ \text{where: } \sigma'(z_j^{(1)}) \text{ is the derivative of the hidden activation function.} $$
    </li>
    <br>

    <li><b><font color="red">Step 4 – Update Weights and Biases:</font></b><br>
        Each weight and bias is adjusted using the learning rate \( \eta \):
        $$ w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} - \eta \frac{\partial L}{\partial w_{ij}^{(l)}} $$
        $$ b_j^{(l)} \leftarrow b_j^{(l)} - \eta \frac{\partial L}{\partial b_j^{(l)}} $$
        The goal is to reduce the overall loss by making small corrections that move the network in the right direction.
    </li>
  </ol>

  <div class="computation-box" style="background:#1e1e2f; color:#fff; padding:15px; border-radius:8px; margin-top:15px;">
    <p><b>Summary of Backpropagation:</b></p>
    <p style="margin-left:20px;">
      1️⃣ Compute loss →
      2️⃣ Find output error →
      3️⃣ Propagate error backward →
      4️⃣ Update weights/biases
    </p>
    <p style="text-align:center;">
      $$ \Delta w_{ij}^{(l)} = -\eta \frac{\partial L}{\partial w_{ij}^{(l)}} $$
      $$ \Delta b_j^{(l)} = -\eta \frac{\partial L}{\partial b_j^{(l)}} $$
    </p>
  </div>

  <div style="margin-top:10px; font-size:13px; color:#ccc;">
    <b>Intuition:</b> Backpropagation is like “blaming” each connection for its share of the error —
    then tuning those connections to make the network’s next guess a little more accurate.
  </div>


  <!-- -->

  <h4>Hyperparameters:</h4>
  <ul>
    <li><b>Learning rate \(\eta\):</b> Controls step size; too large diverges, too small converges slowly.</li>
    <li><b>Number of hidden layers:</b> More layers learn deeper abstractions but risk overfitting.</li>
    <li><b>Hidden layer size:</b> More neurons increase capacity but computational cost.</li>
    <li><b>Epochs:</b> Number of full passes through training data.</li>
    <li><b>Batch size:</b> Number of samples per gradient update (for mini-batch SGD).</li>
  </ul>


  <h4>Who Proposed Backpropagation?</h4>

  <p>
    The history of the <b>Backpropagation</b> algorithm has two key stages:
  </p>

  <ul>
      <li><b> Early Mathematical Idea (1960s):</b><br>
      The idea of computing gradients in multi-layer networks dates back to <b>Paul Werbos (1974)</b> in his Ph.D. dissertation
      <i>"Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences."</i></li>
      <br>
      <li><b> Popularization in Neural Networks (1986):</b><br>
      <b>David E. Rumelhart</b>, <b>Geoffrey E. Hinton</b>, and <b>Ronald J. Williams</b> published the landmark paper:<br>
      <i>“Learning Representations by Back-Propagating Errors”</i> (<b>Nature</b>, 1986).<br>
      This paper demonstrated that backpropagation could effectively train multi-layer perceptrons —
      sparking the <b>neural network revolution</b> in AI.</li>
  </ul>

</section>


  <section id="gd">
    <h3>4. Gradient Descent</h3>
    <p>
      <b>Gradient Descent</b> is the <b><font color="red">optimization algorithm</font></b> used to train neural networks.
      It iteratively updates weights and biases by moving in the direction of the negative gradient of the loss function,
      aiming to reach a minimum.
    </p>

    <h4 id="gd-concept">4.1 The Concept</h4>
    <p>
      Imagine you're on a hill in fog and want to reach the lowest point. You can't see far ahead,
      but you can feel the slope under your feet. You take a step downhill, feel the new slope, and repeat.
      Gradient descent works the same way: it follows the direction of steepest descent.
    </p>

    <p>
      The gradient \(\nabla L\) tells us how much the loss changes with respect to each parameter:
    </p>
    <p style="text-align:center;">
      $$
      \nabla L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, \ldots, \frac{\partial L}{\partial w_d} \right)
      $$
    </p>

    <p>
      The update rule moves weights in the opposite direction (negative gradient):
    </p>
    <p style="text-align:center;">
      $$
      \mathbf{w} \leftarrow \mathbf{w} - \eta \nabla L(\mathbf{w})
      $$
    </p>

    <p><strong>Where:</strong></p>
  <ul>
    <li>\(\eta\) = learning rate; controls step size (typically 0.001 to 0.1).</li>
    <li>\(\nabla L\) = gradient of loss with respect to weights.</li>
    <li>Negative sign ensures we move toward lower loss.</li>
  </ul>

  <h4>Gradient Descent Algorithm:</h4>
  <ol>
    <li>Initialize weights \(\mathbf{w}\) randomly (small values).</li>
    <li>For each iteration (epoch):
      <ul style="margin-left: 1.5em;">
        <li>Compute loss: \(L = \frac{1}{m} \sum_{i=1}^{m} \text{Loss}(f(\mathbf{x}_i; \mathbf{w}), y_i)\)</li>
        <li>Compute gradient: \(\nabla L = \frac{1}{m} \sum_{i=1}^{m} \nabla \text{Loss}(\mathbf{x}_i, y_i)\)</li>
        <li>Update weights: \(\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla L\)</li>
      </ul>
    </li>
    <li>Repeat until convergence (loss stops decreasing or max epochs reached).</li>
  </ol>

  <h4 id="gd-variants">4.2 Variants of Gradient Descent</h4>

  <h4>Batch Gradient Descent (BGD)</h4>
  <ul>
    <li>Uses the <b>entire dataset</b> to compute gradient in each iteration.</li>
    <li>Pro: Smooth convergence, stable updates.</li>
    <li>Con: Slow for large datasets; requires keeping entire dataset in memory.</li>
  </ul>

  <h4>Stochastic Gradient Descent (SGD)</h4>
  <ul>
    <li>Updates weights using <b>one sample</b> at a time.</li>
    <li>Pro: Fast updates; can escape local minima due to noise.</li>
    <li>Con: Noisy gradient leads to oscillations; harder to converge smoothly.</li>
  </ul>

  <h4>Mini-Batch Gradient Descent</h4>
  <ul>
    <li>Uses a <b>small batch</b> of samples (e.g., 32, 64, 128) per update.</li>
    <li>Pro: Balance between stability (batch) and speed (stochastic); practical sweet spot.</li>
    <li>Con: Requires tuning batch size.</li>
  </ul>

  <h4>Momentum</h4>
  <p>Accumulates gradient history to accelerate convergence and reduce oscillations:</p>
  <p style="text-align:center;">$$\mathbf{v} \leftarrow \beta \mathbf{v} + (1 - \beta) \nabla L, \quad \mathbf{w} \leftarrow \mathbf{w} - \eta \mathbf{v}$$</p>
  <ul>
    <li>\(\beta\) ≈ 0.9 (momentum coefficient); \(\mathbf{v}\) accumulates past gradients.</li>
    <li>Acts like a "rolling ball" with inertia; helps escape shallow local minima.</li>
  </ul>

  <h4>Adam (Adaptive Moment Estimation)</h4>
  <p>Modern optimizer that adapts learning rate per parameter:</p>
  <p style="text-align:center;">$$m \leftarrow \beta_1 m + (1-\beta_1) \nabla L, \quad v \leftarrow \beta_2 v + (1-\beta_2) (\nabla L)^2$$</p>
  <p style="text-align:center;">$$\mathbf{w} \leftarrow \mathbf{w} - \eta \frac{m}{\sqrt{v} + \epsilon}$$</p>
  <ul>
    <li>\(m\) = exponential moving average of gradients (momentum).</li>
    <li>\(v\) = exponential moving average of squared gradients (adaptive learning rate).</li>
    <li>\(\epsilon\) = small constant to avoid division by zero.</li>
    <li>Default: \(\beta_1 = 0.9, \beta_2 = 0.999\); widely used in practice.</li>
  </ul>

  <h4>Convergence Tips:</h4>
  <ul>
    <li><b>Learning rate too high:</b> Loss oscillates or diverges.</li>
    <li><b>Learning rate too low:</b> Convergence is very slow.</li>
    <li><b>Feature scaling:</b> Normalize inputs to similar ranges for faster convergence.</li>
    <li><b>Batch size:</b> Larger batches are more stable but slower per update.</li>
  </ul>
</section>

  </section>



  <section id="activation">
  <h3>5. Activation Functions</h3>
  <p>Activation functions introduce <b><font color="red">non-linearity</font></b> into networks. Without them, stacking layers would be equivalent to a single linear transformation.</p>

  <h4 id="common-activations">5.1 Common Activation Functions</h4>

  <h4>Sigmoid</h4>
  <p style="text-align:center;">$$\sigma(z) = \frac{1}{1 + e^{-z}}$$</p>
  <ul>
    <li>Output range: (0, 1); often used in output layers for binary classification.</li>
    <li>Con: Vanishing gradient problem for very positive/negative inputs.</li>
  </ul>

  <h4>Tanh (Hyperbolic Tangent)</h4>
  <p style="text-align:center;">$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$</p>
  <ul>
    <li>Output range: (-1, 1); zero-centered, often better than sigmoid.</li>
    <li>Con: Still suffers from vanishing gradients.</li>
  </ul>

  <h4>ReLU (Rectified Linear Unit)</h4>
  <p style="text-align:center;">$$\text{ReLU}(z) = \max(0, z)$$</p>
  <ul>
    <li>Simple, computationally efficient; widely used in hidden layers.</li>
    <li>Pro: Avoids vanishing gradient; promotes sparsity.</li>
    <li>Con: Dying ReLU (neurons stuck at 0) for very negative inputs.</li>
  </ul>

  <h4>Leaky ReLU</h4>
  <p style="text-align:center;">$$\text{LeakyReLU}(z) = \begin{cases} z & \text{if } z > 0 \\ \alpha z & \text{if } z \leq 0 \end{cases}, \quad \alpha \approx 0.01$$</p>
  <ul>
    <li>Variant of ReLU; allows small negative values to mitigate dying ReLU.</li>
  </ul>

  <h4>Softmax (Output Layer)</h4>
  <p style="text-align:center;">$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$$</p>
  <ul>
    <li>Maps output to probability distribution over classes; used in multi-class classification.</li>
    <li>Ensures outputs sum to 1.</li>
  </ul>

  <h4>Choosing Activation Functions:</h4>
  <ul>
    <li><b>Hidden layers:</b> ReLU or variants are default choices in modern networks.</li>
    <li><b>Binary classification output:</b> Sigmoid.</li>
    <li><b>Multi-class output:</b> Softmax.</li>
    <li><b>Regression output:</b> Linear (identity) activation or none.</li>
  </ul>
</section>

<seciton>

</section>


</body>
</html>
