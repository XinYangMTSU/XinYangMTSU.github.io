<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 3: Supervised Learning - Regression</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */(no penalty) 
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
    
    table, th , td {
    border: 1px solid #cc66ff;
    border-collapse: collapse;
    padding: 2px;
    }

    
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    
    <li>
      <a href="#regression"><i class="fas fa-gamepad"></i>2. Regression</a>
      <div class="subsections">
        <a href="#linear-regression">2.1 Linear Regression</a>
        <a href="#polynomial-regression">2.2 Polynomial Regression</a>
        <a href="#ridge-regression">2.3 Ridge Regression</a>
        <a href="#lasso-regression">2.4 Lasso Regression</a>
        <a href="#elastic-net-regression">2.5 Elastic Net Regression</a>
        
      </div>
    </li>
    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 3: Supervised Learning - Regression</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>

    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li><b>1. Regression</b></li>
       <li>2. Classification</li>
    </ul>
    
  </section>
    
 
  <section id="supervised-reg">
    
  <div id="regression">  
      <h2>2. Regression</h2>
      <p>Regression algorithms <b><font color="red">predict numeric values</font></b> (e.g., house prices, temperature).
      Common methods include:
      </p>
      <ul>
        <li><strong>Linear Regression:</strong> Predicts a continuous outcome based on a linear relationship between input features and the target variable.</li>
        <li><strong>Polynomial Regression:</strong> Extends linear regression by adding polynomial and interaction features (e.g., \(x^2\), \(x_1x_2\)) to capture 
          curved relationships; typically paired with feature scaling and sometimes regularization to reduce overfitting.</li>
        <li><strong>Ridge Regression (L2 Regularization): Linear regression with a penalty for large coefficients to prevent overfitting.</strong></li>
        <li><strong>Lasso Regression (L1 Regularization): Similar to ridge, but can shrink some coefficients to zero, performing feature selection.</strong></li>
      </ul>
  </div>

  </section>


    <section id="linear-regression">
      
      <h4>2.1 Linear Regression</h4>
      <br>
      <b>Idea:</b>
      <p>
        Fits a straight line (in two dimensions) or a hyperplane (in higher dimensions) that best describes the relationship between 
        input features and the target variable. 
        The goal is to <b><font color="red">minimize the difference between the predicted values and the actual values</font></b>.
      </p>
    
      <b>Equation:</b>

      <p style="text-align: center;">
          y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + &hellip; + β<sub>p</sub>x<sub>p</sub> + ε
      </p>

    <p><strong>Where:</strong></p>
    <ul>
      <li><em>y</em> = predicted output</li>
      <li>β<sub>0</sub> = intercept (value of <em>y</em> when all <em>x</em> are zero). Baseline prediction when all features are zero (unpenalized)</li>
      <li>β<sub>1</sub>, β<sub>2</sub>, …, β<sub>p</sub> = coefficients (weights for each feature). Effect of feature \(x_j\) on \(y\) when others are held fixed.</li>
      <li><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub>p</sub> = input features</li>
      <li><strong>Sparsity</strong> – If \(\beta_j = 0\), feature \(x_j\) is effectively removed by the model.</li>
      <li><i>\( p \)</i> means the number of input feature.</li>
      <li><em>ε</em> = error term (difference between predicted and actual values)</li>
    </ul>

  <br>
    
  <center>
    <img src="images/regression1.png" width="800px" height="300px">
  </center>

  <h4>Interactive: Linear Regression</h4>
  <ul>
    <li><a href="https://xinyangmtsu.github.io/3350/LR_2D.html" target="_blank">2D Linear Regression: OLS Best-Fit Line</a></li>
    <li><a href="https://xinyangmtsu.github.io/3350/LR_3D.html" target="_blank">3D Linear Regression: OLS Best-Fit Plane</a></li>
  </ul>
      
      
    <br>
    
    <h4>How Linear Regression Works</h4>
    
    <!-- Put this once in your page (head or before </body>) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
<ol>
  <li><strong>Collect Data</strong> – Gather pairs of input features (\(x_1, x_2, \ldots, x_p\)) and the corresponding output (\(y\)).</li>
  <li><strong>Define the Model</strong> – Assume a linear relationship:
    \[
    \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p
    \]
    where the \(\beta\)'s are unknown parameters.</li>
  <li><strong>Estimate Parameters by Minimizing Error</strong> – Ordinary Least Squares (OLS) chooses the values of \(\beta_0, \beta_1, \ldots, \beta_p\) that minimize the sum of squared errors:
    \[
    \mathrm{SSE} = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
    \]
    This ensures the predictions \(\hat{y}_i\) are as close as possible to the actual values \(y_i\).</li>
</ol>
      
    <br>
    
  <h4>Model & Notation</h4>
  <p>Scalar form:</p>
  <p>$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon $$</p>

  <p>Matrix form:</p>
  <p>
    $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
    where
    $$ \mathbf{X} = 
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1p} \\
        1 & x_{21} & \cdots & x_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mp}
      \end{bmatrix},\quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
      \end{bmatrix},\quad
      \mathbf{y} =
      \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_m
      \end{bmatrix}.
    $$
  </p>


<h4>Objective Function</h4>

<b>What is an Objective Function?</b>
<ul>
  <li>In <strong>mathematics, optimization, and machine learning</strong>, the objective function is the function we want to 
      <strong>optimize</strong> (minimize or maximize).</li>
  <li>It represents the <em>goal</em> of the learning or fitting process.</li>
</ul>
<br>
<b>In Linear Regression:</b>
<ul>
  <li>The goal is to make predictions \(\hat{y}_i\) close to the true values \(y_i\).</li>
  <li>The <strong>Sum of Squared Errors (SSE)</strong> is defined as the objective function.</li>
</ul>

<p><b>Formula (scalar form):</b></p>
<p style="text-align: center;">
  \[
  \mathrm{SSE}(\beta) = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
  \]
</p>

<p><b>Where:</b></p>
<ul>
  <li>\(y_i\) = actual observed value</li>
  <li>\(\hat{y}_i\) = predicted value (model output)</li>
  <li>\(m\) = number of data points</li>
</ul>

<p><b>Vector/Matrix Notation:</b></p>
<p style="text-align: center;">
  \[
  \mathrm{SSE}(\beta) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
  \]
</p>

<p><b>Where:</b></p>
<ul>
  <li>\(\mathbf{y}\) = vector of observed values</li>
  <li>\(\mathbf{X}\) = design matrix (input features)</li>
  <li>\(\boldsymbol{\beta}\) = vector of coefficients (model parameters)</li>
</ul>

<p><b>Residual Vector Interpretation:</b></p>
<p>
If
\[
\mathbf{r} =
\begin{bmatrix}
  r_1 \\ r_2 \\ \vdots \\ r_m
\end{bmatrix},
\quad \text{then} \quad
\mathbf{r}^\top \mathbf{r} =
\begin{bmatrix}
  r_1 & r_2 & \dots & r_m
\end{bmatrix}
\begin{bmatrix}
  r_1 \\ r_2 \\ \vdots \\ r_m
\end{bmatrix}
= r_1^2 + r_2^2 + \cdots + r_m^2
\]
</p>

<p>This shows that \(\mathbf{r}^\top \mathbf{r}\) is exactly the <strong>Sum of Squared Errors (SSE)</strong>.</p>
      
 <!--   -->

    <p>Residual Vector:
      $$
      \mathbf{r} \;=\; \mathbf{y} \;-\; \mathbf{X}\,\boldsymbol{\beta}
      $$
    </p>

    <p>Then:
      $$
      \mathbf{r}^\top \mathbf{r}
      \;=\;
      (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
      \;=\;
      \sum_{i=1}^{m} \bigl(y_i - \hat{y}_i\bigr)^2
      $$
    </p>

    <br>


   <h4>Solving for \( \beta \): The Closed-Form Normal Equation Solution</h4>

<p>After defining the objective function (SSE), the next step is to solve for the coefficients 
\(\boldsymbol{\beta}\) that minimize this function. This is done using the 
<strong>Ordinary Least Squares (OLS)</strong> method.</p>

<p style="color: #ffb3de;"><b><u>Step 1: Express SSE in matrix form</u></b></p>
<p>
  $$
  \mathrm{SSE} = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
  $$
</p>

<p style="color: #ffb3de;"><b><u>Step 2: Expand the quadratic form</u></b></p>
<p>
  $$
  =\;\mathbf{y}^\top \mathbf{y}
  \;-\;\mathbf{y}^\top \mathbf{X}\boldsymbol{\beta}
  \;-\;(\mathbf{X}\boldsymbol{\beta})^\top \mathbf{y}
  \;+\;(\mathbf{X}\boldsymbol{\beta})^\top (\mathbf{X}\boldsymbol{\beta})
  $$
</p>

<p style="color: #ffb3de;"><b><u>Step 3: Simplify using transpose rules</u></b></p>
<ul>
  <li>Transpose rules:\((AB)^\top = B^\top A^\top\). So, we get:</li>

  $$
        (\mathbf{X}\boldsymbol{\beta})^\top\mathbf{y}
        \;=\; \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
  $$
  
  $$
  (\mathbf{X}\boldsymbol{\beta})^\top (\mathbf{X}\boldsymbol{\beta})
  = \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
  $$
  
  <li>A scalar equals its transpose. So, we get:</li> 
   
   $$
        \mathbf{y}^\top \mathbf{X} \boldsymbol{\beta}
        = (\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta})^\top
        = \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y}
    $$

<li>Therefore:</li>
      
<p>
   $$
        - \mathbf{y}^\top\mathbf{X}\boldsymbol{\beta} \;-\; (\mathbf{X}\boldsymbol{\beta})^\top\mathbf{y}
        \;=\; - \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} \;-\; \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
        \;=\; -2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
    $$
</p>

<li>Resulting SSE:</li>
<p>
  $$
  \mathrm{SSE}
  = \mathbf{y}^\top \mathbf{y}
  - 2\,\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y}
  + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
  $$
</p>
</ul>
      
<p style="color: #ffb3de;"><b><u>Step 4: Differentiate with respect to \(\boldsymbol{\beta}\)</u></b></p>

  <ul>

    <li>To minimize the SSE, we compute the derivative of the SSE with respect to \(\boldsymbol{\beta}\).
    Now, take the derivative with respect to \(\boldsymbol{\beta}\).</li>
    
    <li>Recall the first matrix calculus rule:</li>
    <ul>
      <li>\(\frac{\partial}{\partial \boldsymbol{\beta}} \; \mathbf{a}^\top \boldsymbol{\beta} = \mathbf{a}\)</li>
    </ul>
    
      <p><strong>The term is</strong> \( 2\,\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y} \). Here’s how to apply the rule:</p>

  <ol>
    <li>
      <strong>Treat the data as constants.</strong><br>
      In linear regression, \( \mathbf{X} \) and \( \mathbf{y} \) are fixed numbers. So define
      \( \mathbf{c} := \mathbf{X}^\top \mathbf{y} \). Then \( \mathbf{c} \) is a constant vector (same size as \( \boldsymbol{\beta} \)).
    </li>

    <li>
      <strong>Rewrite to match the rule.</strong><br>
      \[
        \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y}
        \;=\;
        \boldsymbol{\beta}^\top \mathbf{c}
        \;=\;
        \mathbf{c}^\top \boldsymbol{\beta}
      \]
      (For scalars, \( \mathbf{a}^\top \boldsymbol{\beta} = (\mathbf{a}^\top \boldsymbol{\beta})^\top = \boldsymbol{\beta}^\top \mathbf{a} \))
    </li>

    <li>
      <strong>Apply the rule.</strong><br>
      \[
        \frac{\partial}{\partial \boldsymbol{\beta}}\bigl(2\,\mathbf{c}^\top \boldsymbol{\beta}\bigr)
        \;=\; 2\,\mathbf{c}
        \;=\; 2\,\mathbf{X}^\top \mathbf{y}
      \]
    </li>
  </ol>

  <p><strong>So:</strong></p>
  <p style="margin-left:8px;">
    \[
      \frac{\partial}{\partial \boldsymbol{\beta}}
      \bigl(2\,\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y}\bigr)
      \;=\; 2\,\mathbf{X}^\top \mathbf{y}
    \]
  </p> 
    
    
    <li>Recall the second matrix calculus rule:</li>
    <ul>
    <li>\(\frac{\partial}{\partial \boldsymbol{\beta}} \; \boldsymbol{\beta}^\top A \boldsymbol{\beta} = (A + A^\top)\boldsymbol{\beta}\). If \(A\) is symmetric, this simplifies to \(2A\boldsymbol{\beta}\).</li>
    </ul>
    
    <p><strong>The term is</strong>\( \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta} \), since \(\mathbf{X}^\top \mathbf{X}\) is always symmetric
(because \((\mathbf{X}^\top \mathbf{X})^\top = \mathbf{X}^\top (\mathbf{X}^\top)^\top = \mathbf{X}^\top \mathbf{X}\)),
we can apply the symmetric case directly. </p>

    <p><strong>So:</strong></p>
    
    <p style="margin-left:8px;">
    \[ \frac{\partial}{\partial \boldsymbol{\beta}}
\bigl(\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}\bigr)
=  (\mathbf{X}^\top \mathbf{X} + (\mathbf{X}^\top \mathbf{X})^\top)\boldsymbol{\beta}
= 2\,\mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
    \]
  </p>     

    <li>Therefore:</li>
    <p>
  $$
  \frac{\partial \,\mathrm{SSE}}{\partial \boldsymbol{\beta}}
  = -2\,\mathbf{X}^\top \mathbf{y}
  + 2\,\mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
  $$
</p>
    
  </ul>

<p style="color: #ffb3de;"><b><u>Step 5: Set gradient = 0 and solve</u></b></p>
<p>
  $$
  -2\,\mathbf{X}^\top \mathbf{y} + 2\,\mathbf{X}^\top \mathbf{X}\,\hat{\boldsymbol{\beta}} \;=\; \mathbf{0}
  $$
  $$
   \;\;\Longrightarrow\;\;
  $$
  $$
  \mathbf{X}^\top \mathbf{X}\,\hat{\boldsymbol{\beta}} = \mathbf{X}^\top \mathbf{y}
  $$
</p>

<p style="color: #ffb3de;"><b><u>Step 6: Closed-form solution (Normal Equation)</u></b></p>
<p>
  $$
  \boxed{
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1}\,\mathbf{X}^\top \mathbf{y}
  }
  $$
</p>

<p><b>Where:</b></p>
<ul>
  <li>\(\hat{\boldsymbol{\beta}}\): estimated coefficients (parameters)</li>
  <li>\(\mathbf{X}^\top \mathbf{X}\): feature covariance matrix</li>
  <li>\(\mathbf{X}^\top \mathbf{y}\): correlation between features and target</li>
</ul>

<h4>Key Points:</h4>
<ul>
  <li>The <strong>Normal Equation</strong> provides a closed-form solution for the coefficients that minimize SSE.</li>
  <li>Computing the inverse of \(\mathbf{X}^\top \mathbf{X}\) can be expensive for large datasets.
  For very large datasets, more efficient methods like gradient descent are often used.</li>
  <li>
  <strong>Don’t compute</strong> <code>(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y</code> yourself.<br>
  Use <code>sklearn.linear_model.LinearRegression</code> instead — it solves the same OLS problem with a
  numerically stable least-squares solver (SVD/QR), handles the intercept correctly, and avoids fragile matrix inverses.
  </li>
</ul>     
<br>
<!-- -->


<h4>Worked Example: Predicting House Price</h4>

<p><b>Model:</b></p>
<p>
  $$ \text{Price} = \beta_0 + \beta_1 \cdot \text{Size} + \beta_2 \cdot \text{Bedrooms} + \epsilon $$
</p>

<p style="color: #ffb3de;"><b><u>Step 1: Data</u></b></p>
<center>  
<table border="1" cellpadding="6">
  <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
  <tr><td>1000</td><td>2</td><td>200,000</td></tr>
  <tr><td>1500</td><td>3</td><td>280,000</td></tr>
  <tr><td>2000</td><td>3</td><td>340,000</td></tr>
  <tr><td>2500</td><td>4</td><td>400,000</td></tr>
  <tr><td>3000</td><td>4</td><td>460,000</td></tr>
</table>
</center>  

<p style="color: #ffb3de;"><b><u>Step 2: Build Matrices \( \mathbf{X} \) and \( \mathbf{y} \)</u></b></p>
<p>
  $$
  \mathbf{X} =
  \begin{bmatrix}
    1 & 1000 & 2 \\
    1 & 1500 & 3 \\
    1 & 2000 & 3 \\
    1 & 2500 & 4 \\
    1 & 3000 & 4
  \end{bmatrix}, \quad
  \mathbf{y} =
  \begin{bmatrix}
    200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
  \end{bmatrix}.
  $$
</p>

<p style="color: #ffb3de;"><b><u>Step 3: Compute \( \mathbf{X}^\top \mathbf{X} \) and \( \mathbf{X}^\top \mathbf{y} \)</u></b></p>
<ul>
  <li>\( \mathbf{X}^\top \mathbf{X} \): feature covariance matrix</li>
  <li>\( \mathbf{X}^\top \mathbf{y} \): feature–target correlation</li>
</ul>

<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
    5 & 10000 & 16 \\
    10000 & 22500000 & 34500 \\
    16 & 34500 & 54
  \end{bmatrix}, \quad
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
    1680000 \\ 3680000000 \\ 5700000
  \end{bmatrix}.
  $$
</p>

<p style="color: #ffb3de;"><b><u>Step 4: Solve the Normal Equation</u></b></p>
<p>
  $$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
</p>

<p>
After computing the inverse and multiplying, we get:
</p>
<p>
  $$ \hat{\beta}_0 = 64{,}000, \quad \hat{\beta}_1 \approx 114.67, \quad \hat{\beta}_2 \approx 13{,}333.33 $$
</p>
 
<p style="color: #ffb3de;"><b><u>Step 5: Final Model</u></b></p>
<p style="text-align:center; font-size:1.1em;">
  $$ \widehat{\text{Price}} = 64{,}000 + 114.67 \times \text{Size} + 13{,}333.33 \times \text{Bedrooms} $$
</p>

<b>Interpret Coefficients:</b>
<ul>
  <li>\(\beta_0\): baseline house price (anchor point).</li>
  <li>\(\beta_1\): each additional sq.ft adds about \$114.67.</li>
  <li>\(\beta_2\): each additional bedroom adds about \$13,333.33.</li>
</ul>

<p style="color: #ffb3de;"><b><u>Step 6: Predictions \((\hat{\mathbf{y}})\) & Errors</u></b></p>
<p>
  $$
  \hat{\mathbf{y}} =
  \mathbf{X}\boldsymbol{\hat{\beta}} \approx
  \begin{bmatrix}
    205{,}333.33 \\ 276{,}000.00 \\ 333{,}333.33 \\ 404{,}000.00 \\ 461{,}333.33
  \end{bmatrix}, \quad
  \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
  \begin{bmatrix}
    -5333.33 \\ 4000.00 \\ 6666.67 \\ -4000.00 \\ -1333.33
  \end{bmatrix}.
  $$
</p>

<p id="step-7" style="color: #ffb3de;"><b><u>Step 7: Model Evaluation — SSE, MSE, RMSE, and \(R^2\)</u></b></p>
<p>
The performance of a regression model can be evaluated using several metrics. 
</p>

<ul>
  <li><b>Sum of Squared Errors (SSE)</b>:</li>
  <p>
  $$
    \mathrm{SSE} = \sum_{i=1}^{m}(y_i - \hat{y}_i)^2 \;\approx\; 1.0667 \times 10^8
  $$
</p>
  <p>
A large SSE doesn’t necessarily mean the model is poor—it simply reflects that 
we are predicting large values (house prices). To make the error easier to interpret, 
we scale SSE into other metrics: 
</p>
  
<li><b>Mean Squared Error (MSE):</b> the average squared error per data point</li>
   $$
  \text{MSE} = \frac{\text{SSE}}{m} 
  = \frac{1}{m} \sum_{i=1}^{m}(y_i - \hat{y}_i)^2 \approx 21{,}333{,}333.33
  $$

  <li><b>Root Mean Squared Error (RMSE):</b> the square root of MSE, expressed in the same units as the target variable</li>

    $$
  \text{RMSE} = \sqrt{\text{MSE}} \approx 4618.80
  $$

  <p>
These values tell us that, on average, predictions are off by about \$4,600. 
Smaller MSE or RMSE values indicate better model performance.
</p>

  <li><b>Coefficient of Determination (\(R^2\))</b></li>
   $$
  R^2 \;=\; 1 \;-\; 
  \frac{\sum_{i=1}^{m}\bigl(y_i - \hat{y}_i\bigr)^2}
       {\sum_{i=1}^{m}\bigl(y_i - \bar{y}\bigr)^2} \approx 0.9974
  $$

  <p>
The \(R^2\) score measures the proportion of variance in the target variable 
explained by the model:
</p>

  <ul>
  <li><strong>\( R^2 \) = 1.0</strong> — Perfect fit: model explains 100% of the variance in \(y\).</li>
  <li><strong>\( R^2 \) = 0.0</strong> — Model explains 0% of the variance: as good as predicting the mean.</li>
  <li><strong>\( R^2 \) &lt; 0</strong> — Model is worse than predicting the mean: poor predictive performance.</li>
</ul>
  
</ul>
  
<br>

<h4>Note on Feature Scaling</h4>
<p>
In Chapter 1, we emphasized the importance of <b>feature scaling</b>. 
Scaling helps when features are measured on very different ranges, 
because it ensures that no single feature dominates the learning process. 
For example, <i>Size</i> is in the thousands (sq.ft), while <i>Bedrooms</i> 
is a single-digit number.
</p>

<p>
In this worked example, we skipped scaling because the goal is to 
<b>demonstrate the mathematics of Ordinary Least Squares (OLS)</b> 
using the Normal Equation. OLS works without scaling, since the closed-form 
solution can handle features of different magnitudes. 
</p>

<p><b>However, standardization is recommended when:</b></p>
<ul>
  <li>You use <b>gradient descent</b> to fit the model (scaling improves convergence speed).</li>
  <li>You want to <b>compare coefficients</b> across features (scaling puts them on the same scale).</li>
  <li>You apply <b>regularization methods</b> such as Ridge or Lasso (scaling ensures fair penalization).</li>
</ul>
<br>
      
</section>
  
<!-- MathJax loader (optional, only needed for the \(y\) math) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
<br>  
<!-- -->

<section id="polynomial-regression">
      
  <h4>2.2 Polynomial Regression</h4>
  <br>
  <b>Idea:</b>
  <p>
    Polynomial Regression is an extension of Linear Regression that models 
    a nonlinear relationship between input features and the target variable 
    by introducing polynomial terms (squared, cubic, etc.). 
    Although the curve looks nonlinear in the original input space, 
    <b><font color="red">it is still a linear model in the expanded feature space</font></b>.
  </p>
    
  <b>Equation (1-D, degree d):</b>
  <p style="text-align: center;">
    y = β<sub>0</sub> + β<sub>1</sub>x + β<sub>2</sub>x<sup>2</sup> + … + β<sub>d</sub>x<sup>d</sup> + ε
  </p>

  <b>Equation (multi-feature, degree 2 example):</b>
  <p style="text-align: center;">
    y = β<sub>0</sub> 
      + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> 
      + β<sub>3</sub>x<sub>1</sub><sup>2</sup> + β<sub>4</sub>x<sub>2</sub><sup>2</sup> 
      + β<sub>5</sub>x<sub>1</sub>x<sub>2</sub> + ε
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li><em>y</em> = predicted output.</li>
    <li>β<sub>0</sub> = intercept (baseline prediction when all features are zero).</li>
    <li>β’s = coefficients learned from data (one for each polynomial/interaction term).</li>
    <li><em>x</em><sub>j</sub> = original input features; we also include powers like x<sub>j</sub><sup>2</sup>, x<sub>j</sub><sup>3</sup>, … and interactions x<sub>j</sub>x<sub>k</sub>.</li>
    <li><i>d</i> = polynomial degree (hyperparameter controlling model flexibility).</li>
    <li><em>ε</em> = error term (difference between predicted and actual values).</li>
    <li>Higher-degree terms allow the model to capture curves and feature interactions</li>
    <li>Still solved by Ordinary Least Squares (OLS) after expanding the feature set</li>
  </ul>
      
  <br>
    
  <h4>How Polynomial Regression Works</h4>
  <ol>
    <li><strong>Collect Data</strong> – Pairs of inputs (\(x\)) and outputs (\(y\)).</li>
    <li><strong>Feature Expansion</strong> – Transform the input feature into higher-order terms:
      \[
      x \;\;\longrightarrow\;\; [\,x,\,x^2,\,x^3,\,\ldots,\,x^d\,]
      \]
      or, for multiple features, include all monomials up to degree \(d\).
    </li>
    <li><strong>Fit a Linear Model</strong> – Apply OLS on the expanded features to estimate coefficients \(\beta\).</li>
    <li><strong>Prediction</strong> – Use the polynomial equation to generate predictions \(\hat{y}\).</li>
  </ol>

  <br>

  <h4>Matrix Form</h4>
  <p>
    With polynomial expansion, the design matrix \( \mathbf{X} \) changes. 
    For degree 3 with one feature:
  </p>

  <p style="text-align:center;">
  $$
  \mathbf{X} =
  \begin{bmatrix}
    1 & x_1 & x_1^2 & x_1^3 \\
    1 & x_2 & x_2^2 & x_2^3 \\
    \vdots & \vdots & \vdots & \vdots \\
    1 & x_m & x_m^2 & x_m^3
  \end{bmatrix}, \quad
  \mathbf{y} =
  \begin{bmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_m
  \end{bmatrix}.
  $$
  </p>

  <p>
  The solution is still obtained via the Normal Equation:
  \[
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}
  \]
  </p>

  <br>

  <h4>Worked Example: Predicting House Price (Nonlinear)</h4>
  <p>
    Suppose house price depends not only linearly on <i>Size</i>, but also 
    shows curvature (e.g., very large houses may not increase proportionally in price). 
    A polynomial term like <i>Size²</i> can capture this effect.
  </p>

  <p style="color: #ffb3de;"><b><u>Model:</u></b></p>
  <p style="text-align:center;">
    $$ \text{Price} = \beta_0 + \beta_1 \cdot \text{Size} + \beta_2 \cdot \text{Size}^2 + \epsilon $$
  </p>

  <p>
    Expanding features in this way lets the model fit a curve rather than a straight line, 
    improving flexibility at the cost of interpretability.
  </p>

  <br>

  <h4>Key Points:</h4>
  <ul>
    <li>Polynomial regression is still a linear model in the coefficients.</li>
    <li>Increasing degree \(d\) makes the model more flexible, but risks <b>overfitting</b>.</li>
    <li>Use cross-validation to choose an appropriate degree.</li>
    <li>For high-dimensional data, prefer regularization (Ridge, Lasso) to control complexity.</li>
  </ul>

</section>


<!-- -->
  

<section id="ridge-regression">
    
      <h4>2.3 Ridge Regression</h4>

    <b>Idea:</b>
<p>
   
    Ridge Regression extends Ordinary Least Squares (OLS) by adding an <b><font color="red">L2 penalty</font></b> on the size of the coefficients. 
    This penalty discourages overly large weights, helping to reduce <b>overfitting</b> and improve <b>generalization</b>, 
    especially when features are correlated or when the number of features \( p \) is close to or greater than the number of training samples \( m \).
    <br><br>
    In Ridge Regression, a penalty is an extra cost you add to the training objective for using large coefficient values. 
    It’s also called <b><font color="red">regularization</font></b>. By penalizing large coefficients, L2 regularization reduces reliance on single features and improves generalization.
    <br><br>

    <h4>When to use Regularization:</h4>
    <ul>
      <li>Preventing Overfitting</li>
      <li>High Dimensionality (\(p &gg; m\))</li>
      <li>Multicollinearity (features are highly correlated)</li>
    </ul>

    
  <br><br>

    Ridge Regression (L2-regularized linear regression) fits a hyperplane like OLS but adds a penalty on the size of the coefficients to reduce overfitting and handle multicollinearity.
    The goal is to <b><font color="red">minimize the sum squared error while shrinking coefficients</font></b>.
</p>

<b>Equation:</b>
<p style="text-align: center;">
  \(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}\), &nbsp; and we choose \(\boldsymbol{\beta}\) to minimize
  \[
    \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^{p} \beta_j^2
  \]
 
</p>

    <p style="text-align:center;">
       
    \[
      \;\;\Longrightarrow\;\;
      
      \underbrace{\sum_{i=1}^{m}\bigl(y_i - \hat{y}_i\bigr)^2}_{\text{SSE}}
      \;+\;
      \underbrace{\lambda \sum_{j=1}^{p} \beta_j^2}_{\text{L2 penalty}}
    \]
       
  </p>

   <p style="text-align:center;">
    \[

    \;\;\Longrightarrow\;\;
    
    \underbrace{\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2}_{\text{sum of squared residuals}}
      \;+\;
      \underbrace{\lambda \|\boldsymbol{\beta}_{1:p}\|_2^2}_{\text{sum of squared coefficients}}
     
  \]

     (note: the intercept \(\beta_0\) is typically <i>not</i> penalized).
  </p>

<p><strong>Where:</strong></p>
<ul>
  <li>\( \lambda \ge 0 \): &lambda; is the regularization strength parameter, 
    it controls the amount of regularization applied</li>
  <li>\(  \lambda = 0 &rArr; Ridge = OLS \) (no penalty) </li>
  <li> Small &lambda; &rArr; weak shrinkage &rArr; coefficients close to OLS values </li>
  <li> Large &lambda; &rArr; strong shrinkage &rArr; coefficients shrink toward zero, coefficients much smaller, model more regularized. </li>
  <li>\( \beta_1, \dots, \beta_p \) = coefficients for features</li>
  <li>\( \mathbf{X} \) = design matrix </li>
  <li>\( \mathbf{y} \) = targets</li>
  <li>\( p \) means the number of input feature </li>
</ul>

<br>


<h4 style="color:#ff79c6; margin-top:0;">What does <span style="white-space:nowrap;">\(\|\cdot\|_2\)</span> mean?</h4>

<p>
    The notation <strong>\(\|\cdot\|_2\)</strong> means the <b><font color="red">L2 norm</font></b> (also called the
    <b><font color="red">Euclidean norm</font></b> or <b>2-norm</b>).
  </p>

  <ul>
    <li>
      <strong>Double bars</strong> <span style="white-space:nowrap;">\(\|\cdot\|\)</span> denote a
      <em>norm</em> in general.
    </li>
    <li>
      The <strong>subscript 2</strong> indicates the <em>L2 norm</em>.
    </li>
  </ul>

  <p><strong>Definition:</strong></p>
  <p style="margin-left:1rem;">
    For a vector \(\mathbf{v} = (v_1,\dots,v_n)\), the L2 norm is
  </p>
  <p style="text-align:center;">
    \[
      \|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}
    \]
  </p>

  <p><strong>Squared L2 norm (sum of squares):</strong></p>
  <p style="text-align:center;">
    \[
      \|\mathbf{v}\|_2^2 = \sum_{i=1}^{n} v_i^2
    \]
  </p>

<br>

<h4>Ridge Shrinkage Illustration</h4>

<svg viewBox="0 0 800 480" role="img" aria-labelledby="ridgeTitle ridgeDesc"
     style="max-width:100%;height:auto;background:#1e1f22;border-radius:12px;box-shadow:0 4px 24px rgba(255,121,198,.12)">
  <title id="ridgeTitle">Ridge Shrinkage Illustration</title>
  <desc id="ridgeDesc">
    Axes for beta1 and beta2, a circular L2 constraint ball centered at the origin, OLS-centered SSE ellipses,
    and the ridge solution at the tangency point between an ellipse and the L2 circle.
  </desc>

  <defs>
    <marker id="arrow-pink" viewBox="0 0 10 10" refX="8" refY="5"
            markerWidth="7" markerHeight="7" orient="auto-start-reverse">
      <path d="M0 0 L10 5 L0 10 z" fill="#ff79c6"></path>
    </marker>
    <style>
      .axis { stroke:#ff79c6; stroke-width:2; marker-end:url(#arrow-pink); }
      .axisLabel { fill:#ffb3de; font: 500 13px Roboto, Arial, sans-serif; }
      .note { fill:#ffb3de; font: 500 12px Roboto, Arial, sans-serif; }
      .label { fill:#ff79c6; font: 600 13px Roboto, Arial, sans-serif; }
      .ellipse { fill:none; stroke:#8be9fd; stroke-width:3; }
      .ellipse--wide { fill:none; stroke:#8be9fd; stroke-opacity:.45; stroke-width:2; stroke-dasharray:6 6; }
      .l2 { fill:none; stroke:#ff79c6; stroke-width:3; }
      .guide { stroke:#9aa0a6; stroke-width:1.5; stroke-dasharray:6 6; }
    </style>
  </defs>

  <!-- Axes -->
  <g id="coords">
    <line x1="60"  y1="240" x2="760" y2="240" class="axis" />
    <line x1="400" y1="440" x2="400" y2="40"  class="axis" />
    <text x="752" y="226" class="axisLabel">β₁</text>
    <text x="410" y="54"  class="axisLabel">β₂</text>
    <circle cx="400" cy="240" r="2.5" fill="#ff79c6"/>
    <text x="408" y="254" class="note">origin</text>
  </g>

  <!-- Smaller L2 ball (reduced radius to 70) -->
  <g id="l2-ball">
    <circle cx="400" cy="240" r="70" class="l2"/>
    <text x="210" y="356" class="label">L2 constraint: ∥β∥₂² ≤ t</text>
  </g>

  <!-- OLS (unregularized) -->
  <g id="ols">
    <text x="560" y="160" text-anchor="middle" dominant-baseline="middle"
          style="font:700 20px/1 Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="572" y="180" class="label">OLS (unregularized)</text>
  </g>

  <!-- Shrinkage direction (origin → OLS) -->
  <g id="shrink">
    <line x1="400" y1="240" x2="560" y2="160" class="guide"/>
    <text x="492" y="206" class="note">shrinkage direction</text>
  </g>

  <!-- SSE contours (original static ones) -->
  <g id="sse">
    <ellipse cx="560" cy="160" rx="110" ry="72" class="ellipse"
             transform="rotate(-25 560 160)"/>
    <ellipse cx="560" cy="160" rx="150" ry="98" class="ellipse--wide"
             transform="rotate(-25 560 160)"/>
    <ellipse cx="560" cy="160" rx="190" ry="124" class="ellipse--wide"
             transform="rotate(-25 560 160)"/>
    <text x="600" y="90" class="label">SSE contours</text>
  </g>

  <!-- NEW: Animated SSE contour that EXPANDS from tiny to first-touch (tangency) -->
  <g id="sse-anim" transform="rotate(-25 560 160)">
    <!-- Start small near the OLS center (cx=560, cy=160), grow to rx=110, ry=72 -->
    <ellipse id="expandEllipse" cx="560" cy="160" rx="5" ry="3" class="ellipse">
      <animate attributeName="rx" from="5" to="110" dur="2.8s" begin="0s" fill="freeze" />
      <animate attributeName="ry" from="3" to="72"  dur="2.8s" begin="0s" fill="freeze" />
    </ellipse>
  </g>

  <!-- Ridge point (moved inward to sit on circle radius 70) -->
  <g id="ridge">
    <circle cx="459.6" cy="210.2" r="5.5" fill="#50fa7b" stroke="#1e1f22" stroke-width="1.5">
      <!-- Fade-in right when the expanding contour reaches tangency -->
      <animate attributeName="opacity" from="0" to="1" dur="0.5s" begin="2.8s" fill="freeze"/>
    </circle>
    <text x="474" y="228" class="label" style="fill:#50fa7b; opacity:0;">
      Ridge solution (tangent)
      <animate attributeName="opacity" from="0" to="1" dur="0.5s" begin="2.8s" fill="freeze"/>
    </text>
  </g>

  <!-- Legend -->
  <g id="legend">
    <rect x="72" y="64" width="215" height="88" rx="10" ry="10"
          fill="rgba(33,17,27,.75)" stroke="#402138"/>
    <circle cx="92" cy="88" r="7" fill="none" stroke="#ff79c6" stroke-width="3"/>
    <text x="110" y="92" class="note">L2 constraint ball</text>

    <line x1="85" y1="114" x2="99" y2="114" class="ellipse"/>
    <text x="110" y="118" class="note">SSE (loss) contour</text>

    <text x="88" y="140" style="font:700 16px Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="110" y="142" class="note">OLS estimate</text>
  </g>
</svg>
  
<br>

<h4>How Ridge Regression Works</h4>
<ol>
  <li><strong>Collect Data</strong> – Gather features \((x_1,\dots,x_p)\) and outputs \(y\).</li>
  <li><strong>(Recommended) Standardize Features</strong> – Put features on comparable scales so the penalty treats them fairly.</li>
  <li><strong>Choose \(\lambda\)</strong> – Typically via cross-validation.</li>
  <li><strong>Fit the Model</strong> – Minimize squared error + L2 penalty to get \(\hat{\boldsymbol{\beta}}\).</li>
  <li><strong>Evaluate</strong> – Check train/validation error; tune \(\lambda\) to balance bias–variance.</li>
</ol>

<h4>Model & Notation</h4>
<p><b>Scalar form:</b></p>
<p>
  $$
   y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \epsilon 
  $$
</p>

<p><b>Matrix form:</b></p>
<p>
  $$
   \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
  $$
   where

  $$
   \mathbf{X} =
  \begin{bmatrix}
    1 & x_{11} & \cdots & x_{1p} \\
    1 & x_{21} & \cdots & x_{2p} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{m1} & \cdots & x_{mp}
  \end{bmatrix},\;
  \boldsymbol{\beta} =
  \begin{bmatrix}
    \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
  \end{bmatrix},\;
  \mathbf{y} =
  \begin{bmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_m
  \end{bmatrix}.
  $$
</p>

<h4>Objective Function (Ridge / L2)</h4>
<p>
  Ridge minimizes
  \[
    \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^{p}\beta_j^2
    \;=\;
    \|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2 \;+\; \lambda\,\|\boldsymbol{\beta}_{1:p}\|_2^2
  \]
  
In vector form with a penalty matrix
  \( \mathbf{P} = \mathrm{diag}(0,1,\dots,1) \) (no penalty on \( \beta_0 \))
  , 
  \( \mathbf{P} \in \mathbb{R}^{(p+1)\times(p+1)}   \text{is a diagonal matrix whose diagonal entries are } [0,\,1,\,1,\,\dots,\,1] \)
  
  
  
  \[
    \min_{\boldsymbol{\beta}} \; (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
    \;+\; \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
  \]
</p>


  <p>
  \( \beta^\top P \beta \) is a quadratic form.
  Since \( P \) is diagonal, multiplying out gives:
</p>

<p>
  \[
    \beta^\top P \beta
    = 0 \cdot \beta_0^2
    + 1 \cdot \beta_1^2
    + 1 \cdot \beta_2^2
    + \dots
    + 1 \cdot \beta_p^2
  \]
</p>

<p>
  That’s exactly:
</p>

<p>
  \[
    \sum_{j=1}^p \beta_j^2
  \]
</p>
  
<p><b>Objective (penalized least squares):</b></p>
  <p style="text-align:center;">
    \[
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      \;=\;
      \arg\min_{\boldsymbol{\beta}}
      \left(
        \|\,\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\,\|_2^2
        \;+\;
        \lambda \|\boldsymbol{\beta}\|_2^2
      \right)
    \]
  </p>
  
<h4>Normal Equation (Ridge)</h4>
 <div class="step">
    <p><b>Expand the objective:</b></p>
    <p>
      $$
      J(\boldsymbol{\beta})
      = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">

    <p><b>Distribute (FOIL) the SSE part and use transpose rules:</b></p>

<p>
  $$
  (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
  = \underbrace{\mathbf{y}^\top\mathbf{y}}_{\text{First}}
    - \underbrace{\mathbf{y}^\top\mathbf{X}\boldsymbol{\beta}}_{\text{Outer}}
    - \underbrace{\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}}_{\text{Inner}}
    + \underbrace{\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}}_{\text{Last}}
  $$
</p>

<p>
  Using the fact that the Outer and Inner terms are equal scalars, we combine them:
</p>

<p>
  $$
  = \mathbf{y}^\top\mathbf{y}
    - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
    + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}.
  $$
</p>
    
    <p>So
      $$
      J(\boldsymbol{\beta}) =
      \mathbf{y}^\top\mathbf{y}
      - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
      + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">
    <p><b>Take gradient w.r.t. \( \boldsymbol{\beta} \) and set to zero:</b></p>
    <p>
      $$
      \frac{\partial J}{\partial \boldsymbol{\beta}}
      = -2\,\mathbf{X}^\top\mathbf{y}
        + 2\,\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
        + 2\lambda\,\mathbf{P}\,\boldsymbol{\beta}
      \;=\; \mathbf{0}.
      $$
    </p>
    <p>
      Rearranging:
      $$
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})\,\hat{\boldsymbol{\beta}}
      = \mathbf{X}^\top\mathbf{y}.
      $$
    </p>
    <p><b>Closed form (if invertible):</b>
      $$
      \boxed{\;
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      =
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
      \;}
      $$
    </p>
  </div>

<br>

<hr>

<h4>Worked Example: Predicting House Price (Ridge)</h4>
<p>Use the same data as in 2.1 and let \( \lambda = 100 \). We do <b>not</b> penalize the intercept, so
\(\mathbf{P}=\mathrm{diag}(0,1,1)\).</p>

<h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
<p>
  $$
  \mathbf{X} =
  \begin{bmatrix}
    1 & 1000 & 2 \\
    1 & 1500 & 3 \\
    1 & 2000 & 3 \\
    1 & 2500 & 4 \\
    1 & 3000 & 4
  \end{bmatrix},\quad
  \mathbf{y} =
  \begin{bmatrix}
    200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
  \end{bmatrix}.
  $$
</p>

<h4>Step 2: Compute \( \mathbf{X}^\top \mathbf{X} + \lambda \mathbf{P} \) and \( \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  From the ridge regression section, 
  $$ 
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 & 34500 \\
  16 & 34500 & 54
  \end{bmatrix}, 
  
  \quad
  
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1680000 \\ 3680000000 \\ 5700000
  \end{bmatrix}.
  $$
</p>
<p>
  With \( \lambda = 100 \) and 
  
  \( \mathbf{P}=
   \begin{bmatrix}
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
  \):
</p>
<p>
  \[
  \mathbf{X}^\top \mathbf{X} + \lambda \mathbf{P} \;=\;
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 + 100 & 34500 \\
  16 & 34500 & 54 + 100
  \end{bmatrix}
  =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500010 & 34500 \\
  16 & 34500 & 154
  \end{bmatrix}.
  \]
</p>

<h4>Step 3: Solve for \( \hat{\boldsymbol{\beta}}_{\text{ridge}} \)</h4>
<p>
  \[
    \hat{\boldsymbol{\beta}}_{\text{ridge}}
    = (\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
    \;\approx\;
    \begin{bmatrix}
      79,962.23 \\
      127.95 \\
      40.01
    \end{bmatrix}.
  \]
</p>

<h4>Final Ridge Model (\(\lambda=100\))</h4>
<p style="text-align:center; font-size:1.1em;">
  \( \widehat{\text{Price}} = 79{,}962.23 \;+\; 127.95 \times \text{Size} \;+\; 40.01 \times \text{Bedrooms} \)
</p>

<h4>Step 4: Predictions \((\hat{\mathbf{y}})\) & Errors</h4>
<p>
  $$
  \hat{\mathbf{y}} \approx
  \begin{bmatrix}
    207,997.12 \\
    272,014.56 \\
    335,992.00 \\
    400,009.44 \\
    463,986.88
  \end{bmatrix},
  
  \quad
  
  \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
  \begin{bmatrix}
    -7,997.12 \\
    7,985.44 \\
    4,008.00 \\
    -9.44 \\
    -3,986.88
  \end{bmatrix}.
  $$
</p>

<h4>Step 5: Training Error (for reference)</h4>
<p>
  $$
    \mathrm{SSE} \approx 1.5968 \times 10^8,\quad
    \mathrm{MSE} \approx 3.1936 \times 10^7,\quad
    \mathrm{RMSE} \approx 5{,}651.20.
  $$
</p>

<h4>Key Points</h4>
<ul>
  <li>Setting \(\lambda=0\) recovers OLS; larger \(\lambda\) shrinks coefficients (reduces variance, increases bias).</li>
  <li>Ridge helps when features are highly correlated (\(\mathbf{X}^\top\mathbf{X}\) is ill-conditioned).</li>
  <li>Do not penalize the intercept; standardize features before ridge to make penalty comparable across features.</li>
  <li>Choose \(\lambda\) via cross-validation for best generalization.</li>
  <li>Noisy features --> shrunk, don’t dominate the model.</li>
  <li>Redundant (correlated) features --> it shrinks both correlated features toward smaller, more balanced values.</li>
</ul>
<br>
  
<h4>Linear Regression vs. Ridge Regression</h4>
  
<table style="width:100%; border-collapse:collapse; font-family:Arial, sans-serif; margin-top:12px;">
  <thead>
    <tr>
      <th style="border:1px solid #ccc; padding:10px; background:#2a1e28; color:#ffb3de; text-align:left; width:50%;">Linear Regression</th>
      <th style="border:1px solid #ccc; padding:10px; background:#2a1e28; color:#ffb3de; text-align:left; width:50%;">Ridge Regression</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border:1px solid #ccc; padding:10px;">
        <ul style="margin:0; padding-left:18px;">
          <li>Works well when you have <b>more observations than features</b></li>
          <li>Assumes features are <b>not highly correlated</b></li>
          <li>Provides the <b>most interpretable coefficients</b></li>
          <li>Prone to <b>Overfitting</b></li>
        </ul>
      </td>
      <td style="border:1px solid #ccc; padding:10px;">
        <ul style="margin:0; padding-left:18px;">
          <li>Better with <b>multicollinearity</b> (highly correlated features)</li>
          <li>Useful when you have <b>many features relative to observations</b></li>
          <li>Reduces <b>overfitting</b> by shrinking coefficients</li>
          <li>Gives <b>more stable and generalizable predictions</b></li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

  
  </section>

  <section id="lasso-regression">
    
      <h4>2.4 Lasso Regression</h4>

<b>Idea:</b>

    <p>
    Lasso Regression extends Ordinary Least Squares (OLS) by adding an 
    <b><font color="red">L1 penalty</font></b> on the absolute values of the coefficients. 
    This penalty encourages sparsity in the model — that is, it tends to drive some coefficients 
    exactly to zero, effectively performing <b>feature selection</b> while fitting the model.
    <br><br>
    In Lasso Regression, the penalty is the sum of the absolute values of the coefficients 
    (excluding the intercept). By penalizing the magnitude of coefficients, L1 regularization 
    both reduces overfitting and simplifies the model by eliminating less important features.
  </p>

  <h4>When to use Regularization:</h4>
  <ul>
    <li>Preventing Overfitting</li>
    <li>High Dimensionality (\(p \gg m\))</li>
    <li>When you suspect many features are irrelevant or redundant</li>
    <li>For automatic feature selection (Lasso can set coefficients to zero)</li>
  </ul>

 <p>
  Lasso Regression (L1-regularized linear regression) fits a hyperplane like OLS but adds a penalty on the
  <em>absolute size</em> of coefficients to prevent overfitting and to perform <b>feature selection</b>.
  The goal is to <b><font color="red">minimize the squared error while pushing unimportant coefficients exactly to zero</font></b>
  (sparse model).
</p>

  <b>Equation:</b>
  <p style="text-align:center;">
    \(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}\), &nbsp; and we choose \(\boldsymbol{\beta}\) to minimize:
    \[
      \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^{p} |\beta_j|
    \]
  </p>

  <p style="text-align:center;">
    \[
      \;\;\Longrightarrow\;\;
      \underbrace{\sum_{i=1}^{m} \bigl(y_i - \hat{y}_i\bigr)^2}_{\text{SSE}}
      \;+\;
      \underbrace{\lambda \sum_{j=1}^{p} |\beta_j|}_{\text{L1 penalty}}
    \]
  </p>

  <p style="text-align:center;">
    \[
      \;\;\Longrightarrow\;\;
      \underbrace{\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2}_{\text{sum of squared residuals}}
      \;+\;
      \underbrace{\lambda \|\boldsymbol{\beta}_{1:p}\|_1}_{\text{sum of absolute coefficients}}
    \]
    (note: the intercept \(\beta_0\) is typically <i>not</i> penalized).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(\lambda \ge 0\): controls the amount of regularization</li>
    <li>\(\lambda = 0 \Rightarrow\) Lasso = OLS (no penalty)</li>
    <li>Small \(\lambda\) → weak shrinkage → coefficients close to OLS values</li>
    <li>Large \(\lambda\) → strong shrinkage → more coefficients set exactly to zero</li>
    <li>\(\beta_1, \dots, \beta_p\) = coefficients for features</li>
    <li>\(\mathbf{X}\) = design matrix</li>
    <li>\(\mathbf{y}\) = targets</li>
  </ul>

  <br>

  <h4 style="color:#ff79c6; margin-top:0;">What does <span style="white-space:nowrap;">\(\|\cdot\|_1\)</span> mean?</h4>
  <p>
    The notation <strong>\(\|\cdot\|_1\)</strong> means the <b><font color="red">L1 norm</font></b> 
    (also called the <b>Manhattan norm</b> or <b>taxicab norm</b>).
  </p>

  <ul>
    <li><strong>Double bars</strong> \(\|\cdot\|\) denote a norm in general.</li>
    <li>The subscript 1 indicates the L1 norm.</li>
  </ul>

  <p><strong>Definition:</strong></p>
  <p style="margin-left:1rem;">
    For a vector \(\mathbf{v} = (v_1, \dots, v_n)\), the L1 norm is
  </p>
  <p style="text-align:center;">
    \[
      \|\mathbf{v}\|_1 = \sum_{i=1}^n |v_i|
    \]
  </p>

  <br>

  <h4>Lasso Shrinkage Illustration</h4>
 <!-- Lasso Constraint Illustration (ellipse above, rotated, adjusted up for tangent) -->
<svg viewBox="0 0 800 480" role="img" aria-labelledby="lassoTitle lassoDesc"
     style="max-width:100%;height:auto;background:#1e1f22;border-radius:12px;box-shadow:0 4px 24px rgba(255,121,198,.12)">
  <title id="lassoTitle">Lasso Shrinkage Illustration</title>
  <desc id="lassoDesc">
    Axes for beta1 and beta2, a diamond-shaped L1 constraint centered at the origin,
    OLS-centered SSE ellipses, and the lasso solution at a corner of the diamond (sparse solution).
  </desc>

  <defs>
    <marker id="arrow-pink" viewBox="0 0 10 10" refX="8" refY="5"
            markerWidth="7" markerHeight="7" orient="auto-start-reverse">
      <path d="M0 0 L10 5 L0 10 z" fill="#ff79c6"></path>
    </marker>
    <style>
      .axis { stroke:#ff79c6; stroke-width:2; marker-end:url(#arrow-pink); }
      .axisLabel { fill:#ffb3de; font: 500 13px Roboto, Arial, sans-serif; }
      .note { fill:#ffb3de; font: 500 12px Roboto, Arial, sans-serif; }
      .label { fill:#ff79c6; font: 600 13px Roboto, Arial, sans-serif; }
      .ellipse { fill:none; stroke:#8be9fd; stroke-width:3; }
      .l1 { fill:none; stroke:#ff79c6; stroke-width:3; }
      .guide { stroke:#9aa0a6; stroke-width:1.5; stroke-dasharray:6 6; }
    </style>
  </defs>

  <!-- Axes -->
  <g id="coords">
    <line x1="60"  y1="240" x2="760" y2="240" class="axis" />
    <line x1="400" y1="440" x2="400" y2="40"  class="axis" />
    <text x="752" y="226" class="axisLabel">β₁</text>
    <text x="410" y="54"  class="axisLabel">β₂</text>
    <circle cx="400" cy="240" r="2.5" fill="#ff79c6"/>
    <text x="408" y="254" class="note">origin</text>
  </g>

  <!-- L1 diamond constraint -->
  <g id="l1-ball">
    <polygon points="400,170 470,240 400,310 330,240" class="l1"/>
    <text x="210" y="356" class="label">L1 constraint: ‖β‖₁ ≤ t</text>
  </g>

  <!-- OLS (unregularized) -->
  <g id="ols">
    <text x="560" y="160" text-anchor="middle" dominant-baseline="middle"
          style="font:700 20px/1 Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="572" y="180" class="label">OLS (unregularized)</text>
  </g>

  <!-- Shrinkage direction -->
  <g id="shrink">
    <line x1="400" y1="240" x2="560" y2="160" class="guide"/>
    <text x="492" y="206" class="note">shrinkage direction</text>
  </g>

  <!-- Animated SSE contour: aligned with vector (-90,+80) -->
  <g id="sse-anim">
    <ellipse id="expandEllipse" cx="560" cy="160" rx="5" ry="3" class="ellipse"
             transform="rotate(138 560 160)">
      <!-- final radii tuned so ellipse just touches (470,240) -->
      <animate attributeName="rx" from="5" to="120" dur="2.8s" begin="0s" fill="freeze" />
      <animate attributeName="ry" from="3" to="45"  dur="2.8s" begin="0s" fill="freeze" />
    </ellipse>
  </g>

  <!-- Lasso solution (corner of diamond) -->
  <g id="lasso">
    <circle cx="470" cy="240" r="6" fill="#50fa7b" stroke="#1e1f22" stroke-width="1.5" opacity="0">
      <animate attributeName="opacity" from="0" to="1" dur="0.5s" begin="2.8s" fill="freeze"/>
    </circle>
    <text x="482" y="258" class="label" style="fill:#50fa7b; opacity:0;">
      Lasso solution (corner, sparse)
      <animate attributeName="opacity" from="0" to="1" dur="0.5s" begin="2.8s" fill="freeze"/>
    </text>
  </g>

  <!-- Legend -->
  <g id="legend">
    <rect x="72" y="64" width="240" height="110" rx="10" ry="10"
          fill="rgba(33,17,27,.75)" stroke="#402138"/>
    <polygon points="100,88 114,102 100,116 86,102" class="l1"/>
    <text x="125" y="105" class="note">L1 constraint (diamond)</text>

    <line x1="85" y1="132" x2="99" y2="132" class="ellipse"/>
    <text x="110" y="136" class="note">SSE (loss) contour</text>

    <text x="88" y="158" style="font:700 16px Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="110" y="160" class="note">OLS estimate</text>
  </g>
</svg>


<br>

<h4>How Lasso Regression Works</h4>

<!-- Keep MathJax loaded once on the page -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<ol>
  <li><strong>Collect Data</strong> – Gather features \((x_1, x_2, \ldots, x_p)\) and outputs \(y\).</li>
  <li><strong>Standardize Features</strong> – Put features on comparable scales so the penalty treats them fairly; don’t penalize the intercept.</li>
  <li><strong>Choose \(\lambda\)</strong> – Typically via cross-validation (try a grid; pick the best validation score).</li>
  <li><strong>Fit the Model</strong> – Solve the L1-regularized optimization; common solvers include <em>coordinate descent</em> and <em>LARS</em>.</li>
  <li><strong>Interpret</strong> – Some coefficients become exactly 0 ⇒ automatic feature selection, simpler model.</li>
</ol>

<h4>Interpreting the Coefficients</h4>
<ul>
  <li><strong>Intercept (\(\beta_0\))</strong> – Baseline prediction when all features are zero (unpenalized).</li>
  <li><strong>Coefficient (\(\beta_j\))</strong> – Effect of feature \(x_j\) on \(y\) when others are held fixed.</li>
  <li><strong>Sparsity</strong> – If \(\beta_j = 0\), feature \(x_j\) is effectively removed by the model.</li>
</ul>

<br>

<h4>Model & Notation</h4>
<p><b>Scalar form:</b></p>
<p>
  $$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon $$
</p>

<p><b>Matrix form:</b></p>
<p>
  $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
  where
  $$ \mathbf{X} =
    \begin{bmatrix}
      1 & x_{11} & \cdots & x_{1p} \\
      1 & x_{21} & \cdots & x_{2p} \\
      \vdots & \vdots & \ddots & \vdots \\
      1 & x_{m1} & \cdots & x_{mp}
    \end{bmatrix},\quad
    \boldsymbol{\beta} =
    \begin{bmatrix}
      \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      y_1 \\ y_2 \\ \vdots \\ y_m
    \end{bmatrix}.
  $$
</p>

<h4>Objective Function (L1 Regularized)</h4>

    <p>
    Lasso minimizes:
    \[
      \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^p |\beta_j|
      \;=\;
      \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2
      \;+\; \lambda \|\boldsymbol{\beta}_{1:p}\|_1
    \]
    This penalty tends to produce exact zeros in \(\beta_j\) for large enough \(\lambda\).
  </p>
    

<h4>Optimization & Soft-Thresholding (No Closed-Form)</h4>
<p>
  A common solver is <em>coordinate descent</em>. With standardized features (mean 0, variance 1) and centered targets (so the intercept is separate), the update for each coefficient has a closed-form soft-threshold:
</p>
<p style="text-align:center;">
  $$ \beta_j \leftarrow \frac{1}{\|\mathbf{x}_j\|_2^2}\; S\!\left(\mathbf{x}_j^\top \bigl(\mathbf{y} - \sum_{k\ne j} \mathbf{x}_k \beta_k \bigr),\; \tfrac{\lambda}{2}\right), $$
  where \( S(t,\gamma) = \mathrm{sign}(t)\,\max(|t|-\gamma,\,0) \) is the <b>soft-threshold</b> operator.
</p>
<p>
  Intuition: if the correlation of feature \(j\) with the current residual is small, soft-thresholding sets \(\beta_j\) to <b>exactly zero</b>; otherwise it shrinks it toward zero.
</p>

<br>

<hr>

<h4>Worked Example: Predicting House Price (Lasso)</h4>
<p>Use the same data as in Section 2.1:</p>

<h4>Data</h4>
<center>
<table border="1" cellpadding="6">
  <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
  <tr><td>1000</td><td>2</td><td>200,000</td></tr>
  <tr><td>1500</td><td>3</td><td>280,000</td></tr>
  <tr><td>2000</td><td>3</td><td>340,000</td></tr>
  <tr><td>2500</td><td>4</td><td>400,000</td></tr>
  <tr><td>3000</td><td>4</td><td>460,000</td></tr>
</table>
</center>
    
<h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
<p>
  $$ 
  \mathbf{X} =
  \begin{bmatrix}
    1 & 1000 & 2 \\
    1 & 1500 & 3 \\
    1 & 2000 & 3 \\
    1 & 2500 & 4 \\
    1 & 3000 & 4
  \end{bmatrix},\quad
  \mathbf{y} =
  \begin{bmatrix}
    200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
  \end{bmatrix}.
  $$
</p>

<h4>Step 2: Preprocess (Recommended for Lasso)</h4>
<ul>
  <li>Standardize the non-intercept columns (Size, Bedrooms) to mean 0 and variance 1.</li>
  <li>Center \( \mathbf{y} \) (so the intercept can be estimated as the mean of \(y\)).</li>
  <li>Do <em>not</em> penalize the intercept.</li>
</ul>

<h4>Step 3: Fit via Coordinate Descent (Conceptual)</h4>
<ol>
  <li>Initialize all \(\beta_j=0\) (or from OLS).</li>
  <li>For \(j=1\) to \(n\): compute the partial residual, apply soft-threshold \(S(\cdot,\lambda/2)\), update \(\beta_j\).</li>
  <li>Repeat passes over features until coefficients stabilize.</li>
</ol>
<p>
  <b>What you’ll see:</b> with small \(\lambda\), coefficients are close to OLS; as \(\lambda\) increases, both shrink; if Bedrooms is less informative than Size,
  Lasso will often set \(\beta_{\text{Bedrooms}}\) to <b>exactly 0</b> first (sparser model).
</p>

<h4>Final Model (Example Form)</h4>
<p style="text-align:center; font-size:1.1em;">
  $$ \widehat{\text{Price}} \;=\; \hat{\beta}_0 \;+\; \hat{\beta}_{\text{Size}}\times \text{Size} \;+\; \hat{\beta}_{\text{Bedrooms}}\times \text{Bedrooms}, $$
  where for a sufficiently large \(\lambda\), you may get
  $$ \hat{\beta}_{\text{Bedrooms}} = 0 \quad\Rightarrow\quad
     \widehat{\text{Price}} \;=\; \hat{\beta}_0 \;+\; \hat{\beta}_{\text{Size}}\times \text{Size}. $$
</p>

<h4>Predictions \((\hat{\mathbf{y}})\) & Errors</h4>
<p>
  After fitting, compute \( \hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}} \), residuals \( \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \),
  and the usual metrics:
</p>
<p>
  $$ \mathrm{SSE} = \sum_{i=1}^{m}(y_i - \hat{y}_i)^2,\quad
     \mathrm{MSE} = \frac{\mathrm{SSE}}{m},\quad
     \mathrm{RMSE} = \sqrt{\mathrm{MSE}}. $$
</p>
<p>
  <em>Note:</em> Training error may increase slightly vs OLS (added penalty), but test error often improves thanks to reduced variance and feature selection.
</p>

<h4>Key Points</h4>
<ul>
  <li><b>Sparsity:</b> Lasso can set coefficients exactly to zero ⇒ built-in feature selection.</li>
  <li><b>No closed-form solution:</b> solved by iterative methods (coordinate descent, LARS); the L1 penalty is nondifferentiable at 0.</li>
  <li><b>Standardize features:</b> ensures fair penalization across features with different units.</li>
  <li><b>Choose \(\lambda\) by cross-validation:</b> small \(\lambda\) ≈ OLS; large \(\lambda\) ⇒ simpler, sparser models.</li>
  <li><b>Contrast with Ridge:</b> Ridge (L2) <em>shrinks</em> but rarely zeros; Lasso (L1) <em>shrinks and selects</em>.</li>
</ul>
      
  </div>

  </section>


  <section id="elastic-net-regression">
  <h4>2.5 Elastic Net Regression</h4>

  <b>Idea:</b>
  <p>
    <b>Elastic Net</b> combines the strengths of <b>Ridge (L2)</b> and <b>Lasso (L1)</b>.
    It shrinks coefficients (like Ridge) and can set some of them exactly to zero (like Lasso).
    This is especially useful when you have <b>many correlated features</b> or when pure L1 is unstable.
  </p>

  <h4>Objective (Elastic Net)</h4>
  <p style="text-align:center;">
    Minimize
    \[
      \mathrm{SSE}(\boldsymbol{\beta})\;+\;\lambda\left(\,
      (1-\alpha)\,\frac{\|\boldsymbol{\beta}_{1:p}\|_2^2}{2}
      \;+\;
      \alpha\,\|\boldsymbol{\beta}_{1:p}\|_1
      \right)
    \]
  </p>
  <ul>
    <li>\(\mathrm{SSE}(\boldsymbol{\beta}) = \|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2\)</li>
    <li>\(\lambda \ge 0\) = overall regularization strength</li>
    <li>\(0 \le \alpha \le 1\) (a.k.a. <code>l1_ratio</code> in sklearn)</li>
    <li>\(\alpha = 0 \Rightarrow\) Ridge; &nbsp; \(\alpha = 1 \Rightarrow\) Lasso</li>
    <li>Intercept \(\beta_0\) is typically <em>not</em> penalized</li>
  </ul>

  <h4>When to Use Elastic Net</h4>
  <ul>
    <li>Features are <b>highly correlated</b> (groups of related predictors)</li>
    <li>You want a balance of <b>shrinkage</b> (stability) and <b>sparsity</b> (feature selection)</li>
    <li>Lasso alone is <b>unstable</b> (selects 1 feature from a correlated group at random)</li>
  </ul>

  <h4>Bias–Variance Behavior</h4>
  <ul>
    <li>Increasing \(\lambda\) → more shrinkage → higher bias, lower variance</li>
    <li>Increasing \(\alpha\) (toward 1) → more L1 → more sparsity (more coefficients driven to zero)</li>
    <li>Decreasing \(\alpha\) (toward 0) → more L2 → less sparsity, more group-wise shrinkage</li>
  </ul>

  <h4>Scikit-learn Example</h4>
  <pre><code>
# Elastic Net with scaling and simple hyperparameter search
from sklearn.linear_model import ElasticNet
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, KFold

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNet(max_iter=10000, fit_intercept=True, random_state=42))
])

param_grid = {
    "enet__alpha": [0.0005, 0.001, 0.01, 0.1, 1.0],   # λ (overall strength)
    "enet__l1_ratio": [0.1, 0.3, 0.5, 0.7, 0.9]      # α (mix: 0=Ridge ... 1=Lasso)
}

cv = KFold(n_splits=5, shuffle=True, random_state=42)

search = GridSearchCV(
    pipe,
    param_grid=param_grid,
    scoring="neg_root_mean_squared_error",  # minimize RMSE
    cv=cv,
    n_jobs=-1
)
search.fit(X_train, y_train)

print("Best params:", search.best_params_)
print("Train RMSE:", -search.best_score_)

best_model = search.best_estimator_
test_rmse = (( (y_test - best_model.predict(X_test))**2 ).mean())**0.5
print("Test RMSE:", test_rmse)
  </code></pre>

  <h4>Tips</h4>
  <ul>
    <li><b>Always standardize</b> features before Elastic Net so penalties are fair across scales.</li>
    <li>Start with <code>l1_ratio</code> around <b>0.3–0.7</b>, tune by cross-validation.</li>
    <li>Use cross-validation to pick both <code>alpha</code> (λ) and <code>l1_ratio</code> (α).</li>
    <li>Interpretation: non-zero coefficients indicate selected features; magnitude is shrunk.</li>
  </ul>
</section>



  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
