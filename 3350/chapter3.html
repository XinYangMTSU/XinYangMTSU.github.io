<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 3: Supervised Learning - Classification</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    
    <li>
      <a href="#classification"><i class="fas fa-gamepad"></i>2. Classification</a>
      <div class="subsections">
        <a href="#logistic-regression">2.1 Logistic Regression</a>
        <a href="#knn">2.2 K-Nearest Neighbors (KNN)</a>
        <a href="#svm">2.3 Support Vector Machines (SVM)</a>
        <a href="#dt">2.4 Decision Tree </a>
        <a href="#nb">2.5 Naive Bayes </a>
      </div>
    </li>

    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 3: Supervised Learning - Classification</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>

    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li>1. Regression</li>
       <li><b>2. Classification</b></li>
    </ul>
    
  </section>
    

  <section id="supervised-cla">
    
    <div id="classification">
      <h3>2. Classification</h3>
      <p>Classification algorithms <b><font color="red">predicte categories</font> </b> (e.g., spam detection, image recognition).
      Common methods include:
      </p>
      
      <ul>
        <li><strong>Logistic Regression:</strong> Models the probability that an input belongs to a certain category; suitable for binary and multiclass problems.</li>
        <li><strong>k-Nearest Neighbors (kNN):</strong> Assigns a class based on the majority class among the k closest data points.</li>
        <li><strong>Support Vector Machines (SVM):</strong> Finds the optimal boundary (hyperplane) that best separates different classes.</li>
        <li><strong>Decision Tree Classification:</strong> Uses a tree-like structure to make decisions and assign labels.</li>
        <li><strong>Naive Bayes:</strong> Probabilistic classifier based on Bayes’ theorem; effective for text classification and spam detection.</li>
        <!--
        <li><strong>Neural Network Classification:</strong> Uses artificial neural networks to model complex patterns for multi-class and binary problems.</li>
        -->
      </ul>
      
    </div>
  </section>


<section id="logistic-regression">
    
    <h3>2.1 Logistic Regression</h3>

    <!-- 1. Motivation -->
    <h4>1. Motivation</h4>
    <div class="card">
      <p><strong>Problem with Linear Regression for classification:</strong></p>
      <ul>
        <li>Linear Regression can predict any real number (e.g., −3.2 or 2.7).</li>
        <li>But classification tasks need <em>Yes/No</em> outputs (Admit/Reject, Spam/Not Spam, Disease/No Disease).</li>
        <li>We need outputs between <strong>0 and 1</strong> so we can interpret them as <strong>probabilities</strong>.</li>
      </ul>
    </div>

    <!-- 2. Idea -->
    <h4>2. Idea</h4>
    <p>Logistic Regression takes the linear model from Linear Regression:</p>
    <p class="eq">
      $$ z \;=\; \beta_0 \;+\; \beta_1 x_1 \;+\; \cdots \;+\; \beta_p x_p $$
    </p>
    <p>and passes it through the <em>sigmoid</em> (logistic) function:</p>
    <p class="eq">
      $$ \sigma(z) \;=\; \frac{1}{1 + e^{-z}} $$
    </p>
    <ul>
      <li>Output is always in \((0,1)\).</li>
      <li>Can be interpreted as the probability of class \(1\).</li>
    </ul>

    <!-- 3. Equation -->
    <h4>3. Equation</h4>
    <p class="eq">
      $$ P(y=1 \mid x) \;=\; \frac{1}{\,1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}\,} $$
    </p>
    <p class="center small">If the probability \(\ge 0.5\) → predict \(1\); otherwise \(0\).</p>

    <!-- 4. Interpretation -->
    <h4>4. Interpretation of Coefficients</h4>
    <p>Each coefficient \(\beta_j\) changes the <strong>log-odds</strong> of the outcome:</p>
    <p class="eq">
      $$ \log\!\left(\frac{p}{1-p}\right) \;=\; \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$
    </p>
    <p>If you exponentiate \(\beta_j\), you get an <strong>odds ratio</strong>:</p>
    <ul>
      <li>\(e^{\beta_j} &gt; 1\): increases chance of the event.</li>
      <li>\(e^{\beta_j} &lt; 1\): decreases chance of the event.</li>
    </ul>

    <!-- 5. Objective -->
    <h4>5. Objective Function (How the Model Learns)</h4>
    <p>Instead of minimizing squared error, Logistic Regression uses <strong>Maximum Likelihood Estimation (MLE)</strong>.</p>

  <h4>What is Maximum Likelihood Estimation (MLE)?</h4>
  <p><strong>Idea:</strong> pick the parameters that make the data you observed <b>most probable</b> under your model.</p>

  <p>We maximize the log-likelihood (sums are easier &amp; numerically stable):</p>

   <div style="text-align:center; margin:6px 0;">
      $$ \ell(\boldsymbol{\beta}) \;=\; \sum_{i=1}^{m} \Big[\, y_i \log p_i \;+\; (1-y_i)\log(1-p_i) \,\Big]. $$
    </div>
  
  <h4>How it works?</h4>

  <ul>
    <li>1. Logistic Regression Setup</li>

      <div style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:14px 16px; margin:12px 0;">
        <div style="text-align:center; margin:6px 0;">
          $$ p_i \;=\; P(y_i=1 \mid x_i;\,\boldsymbol{\beta}) \;=\; \sigma(z_i)
             \;=\; \frac{1}{1+e^{-z_i}}, \qquad z_i \;=\; x_i^{\top}\boldsymbol{\beta}. $$
        </div>
        <p style="margin:6px 0 0;">with binary outcomes \(y_i \in \{0,1\}\), \(i=1,\dots,m\).</p>
    </div>

    <strong>Bernoulli: </strong>
    
    <p style="margin:6px 0;">Each label \(y_i \in \{0,1\}\) has</p>
      <div style="text-align:center; margin:8px 0;">
        $$ 
        P(y_i \mid x_i;\boldsymbol{\beta}) \;=\;
        \begin{cases}
          p_i, & y_i=1,\\[4pt]
          1-p_i, & y_i=0
        \end{cases}
        \;=\; p_i^{\,y_i}\,(1-p_i)^{\,1-y_i}.
        $$
      </div>
    
      <p><em>(The exponents \(y_i\) and \(1-y_i\) act like on/off switches.)</em></p>
    <br>
    
    <li>2. Form the likelihood:</li>

     <strong>Independence across samples (i.i.d.)</strong>
      <p style="margin:6px 0;">Assuming examples are independent given their features and \(\boldsymbol{\beta}\), the joint probability of the whole dataset is the product of per-sample probabilities:</p>
      
    <div style="text-align:center; margin:8px 0;">
        $$ L(\boldsymbol{\beta}) \;=\; \prod_{i=1}^{m} P(y_i \mid x_i;\boldsymbol{\beta}). $$
      </div>
    
     <strong>Plug the Bernoulli form in</strong>

    <div style="text-align:center; margin:6px 0;">
      $$ L(\boldsymbol{\beta}) \;=\; \prod_{i=1}^{m} \; p_i^{\,y_i}\; (1-p_i)^{\,1-y_i}. $$
    </div>

      <ul style="padding-left:1.1rem; margin:12px 0;">
    <li style="margin:6px 0;">
      <strong>\(L(\theta)\) — the likelihood:</strong>
      a score (function of the parameters \(\theta\)) that tells you how plausible \(\theta\) is
      given the data you observed.
    </li>

    <li style="margin:6px 0;">
      <strong>\(\displaystyle \prod_{i=1}^{m}\) — “product from \(i=1\) to \(m\)”:</strong>
      multiply one term for each data point in the dataset.
    </li>

    <li style="margin:6px 0;">
      <strong>\(p(y_i \mid x_i;\,\theta)\):</strong>
      the model’s probability (or probability density) of observing label \(y_i\) for example \(i\),
      given its features \(x_i\) and parameters \(\theta\).
    </li>
  </ul>

  <div style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:14px 16px; margin:12px 0;">
    <div style="text-align:center; margin:6px 0;">
      <strong>Note.</strong> This product form assumes examples are independent (given their features); that’s why we can multiply their individual probabilities.
    </div>
  </div>
  <br>
    
    <li>3. Log-Likelihood (Take logs: turns products into sums (easier, more stable))</li>
   
    <p>Apply <em>log</em>(·) to both sides:</p>
    
    <div style="text-align:center; margin:10px 0; background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px;">
    $$ \ell(\theta) 
        \;=\; \log L(\theta) \;=\; \sum_{i=1}^{m} \log\, p\!\left(y_i \mid x_i;\,\theta\right) 
        \;=\; \sum_{i=1}^{m} \log\!\Big( p_i^{\,y_i}\,(1-p_i)^{\,1-y_i} \Big).
      
      $$
  </div>

  <p style="margin-top:10px;">We used the identity</p>
  <div style="text-align:center; margin:8px 0;">
    $$ \log\!\Bigg(\prod_{i} a_i\Bigg) \;=\; \sum_{i} \log a_i. $$
  </div>

    <h4>Why do this? (three big reasons)</h4>
  <ul style="padding-left:1.1rem; margin:10px 0;">
    <li style="margin:8px 0;">
      <strong>Numerical stability:</strong> Products of many small probabilities underflow to 0; sums of logs don’t.
    </li>

    <li style="margin:8px 0;">
      <strong>Optimization convenience:</strong> sums are easier to differentiate:
      <div style="text-align:center; margin:8px 0;">
        $$ \nabla_{\theta}\,\ell(\theta) \;=\; \sum_{i=1}^{m} \nabla_{\theta}\,\log p\!\left(y_i \mid x_i;\,\theta\right), $$
      </div>
      so the gradient is just an additive sum of per-example gradients.
    </li>

    <li style="margin:8px 0;">
      <strong>Same optimum:</strong> \(\log(\cdot)\) is strictly increasing, so
      <div style="text-align:center; margin:8px 0;">
        $$ \arg\max_{\theta}\, L(\theta) \;=\; \arg\max_{\theta}\, \ell(\theta). $$
      </div>
      Maximizing the product or the sum of logs gives the same best \(\theta\).
    </li>
  </ul>
  
     
    <p>(Equivalently, minimize \(-\ell(\boldsymbol{\beta})\), the <em>cross-entropy loss</em>.)</p>
    
  <br>

    <li>4. Expand each term using \( \log(a^b)=b\log a \)</li>
    
  <div style="text-align:center; margin:8px 0;">
    $$ \log\!\Big( p_i^{\,y_i}\,(1-p_i)^{\,1-y_i} \Big)
       \;=\; y_i \log p_i \;+\; (1-y_i)\log(1-p_i). $$
  </div>

    <li>5. Combine terms</li>
    
  <div style="text-align:center; margin:8px 0; background:#eef6ff; border:1px solid #c7e0ff; border-radius:10px; padding:12px;">
    $$ \boxed{ \;
       \ell(\boldsymbol{\beta})
       \;=\; \sum_{i=1}^{m} \Big[\, y_i \log p_i \;+\; (1-y_i)\log(1-p_i) \,\Big]
       \; } $$
  </div>

  <p style="margin-top:10px; text-align:center;">
    where \( \; p_i = \sigma(x_i^{\top}\boldsymbol{\beta}) = \dfrac{1}{1+e^{-x_i^{\top}\boldsymbol{\beta}}} \; \).
  </p>

    <br>
    
     <div style="background:#fff7ed; border:1px solid #fed7aa; border-radius:10px; padding:12px 14px; margin-top:10px;">
    There’s no closed-form solution for \( \boldsymbol{\beta} \) in logistic regression, so we use iterative optimization.
  </div>

    <br>
    
    <li>6. Optimize (MLE): choose parameters that maximize the log-likelihood</li>

    <div style="text-align:center; margin:8px 0;">
      $$ \hat{\theta} \;=\; \arg\max_{\theta}\; \ell(\theta). $$
    </div>

    
     <li>7. Equivalently: Minimize the Cross-Entropy Loss</li>
    
  <p><strong>Cross-entropy</strong> is just the <em>negative log-likelihood</em> for binary logistic regression.</p>

  <div style="text-align:center; margin:10px 0; background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px;">
    $$ \min_{\boldsymbol{\beta}}\; -\ell(\boldsymbol{\beta})
       \;=\; \min_{\boldsymbol{\beta}} \sum_{i=1}^{m}
       \Big[\,-y_i \log p_i \;-\; (1-y_i)\log(1-p_i)\,\Big], \qquad
       p_i=\sigma(x_i^{\top}\boldsymbol{\beta}). $$
  </div>

  <p style="margin-top:10px;">Often we use the <em>average</em> loss (same minimizer):</p>
  <div style="text-align:center; margin:8px 0;">
    $$ \min_{\boldsymbol{\beta}}\; \frac{1}{m}\sum_{i=1}^{m}
       \Big[\,-y_i \log p_i \;-\; (1-y_i)\log(1-p_i)\,\Big]. $$
  </div>

  <div style="background:#eef6ff; border:1px solid #c7e0ff; border-radius:10px; padding:12px 14px; margin:12px 0;">
    <strong>Why it’s equivalent:</strong> \(\log(\cdot)\) is strictly increasing, so maximizing the log-likelihood
    \(\ell(\boldsymbol{\beta})\) is the same as minimizing its negative, i.e., the cross-entropy loss.
  </div>
    
    
    
  </ul>


  <h4>How do we find \( \boldsymbol{\beta} \)?</h4>
  <p><strong>Two common choices:</strong></p>

  <ol style="padding-left:1.2rem; margin-top:6px;">
    <li style="margin-bottom:10px;">
      <strong>Gradient Descent (maximize \( \ell \))</strong>
      <ul style="margin:6px 0 0 1.0rem;">
        <li>Compute probabilities: 
          <div style="text-align:center; margin:6px 0;">
            $$ \mathbf{p} \;=\; \sigma(\mathbf{X}\boldsymbol{\beta}). $$
          </div>
        </li>
        <li>Gradient of log-likelihood:
          <div style="text-align:center; margin:6px 0;">
            $$ \nabla \ell(\boldsymbol{\beta}) \;=\; \mathbf{X}^{\top}(\mathbf{y}-\mathbf{p}). $$
          </div>
        </li>
        <li>Update rule (with small learning rate \( \eta \)):
          <div style="text-align:center; margin:6px 0;">
            $$ \boldsymbol{\beta} \;\leftarrow\; \boldsymbol{\beta} \;+\; \eta\, \mathbf{X}^{\top}(\mathbf{y}-\mathbf{p}), $$
          </div>
          repeat until convergence.
        </li>
      </ul>
    </li>
  </ol>



    <!-- 6. Example -->
    <h4>6. Example</h4>
    <p>Imagine predicting if a student is admitted based on GPA and test score.</p>
    <table aria-label="Example dataset for admission">
      <thead>
        <tr><th>GPA</th><th>Test Score</th><th>Admitted (y)</th></tr>
      </thead>
      <tbody>
        <tr><td>3.0</td><td>600</td><td>0</td></tr>
        <tr><td>3.8</td><td>700</td><td>1</td></tr>
        <tr><td>4.0</td><td>720</td><td>1</td></tr>
        <tr><td>2.8</td><td>580</td><td>0</td></tr>
      </tbody>
    </table>

    <p class="eq">
      $$ P(\text{Admit}=1) \;=\; \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{GPA} + \beta_2 \cdot \text{TestScore})}} $$
    </p>
    <div class="tip">
      If for a new student (GPA = 3.6, Score = 680) we calculate \(p = 0.72\): that’s a 72% chance of admission → predict <strong>Admitted = 1</strong>.
    </div>

    <!-- 7. Key Points -->
    <h4>7. Key Points</h4>
    <ul>
      <li>Logistic Regression outputs <strong>probabilities</strong>, not raw continuous values.</li>
      <li>A threshold (commonly 0.5) converts probability → class.</li>
      <li>Uses <strong>MLE</strong>, not Ordinary Least Squares.</li>
      <li>Works best when the relationship between features and <em>log-odds</em> is approximately linear.</li>
    </ul>

    <!-- 8. Visualization -->
    <h4>The Sigmoid (Logistic) Function</h4>

    <ul>
      <li>1. Definition</li>
        <div class="card">
          <p class="eq">$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$</p>
          <ul>
            <li><strong>Input:</strong> any real number \( z \in (-\infty, +\infty) \).</li>
            <li><strong>Output:</strong> a number between 0 and 1.</li>
          </ul>
        </div>

      <li>2. Shape</li>
        <ul>
          <li>The graph looks like an S-shaped curve.</li>
          <li>As \( z \to +\infty \), \( \sigma(z) \to 1 \).</li>
          <li>As \( z \to -\infty \), \( \sigma(z) \to 0 \).</li>
          <li>At \( z = 0 \), \( \sigma(0) = 0.5 \).</li>
        </ul>
        <p>That’s why it’s perfect for mapping any real number into a probability.</p>

      <li>3. Why it’s used in Logistic Regression</li>
        <p>Logistic Regression first computes a linear combination:</p>
        <p class="eq">$$ z = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$</p>
        <p>But \(z\) can be any real number. To turn \(z\) into a probability, we pass it through the sigmoid:</p>
        <p class="eq">$$ P(y=1 \mid x) = \sigma(z) $$</p>
        <p>The result is always between 0 and 1 → interpretable as probability.</p>
      
    </ul>

    <canvas id="sigmoidCanvas" width="720" height="380" aria-label="Sigmoid function plot"></canvas>
    <center>
    <div class="caption">Sigmoid function \( \sigma(z) = \dfrac{1}{1 + e^{-z}} \)</div>
    </center>
  
  <script>
    // Draw a simple sigmoid plot on the canvas (z in [-8, 8])
    (function drawSigmoid() {
      const canvas = document.getElementById('sigmoidCanvas');
      if (!canvas || !canvas.getContext) return;
      const ctx = canvas.getContext('2d');
      const W = canvas.width, H = canvas.height;

      // Padding for axes
      const pad = { left: 60, right: 20, top: 20, bottom: 50 };
      const plotW = W - pad.left - pad.right;
      const plotH = H - pad.top - pad.bottom;

      // Domain & range
      const zMin = -8, zMax = 8;
      const pMin = 0,  pMax = 1;

      const xScale = z => pad.left + ( (z - zMin) / (zMax - zMin) ) * plotW;
      const yScale = p => pad.top + (1 - ( (p - pMin) / (pMax - pMin) )) * plotH; // invert y

      // Clear
      ctx.clearRect(0, 0, W, H);

      // Background
      ctx.fillStyle = '#ffffff';
      ctx.fillRect(0, 0, W, H);

      // Axes
      ctx.strokeStyle = '#cbd5e1';
      ctx.lineWidth = 1;

      // Border of plot area
      ctx.strokeRect(pad.left, pad.top, plotW, plotH);

      // Horizontal grid lines at p = 0, 0.5, 1
      const gridPs = [0, 0.5, 1];
      ctx.beginPath();
      gridPs.forEach(p => {
        const y = yScale(p);
        ctx.moveTo(pad.left, y);
        ctx.lineTo(pad.left + plotW, y);
      });
      ctx.stroke();

      // Vertical grid lines at z = -6, -3, 0, 3, 6
      const gridZs = [-6, -3, 0, 3, 6];
      ctx.beginPath();
      gridZs.forEach(z => {
        const x = xScale(z);
        ctx.moveTo(x, pad.top);
        ctx.lineTo(x, pad.top + plotH);
      });
      ctx.stroke();

      // Labels
      ctx.fillStyle = '#374151';
      ctx.font = '12px Roboto, sans-serif';
      // y labels
      gridPs.forEach(p => {
        const y = yScale(p);
        ctx.fillText(p.toFixed(1), 28, y + 4);
      });
      // x labels
      gridZs.forEach(z => {
        const x = xScale(z);
        ctx.fillText(z.toString(), x - 6, pad.top + plotH + 18);
      });

      // Axis titles
      ctx.font = '13px Roboto, sans-serif';
      ctx.fillText('z', pad.left + plotW / 2, H - 18);
      ctx.save();
      ctx.translate(18, pad.top + plotH / 2);
      ctx.rotate(-Math.PI / 2);
      ctx.fillText('Probability  p = σ(z)', 0, 0);
      ctx.restore();

      // Sigmoid curve
      ctx.strokeStyle = '#2563eb';
      ctx.lineWidth = 2;
      ctx.beginPath();
      const steps = 600;
      for (let i = 0; i <= steps; i++) {
        const t = i / steps;
        const z = zMin + t * (zMax - zMin);
        const p = 1 / (1 + Math.exp(-z));
        const x = xScale(z);
        const y = yScale(p);
        if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
      }
      ctx.stroke();

      // Mark p=0.5 at z=0
      const x0 = xScale(0);
      const y05 = yScale(0.5);
      ctx.fillStyle = '#1f2937';
      ctx.beginPath();
      ctx.arc(x0, y05, 3.5, 0, 2 * Math.PI);
      ctx.fill();

      // Dashed helpers to axes
      ctx.setLineDash([5, 4]);
      ctx.strokeStyle = '#9ca3af';
      ctx.beginPath();
      ctx.moveTo(x0, y05);
      ctx.lineTo(x0, pad.top + plotH);
      ctx.moveTo(x0, y05);
      ctx.lineTo(pad.left, y05);
      ctx.stroke();
      ctx.setLineDash([]);
    })();
  </script>

  <br>

  <h4>From Sigmoid (Logistic) Function to Log-Odds</h4>
  <p>We start from the sigmoid and derive the log-odds step by step.</p>

  <ol style="padding-left:1.2rem;">
    <li>
      <strong>Start with the sigmoid function</strong>
      <div style="text-align:center; margin:6px 0;">
        $$ p \;=\; \sigma(z) \;=\; \frac{1}{1 + e^{-z}} $$
      </div>
    </li>

    <li>
      <strong>Invert the fraction</strong> — take reciprocals of both sides:
      <div style="text-align:center; margin:6px 0;">
        $$ \frac{1}{p} \;=\; 1 + e^{-z} $$
      </div>
    </li>

    <li>
      <strong>Subtract 1 from both sides</strong> and simplify:
      <div style="text-align:center; margin:6px 0;">
        $$ \frac{1}{p} - 1 \;=\; e^{-z}
        \quad\Longrightarrow\quad
        \frac{1-p}{p} \;=\; e^{-z} $$
      </div>
    </li>

    <li>
      <strong>Take natural log</strong>:
      <div style="text-align:center; margin:6px 0;">
        $$ \ln\!\left(\frac{1-p}{p}\right) \;=\; -z $$
      </div>
    </li>

    <li>
      <strong>Rearrange</strong> — multiply by \(-1\):
      <div style="text-align:center; margin:6px 0;">
        $$ \ln\!\left(\frac{p}{1-p}\right) \;=\; z $$
      </div>
    </li>
  </ol>

  <div style="border:1px solid #e5e7eb; background:#f9fafb; padding:12px 14px; border-radius:10px; margin-top:10px;">
    <strong>Final Result (Log-Odds / Logit)</strong>
    <div style="text-align:center; margin-top:6px;">
      $$ \text{Log-Odds} \;=\; \ln\!\left(\frac{p}{1-p}\right) \;=\; z \;=\; \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$
    </div>
  </div>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Scalar form:</b></p>
  <p>
    $$ p(y=1 \mid \mathbf{x}) = \sigma\!\big(\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n\big), \quad
       \sigma(t) = \frac{1}{1+e^{-t}}. $$
    $$ \text{logit}(p) = \log\frac{p}{1-p} = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n. $$
  </p>

  <p><b>Matrix form:</b></p>
  <p>
    $$ \mathbf{p} = \sigma(\mathbf{X}\boldsymbol{\beta}), \quad
       \mathbf{X} =
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1n} \\
        1 & x_{21} & \cdots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mn}
      \end{bmatrix}, \quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n
      \end{bmatrix}. $$
  </p>

  <h4>Objective (Maximum Likelihood / Cross-Entropy)</h4>
  <p>
    For samples \((\mathbf{x}_i, y_i)\) with \(y_i \in \{0,1\}\), the (negative) log-likelihood (a.k.a. cross-entropy loss) is
  </p>
  <p>
    $$ \mathcal{L}(\boldsymbol{\beta})
       = -\sum_{i=1}^{m}\Big[\, y_i \log p_i + (1-y_i)\log(1-p_i) \,\Big],
       \quad p_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}). $$
  </p>
  <p>
    In vector form, with \(\mathbf{p}=\sigma(\mathbf{X}\boldsymbol{\beta})\):
    $$ \mathcal{L}(\boldsymbol{\beta}) = -\big( \mathbf{y}^\top \log \mathbf{p} + (\mathbf{1}-\mathbf{y})^\top \log (\mathbf{1}-\mathbf{p}) \big). $$
  </p>

  <hr>

  <h4>Step 5: Evaluation</h4>
  <p>
    Common metrics: accuracy, precision, recall, F1, ROC-AUC, and log loss.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li>Outputs calibrated probabilities in \([0,1]\), not raw scores.</li>
    <li>Linear decision boundary in feature space (after the logit transform).</li>
    <li>No closed-form solution; solved by iterative optimization.</li>
    <li>For regularization, use <b>Ridge (L2)</b> or <b>Lasso (L1)</b> penalties to improve generalization.</li>
  </ul>

</section>

  
<section id="knn">

  <h4>2.2 k-Nearest Neighbors (kNN)</h4>
  <br>

  <b>Idea:</b>
  <p>
    k-Nearest Neighbors is a <b>non-parametric, instance-based</b> method. To make a prediction for a new point,
    it looks at the <b>k closest training examples</b> and combines their labels/values:
    majority vote for classification, average for regression.
    The goal is to <b><font color="red">use local neighborhoods to predict</font></b> rather than fitting a global line or plane.
  </p>

  <b>Equation (Distance & Rules):</b>
  <p style="text-align:center;">
    Euclidean distance:
    \[
      d(\mathbf{x}_i,\mathbf{x}_q) \;=\; \sqrt{\sum_{j=1}^{n} (x_{ij} - x_{qj})^2}
    \]
    (Common alternatives: Manhattan \( \sum_j |x_{ij}-x_{qj}| \), Minkowski.)
  </p>
  <p><b>Prediction (Classification):</b> \(\hat{y} = \mathrm{mode}\{\,y_{i_1},\ldots,y_{i_k}\,\}\). Weighted variant:
    \(\hat{p} = \dfrac{\sum_{r=1}^{k} w_r \, y_{i_r}}{\sum_{r=1}^{k} w_r}\), with \(w_r = \dfrac{1}{d_r + \varepsilon}\).
  </p>
  <p><b>Prediction (Regression):</b> \(\hat{y} = \dfrac{1}{k}\sum_{r=1}^{k} y_{i_r}\). Weighted:
    \(\hat{y} = \dfrac{\sum_{r=1}^{k} w_r \, y_{i_r}}{\sum_{r=1}^{k} w_r}\).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li><em>k</em> = number of neighbors (hyperparameter)</li>
    <li>Distance metric (Euclidean by default)</li>
    <li>Weights (uniform or distance-weighted)</li>
  </ul>

  <br>

  <center>
    <img src="images/knn1.png" width="800px" height="300px" alt="kNN neighborhoods around a query point">
  </center>

  <br>

  <h4>How kNN Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Keep the training examples in memory (lazy learning; no parameter fitting).</li>
    <li><strong>Choose k, distance, weights</strong> – Often selected via cross-validation.</li>
    <li><strong>Find neighbors</strong> – For a query point \( \mathbf{x}_q \), compute distances to all training points and pick the k smallest.</li>
    <li><strong>Aggregate</strong> – Classification: majority vote; Regression: average (optionally distance-weighted).</li>
    <li><strong>Evaluate</strong> – Use accuracy/F1/AUC for classification, or MSE/RMSE/MAE for regression.</li>
  </ol>

  <h4>Interpreting the Parameters</h4>
  <ul>
    <li><strong>k:</strong> Small k → low bias, high variance (sensitive/noisy). Large k → higher bias, smoother predictions.</li>
    <li><strong>Distance weighting:</strong> Nearer points get more influence (e.g., \(w=1/(d+\varepsilon)\)).</li>
    <li><strong>Feature scaling:</strong> <b>Standardize</b> features; otherwise large-scale features dominate distance.</li>
  </ul>

  <br>

  <h4>Model & Notation</h4>
  <p>
    Given training set \(\{(\mathbf{x}_i, y_i)\}_{i=1}^m\) and a query \(\mathbf{x}_q\), let
    \(i_1,\ldots,i_k\) index the k nearest neighbors by a chosen distance \(d(\cdot,\cdot)\).
  </p>
  <p>
    <b>kNN-Classification:</b>
    \(\hat{y} = \mathrm{mode}\{y_{i_1},\ldots,y_{i_k}\}\), or probability
    \(\hat{p} = \frac{\sum_r w_r \, y_{i_r}}{\sum_r w_r}\).
  </p>
  <p>
    <b>kNN-Regression:</b>
    \(\hat{y} = \frac{1}{k}\sum_r y_{i_r}\) (uniform), or
    \(\hat{y} = \frac{\sum_r w_r \, y_{i_r}}{\sum_r w_r}\) (distance-weighted).
  </p>

  <h4>Objective</h4>
  <p>
    kNN has <b>no explicit training objective</b> (no coefficients to fit). It is a <em>lazy learner</em>:
    all computation happens at prediction time. The “objective” is implicit—use local neighborhoods to minimize local error.
  </p>

  <br>

  <hr>

  <h4>Worked Example: Predicting House Price (kNN-Regression)</h4>
  <p>Use the same data as before. Predict the price for a query house with <b>Size = 2200</b> sq.ft and <b>Bedrooms = 3</b>.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
    <tr><td>1000</td><td>2</td><td>200,000</td></tr>
    <tr><td>1500</td><td>3</td><td>280,000</td></tr>
    <tr><td>2000</td><td>3</td><td>340,000</td></tr>
    <tr><td>2500</td><td>4</td><td>400,000</td></tr>
    <tr><td>3000</td><td>4</td><td>460,000</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\), \(\mathbf{y}\), and the query \(\mathbf{x}_q\)</h4>
  <p>
    \(\mathbf{X} = \begin{bmatrix}
      1000 & 2\\
      1500 & 3\\
      2000 & 3\\
      2500 & 4\\
      3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} = \begin{bmatrix}
      200000\\ 280000\\ 340000\\ 400000\\ 460000
    \end{bmatrix},\quad
    \mathbf{x}_q = \begin{bmatrix}2200 & 3\end{bmatrix}.
  \)
  </p>

  <h4>Step 2: Distances to the Query (Euclidean)</h4>
  <p>
    \( d_i = \sqrt{(\text{Size}_i - 2200)^2 + (\text{Bed}_i - 3)^2} \).
  </p>
  <table border="1" cellpadding="6">
    <tr><th>Training point</th><th>(Size, Beds)</th><th>Distance to (2200,3)</th></tr>
    <tr><td>1</td><td>(1000, 2)</td><td>≈ 1200.00</td></tr>
    <tr><td>2</td><td>(1500, 3)</td><td>700.00</td></tr>
    <tr><td>3</td><td>(2000, 3)</td><td>200.00</td></tr>
    <tr><td>4</td><td>(2500, 4)</td><td>≈ 300.00</td></tr>
    <tr><td>5</td><td>(3000, 4)</td><td>≈ 800.00</td></tr>
  </table>

  <h4>Step 3: Choose k and Neighbors</h4>
  <p>
    Let \(k=3\). The 3 nearest neighbors are points #3 (340,000), #4 (400,000), and #2 (280,000).
  </p>

  <h4>Step 4: Predict (Regression)</h4>
  <p>
    <b>Uniform kNN:</b>
    \(\hat{y} = \frac{340{,}000 + 400{,}000 + 280{,}000}{3} = 340{,}000.\)
  </p>
  <p>
    <b>Distance-weighted kNN</b> (with \(w_i = 1/(d_i+\varepsilon)\)):
    \[
      \hat{y}
      \approx \frac{ \frac{340{,}000}{200} + \frac{400{,}000}{300} + \frac{280{,}000}{700} }
      { \frac{1}{200} + \frac{1}{300} + \frac{1}{700} }
      \;\approx\; 351{,}707.
    \]
  </p>

  <p class="note">
    <b>Scaling matters:</b> Size is measured in thousands while Bedrooms is small, so distance is dominated by Size.
    In practice, <b>standardize</b> features before kNN.
  </p>

  <br>

  <h4>Classification Note (Same Setup)</h4>
  <p>
    If the target were binary (e.g., Sold in 30 days: 0/1), the same neighbors would vote.
    With labels \([1,1,0]\) among the 3 nearest, the majority is <b>1 (Yes)</b>, and a simple probability estimate is \(2/3 \approx 0.67\).
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Non-parametric, lazy:</b> No training fit; predicts via local neighborhoods.</li>
    <li><b>Hyperparameters:</b> k, distance metric, and weighting—choose via cross-validation.</li>
    <li><b>Feature scaling is essential.</b> Consider standardization or normalization.</li>
    <li><b>Computational cost:</b> Naïve prediction is \(O(mn)\) per query; KD-trees/ball trees/ANN speed this up.</li>
    <li><b>Curses in high-D:</b> Distances become less informative as dimension grows (curse of dimensionality).</li>
  </ul>

</section>


  <section id="svm">

  <h4>2.3 Support Vector Machines (SVM)</h4>
  <br>

  <b>Idea:</b>
  <p>
    Support Vector Machines are margin-based classifiers. They find a decision boundary that <b>maximizes the margin</b>
    (distance) between classes while allowing some misclassifications when needed. The goal is to
    <b><font color="red">separate classes with the widest possible margin</font></b> (for robustness), and optionally use
    <em>kernels</em> to separate data that are not linearly separable.
  </p>

  <b>Equation (Linear SVM decision function):</b>
  <p style="text-align:center;">
    Decision score: \(\; f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b \;\) &nbsp;&nbsp;&nbsp;
    Prediction: \(\; \hat{y} = \mathrm{sign}\!\big(f(\mathbf{x})\big)\).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(\mathbf{w}\) = weight vector; \(b\) = bias (intercept)</li>
    <li>\(\mathbf{x} = (x_1,\ldots,x_n)\) = feature vector</li>
    <li>\(\hat{y} \in \{-1,+1\}\) = predicted class label</li>
    <li>For non-linear SVM, replace inner products \(\mathbf{x}_i^\top \mathbf{x}_j\) with a kernel \(K(\mathbf{x}_i,\mathbf{x}_j)\)</li>
  </ul>

  <br>

  <center>
    <img src="images/svm1.png" width="800px" height="300px" alt="SVM maximum-margin separator with support vectors">
  </center>

  <br>

  <h4>How SVM Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Feature vectors \(\mathbf{x}_i\) with labels \(y_i \in \{-1,+1\}\).</li>
    <li><strong>Choose linear or kernel SVM</strong> – Linear for high-dimensional but roughly linear data; kernels (RBF/poly) for curved boundaries.</li>
    <li><strong>Fit the model</strong> – Optimize a margin objective with regularization parameter \(C\) (trade-off: margin size vs. training errors).</li>
    <li><strong>Predict</strong> – Use the sign of the decision score \(f(\mathbf{x})\). (Scores are margins, not probabilities.)</li>
    <li><strong>Evaluate</strong> – Accuracy, precision/recall, F1, ROC-AUC; optionally calibrate probabilities (e.g., Platt scaling).</li>
  </ol>

  <h4>Interpreting the Model</h4>
  <ul>
    <li><strong>Support vectors</strong> – Training points on/inside the margin; they alone determine the boundary.</li>
    <li><strong>Margin</strong> – Width \(= \dfrac{2}{\|\mathbf{w}\|}\); maximizing margin improves robustness.</li>
    <li><strong>Parameter \(C\)</strong> – Larger \(C\) penalizes misclassification more (narrower margin, lower bias, higher variance). Smaller \(C\) allows a wider margin (higher bias, lower variance).</li>
    <li><strong>Kernel</strong> – Implicitly maps features to a higher-dimensional space: \(K(\mathbf{x},\mathbf{z}) = \phi(\mathbf{x})^\top \phi(\mathbf{z})\).</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Primal (soft-margin, linear SVM):</b></p>
  <p>
    $$ \min_{\mathbf{w},\,b,\,\boldsymbol{\xi}\ge 0} \;\; \frac{1}{2}\|\mathbf{w}\|_2^2 + C \sum_{i=1}^{m} \xi_i
       \quad \text{s.t.}\quad y_i \big(\mathbf{w}^\top \mathbf{x}_i + b\big) \ge 1 - \xi_i. $$
  </p>

  <p><b>Unconstrained hinge-loss form:</b></p>
  <p>
    $$ \min_{\mathbf{w},\,b} \;\; \frac{1}{2}\|\mathbf{w}\|_2^2 \;+\; C \sum_{i=1}^{m} \max\!\big(0,\, 1 - y_i(\mathbf{w}^\top \mathbf{x}_i + b)\big). $$
  </p>

  <p><b>Dual with kernels (decision rule):</b></p>
  <p>
    $$ \max_{\boldsymbol{\alpha}} \;\; \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j K(\mathbf{x}_i,\mathbf{x}_j),
       \quad 0 \le \alpha_i \le C,\; \sum_i \alpha_i y_i = 0, $$
    $$ f(\mathbf{x}) = \sum_{i \in \mathcal{SV}} \alpha_i y_i\, K(\mathbf{x}_i,\mathbf{x}) + b. $$
  </p>

  <br>

  <hr>

  <h4>Worked Example: “Sold in 30 days” (Binary)</h4>
  <p>We reuse the small dataset; set \(y = -1\) for “No”, \(y = +1\) for “Yes”.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>-1</td></tr>
    <tr><td>1500</td><td>3</td><td>-1</td></tr>
    <tr><td>2000</td><td>3</td><td>+1</td></tr>
    <tr><td>2500</td><td>4</td><td>+1</td></tr>
    <tr><td>3000</td><td>4</td><td>+1</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1000 & 2 \\
      1500 & 3 \\
      2000 & 3 \\
      2500 & 4 \\
      3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      -1 \\ -1 \\ +1 \\ +1 \\ +1
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Choose model &amp; hyperparameters</h4>
  <ul>
    <li>Scale features (recommended).</li>
    <li>Linear SVM with \(C=1\) (baseline). Try RBF kernel with \(\gamma\) tuned via cross-validation if the boundary is curved.</li>
  </ul>

  <h4>Step 3: Fit</h4>
  <p>
    Train the SVM on \((\mathbf{X},\mathbf{y})\). The support vectors will be the examples closest to the boundary (in this toy set, likely near the transition around 2000–2500 sq.ft).
  </p>

  <h4>Step 4: Predict</h4>
  <p>
    For a new house \(\mathbf{x}_q=(2200,3)\), compute \(f(\mathbf{x}_q)\). If \(f(\mathbf{x}_q)&gt;0\) predict <b>+1 (Yes)</b>, else <b>-1 (No)</b>.
  </p>

  <h4>Step 5: Evaluation</h4>
  <p>
    Report accuracy, precision/recall, F1, ROC-AUC. For probability-like outputs, calibrate scores (e.g., Platt scaling) after training.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Maximum-margin:</b> promotes robustness to small perturbations.</li>
    <li><b>Support vectors only:</b> the decision boundary depends on a subset of training points.</li>
    <li><b>\(C\) controls bias–variance:</b> large \(C\) fits training more tightly; small \(C\) widens the margin.</li>
    <li><b>Kernels:</b> RBF, polynomial, and others enable non-linear boundaries via the kernel trick.</li>
    <li><b>Scaling is essential:</b> SVMs are sensitive to feature scales.</li>
    <li><b>Outputs are margins, not probabilities:</b> calibrate if you need probabilities.</li>
  </ul>

</section>


  <section id="dt">

  <h4>2.4 Decision Tree (Classification)</h4>
  <br>

  <b>Idea:</b>
  <p>
    A Decision Tree predicts a class by asking a sequence of <b>if–then</b> questions on features.
    At each split, it chooses the feature/threshold that best <b><font color="red">separates the classes (reduces impurity)</font></b>.
    The final prediction comes from the class majority at a leaf.
  </p>

  <b>Split Criteria (Impurity):</b>
  <p style="text-align:center;">
    Common impurities for classification:
    <br>
    <b>Gini:</b>
    \( \displaystyle G(S) = 1 - \sum_{c} p_c^2 \)
    &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;
    <b>Entropy:</b>
    \( \displaystyle H(S) = - \sum_{c} p_c \log_2 p_c \)
  </p>
  <p style="text-align:center;">
    <b>Information Gain:</b>
    \( \displaystyle \mathrm{Gain}(S,\text{split})
       = \mathrm{Impurity}(S) - \sum_{v} \frac{|S_v|}{|S|}\,\mathrm{Impurity}(S_v) \)
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\( S \) = current set of training samples at a node; \( S_v \) = subset after a split option \(v\)</li>
    <li>\( p_c \) = fraction of class \(c\) in \(S\)</li>
    <li>Impurity = Gini or Entropy (choose one consistently)</li>
  </ul>

  <br>

  <center>
    <img src="images/tree1.png" width="800px" height="300px" alt="Decision tree splitting space into class-pure regions">
  </center>

  <br>

  <h4>How a Decision Tree Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Features \((x_1,\ldots,x_n)\) with class labels \(y\).</li>
    <li><strong>Choose a split</strong> – For each feature/threshold, compute impurity reduction; pick the best.</li>
    <li><strong>Recurse</strong> – Split the resulting subsets until a stopping rule (pure leaf, max depth, min samples).</li>
    <li><strong>Predict</strong> – A new sample follows the if–then path; output the leaf’s majority class (optionally class probabilities).</li>
    <li><strong>Prune/Tune</strong> – Limit depth or prune to avoid overfitting; evaluate with accuracy, precision/recall, F1, ROC-AUC.</li>
  </ol>

  <h4>Interpreting the Tree</h4>
  <ul>
    <li><strong>Rules:</strong> Each root-to-leaf path is a human-readable rule (e.g., <em>if Size &lt; 2000 then No</em>).</li>
    <li><strong>Feature importance:</strong> Features used near the root (large impurity drop) are typically more influential.</li>
    <li><strong>Probabilities:</strong> Leaf probabilities = class frequencies in that leaf.</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p>
    A tree consists of <b>internal nodes</b> (tests like \(x_j \le t\)) and <b>leaves</b> (class distributions).
    For a dataset \( \{(\mathbf{x}_i, y_i)\}_{i=1}^m \), a split at node \(N\) chooses feature \(j\) and threshold \(t\)
    that maximize \( \mathrm{Gain}(S_N, (j,t)) \).
  </p>

  <h4>Objective</h4>
  <p>
    Greedy, top-down growth: at each node, pick the split with the <b>largest information gain</b> (or Gini decrease).
    Overfitting is controlled via <b>pre-pruning</b> (max depth, min samples per leaf) or <b>post-pruning</b>
    (e.g., cost-complexity pruning with parameter \( \alpha \)).
  </p>

  <br>

  <hr>

  <h4>Worked Example: “Sold in 30 Days”</h4>
  <p>Binary labels: 1 = Yes, 0 = No. Features: Size (sq.ft), Bedrooms.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>0</td></tr>
    <tr><td>1500</td><td>3</td><td>0</td></tr>
    <tr><td>2000</td><td>3</td><td>1</td></tr>
    <tr><td>2500</td><td>4</td><td>1</td></tr>
    <tr><td>3000</td><td>4</td><td>1</td></tr>
  </table>

  <h4>Step 1: Parent Impurity</h4>
  <p>
    Class counts: 3 ones, 2 zeros ⇒ \(p_1 = 3/5,\; p_0 = 2/5\).
  </p>
  <p>
    Gini:
    \( \displaystyle G(S) = 1 - (3/5)^2 - (2/5)^2 = 1 - 9/25 - 4/25 = 12/25 = 0.48 \).
    <br>
    Entropy:
    \( \displaystyle H(S) = -\tfrac{3}{5}\log_2\tfrac{3}{5} - \tfrac{2}{5}\log_2\tfrac{2}{5} \approx 0.971 \).
  </p>

  <h4>Step 2: Try a Split on Size</h4>
  <p><b>Threshold \( \text{Size} &lt; 2000 \):</b></p>
  <ul>
    <li>Left: \((1000,2,0), (1500,3,0)\) ⇒ all 0 ⇒ Gini = 0</li>
    <li>Right: \((2000,3,1), (2500,4,1), (3000,4,1)\) ⇒ all 1 ⇒ Gini = 0</li>
  </ul>
  <p>
    Weighted child Gini = \( \tfrac{2}{5}\cdot 0 + \tfrac{3}{5}\cdot 0 = 0\).
    <br>
    <b>Information Gain = 0.48 − 0 = 0.48</b> (perfect split).
  </p>

  <p><b>Threshold \( \text{Size} &lt; 2250 \) (for comparison):</b></p>
  <ul>
    <li>Left (3 items): labels 0,0,1 ⇒ \(G = 1 - (2/3)^2 - (1/3)^2 = 4/9 \approx 0.444\)</li>
    <li>Right (2 items): labels 1,1 ⇒ \(G = 0\)</li>
  </ul>
  <p>
    Weighted child Gini \(= \tfrac{3}{5}\cdot 0.444 + \tfrac{2}{5}\cdot 0 \approx 0.267\).
    <br>
    <b>Information Gain ≈ 0.48 − 0.267 = 0.213</b>.
  </p>

  <h4>Step 3: Stopping / Pruning</h4>
  <p>
    For this toy set, the split <em>Size &lt; 2000</em> yields pure leaves, so the tree can stop (depth = 1).
    In larger datasets, use max depth / min samples per leaf or post-pruning (\( \alpha \)) to avoid overfitting.
  </p>

  <h4>Final Tree (Rules)</h4>
  <p style="text-align:center; font-size:1.05em;">
    If \( \text{Size} &lt; 2000 \)\,: predict <b>No</b> (0); &nbsp; else: predict <b>Yes</b> (1).
  </p>

  <h4>Step 4: Predictions &amp; Evaluation</h4>
  <p>
    Apply the rule to training points (all classified correctly here). Report accuracy, precision/recall, F1; for calibrated probabilities, use leaf frequencies.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Interpretable:</b> human-readable if–then rules.</li>
    <li><b>No scaling needed:</b> handles mixed feature types and non-linear boundaries.</li>
    <li><b>Can overfit:</b> control with depth/leaf constraints or pruning (\( \alpha \)).</li>
    <li><b>Unstable alone:</b> small data changes can alter splits; ensembles (Random Forests, Gradient Boosted Trees) improve stability and accuracy.</li>
  </ul>

</section>

  <section id="nb">

  <h4>2.5 Naive Bayes</h4>
  <br>

  <b>Idea:</b>
  <p>
    Naive Bayes is a fast, probabilistic classifier that applies <b>Bayes’ rule</b> with a strong (naive) assumption:
    <em>features are conditionally independent given the class</em>. It predicts the class with the highest posterior
    probability. The goal is to <b><font color="red">estimate class posteriors from simple per-feature models</font></b>.
  </p>

  <b>Equation (Posterior via Bayes’ Rule):</b>
  <p style="text-align:center;">
    $$ P(y=c \mid \mathbf{x}) \;\propto\; P(y=c)\,\prod_{j=1}^{n} P(x_j \mid y=c). $$
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(y \in \{1,\ldots,C\}\) is the class label.</li>
    <li>\(\mathbf{x} = (x_1,\ldots,x_n)\) are features.</li>
    <li>\(P(y=c)\) is the class prior; \(P(x_j \mid y=c)\) is the class-conditional likelihood for feature \(j\).</li>
  </ul>

  <br>

  <center>
    <img src="images/nb1.png" width="800" height="300" alt="Naive Bayes: class-wise feature likelihoods multiply into posteriors">
  </center>

  <br>

  <h4>How Naive Bayes Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Choose a variant</strong> – Common choices:
      <ul>
        <li><b>Gaussian NB</b>: continuous features \(x_j \sim \mathcal{N}(\mu_{jc}, \sigma^2_{jc})\).</li>
        <li><b>Multinomial NB</b>: counts/frequencies (e.g., word counts in text) with Laplace/Lidstone smoothing.</li>
        <li><b>Bernoulli NB</b>: binary features (word present/absent).</li>
      </ul>
    </li>
    <li><strong>Estimate parameters</strong> – Priors \( \hat P(y=c)=n_c/m \). For Gaussian NB:
      \( \hat\mu_{jc}=\tfrac{1}{n_c}\sum_{i:y_i=c} x_{ij} \), 
      \( \hat\sigma^2_{jc}=\tfrac{1}{n_c}\sum_{i:y_i=c}(x_{ij}-\hat\mu_{jc})^2 \).</li>
    <li><strong>Score a new sample</strong> – Compute
      \( \log P(y=c) + \sum_j \log P(x_j \mid y=c) \) (use logs to avoid underflow).</li>
    <li><strong>Predict</strong> – Choose the class with the highest (log) posterior (MAP decision).</li>
    <li><strong>Evaluate</strong> – Accuracy, precision/recall, F1, ROC-AUC; log-loss if you care about probabilities.</li>
  </ol>

  <h4>Interpreting the Parameters</h4>
  <ul>
    <li><strong>Class priors</strong> \(P(y=c)\) reflect base rates of classes.</li>
    <li><strong>Per-class feature models</strong> capture how each feature behaves within a class
      (e.g., Gaussian mean/variance or multinomial word probabilities).</li>
    <li><strong>Smoothing (Multinomial/Bernoulli):</strong> add-\(\alpha\) (e.g., Laplace \(\alpha=1\)) prevents zero probabilities.</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p>
    <b>Gaussian NB likelihood:</b>
    \( \displaystyle P(x_j \mid y=c) = \mathcal{N}(x_j;\,\mu_{jc},\sigma^2_{jc})
      = \frac{1}{\sqrt{2\pi\sigma^2_{jc}}}\exp\!\left(-\frac{(x_j-\mu_{jc})^2}{2\sigma^2_{jc}}\right) \).
  </p>
  <p>
    <b>Multinomial NB (counts):</b>
    \( \displaystyle P(\mathbf{x}\mid y=c)=\frac{(\sum_w x_w)!}{\prod_w x_w!}\prod_w \theta_{wc}^{\,x_w} \),
    with \( \hat\theta_{wc}=\frac{N_{wc}+\alpha}{N_c+\alpha V} \).
  </p>

  <h4>Objective (MAP Rule)</h4>
  <p>
    Predict the class maximizing the posterior:
    $$ \hat y = \arg\max_c \; \log P(y=c) + \sum_{j=1}^{n} \log P(x_j \mid y=c). $$
  </p>

  <br>

  <hr>

  <h4>Worked Example: “Sold in 30 Days” (Gaussian NB)</h4>
  <p>Binary labels: 1 = Yes, 0 = No. Features: Size (sq.ft), Bedrooms.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>y</th></tr>
    <tr><td>1000</td><td>2</td><td>0</td></tr>
    <tr><td>1500</td><td>3</td><td>0</td></tr>
    <tr><td>2000</td><td>3</td><td>1</td></tr>
    <tr><td>2500</td><td>4</td><td>1</td></tr>
    <tr><td>3000</td><td>4</td><td>1</td></tr>
  </table>

  <h4>Step 1: Estimate Priors</h4>
  <p>
    \(P(y=0)=2/5=0.4\), &nbsp; \(P(y=1)=3/5=0.6\).
  </p>

  <h4>Step 2: Per-Class Gaussian Parameters</h4>
  <p><b>Class 0 (y=0):</b> points \((1000,2),(1500,3)\)</p>
  <ul>
    <li>Size: \(\mu_{0}=1250\), \(\sigma^2_{0}=62{,}500\).</li>
    <li>Bedrooms: \(\mu_{0}=2.5\), \(\sigma^2_{0}=0.25\).</li>
  </ul>
  <p><b>Class 1 (y=1):</b> points \((2000,3),(2500,4),(3000,4)\)</p>
  <ul>
    <li>Size: \(\mu_{1}=2500\), \(\sigma^2_{1}\approx 166{,}666.67\).</li>
    <li>Bedrooms: \(\mu_{1}\approx 3.6667\), \(\sigma^2_{1}\approx 0.2222\).</li>
  </ul>

  <h4>Step 3: Score a Query \(\mathbf{x}_q=(2200,\,3)\)</h4>
  <p>
    Compute class-conditional densities and multiply with priors (use logs in code).
  </p>
  <p>
    For \(y=0\): \(P(\text{Size}=2200\mid0)\approx 1.16\times10^{-6}\), \(P(\text{Beds}=3\mid0)\approx 0.484\).
    <br>
    Product \(\approx 5.6\times10^{-7}\); multiply prior \(0.4\) ⇒ \(\approx 2.3\times10^{-7}\).
  </p>
  <p>
    For \(y=1\): \(P(\text{Size}=2200\mid1)\approx 7.46\times10^{-4}\), \(P(\text{Beds}=3\mid1)\approx 0.311\).
    <br>
    Product \(\approx 2.32\times10^{-4}\); multiply prior \(0.6\) ⇒ \(\approx 1.39\times10^{-4}\).
  </p>

  <h4>Step 4: Predict</h4>
  <p>
    Compare posteriors: \(1.39\times10^{-4} \gg 2.3\times10^{-7}\) ⇒ predict <b>y = 1 (Yes)</b>.
  </p>

  <h4>Step 5: Evaluation</h4>
  <p>
    Report accuracy, precision/recall, F1; use <em>log-loss</em> for probability quality. NB is strong on small, high-dimensional,
    or text data (with Multinomial NB), but can degrade when features are highly correlated (independence assumption is violated).
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Simple and fast:</b> closed-form parameter estimates; scales well.</li>
    <li><b>Works well with little data:</b> especially Multinomial NB for text.</li>
    <li><b>Sensitive to independence violations:</b> correlated features can bias posteriors.</li>
    <li><b>Smoothing is essential</b> for Multinomial/Bernoulli to avoid zero probabilities.</li>
    <li><b>Use log-space</b> for products of probabilities to avoid underflow.</li>
  </ul>

</section>

  
  

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
