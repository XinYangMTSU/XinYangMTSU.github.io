<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 3: Supervised Learning - Classification</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    
    <li>
      <a href="#classification"><i class="fas fa-gamepad"></i>2. Classification</a>
      <div class="subsections">
        <a href="#logistic-regression">2.1 Logistic Regression</a>
        <a href="#knn">2.2 k-Nearest Neighbors (kNN)</a>
        <a href="#svm">2.3 Support Vector Machines (SVM)</a>
        <a href="#dt">2.4 Decision Tree </a>
        <a href="#nb">2.5 Naive Bayes </a>
        <a href="#nn">2.6 Neural Network </a>
      </div>
    </li>

    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 3: Supervised Learning - Classification</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>

    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li>1. Regression</li>
       <li><b>2. Classification</b></li>
    </ul>
    
  </section>
    

  <section id="supervised-cla">
    
    <div id="classification">
      <h3>2. Classification</h3>
      <p>Classification algorithms <b><font color="red">predicte categories</font> </b> (e.g., spam detection, image recognition).
      Common methods include:
      </p>
      
      <ul>
        <li><strong>Logistic Regression:</strong> Models the probability that an input belongs to a certain category; suitable for binary and multiclass problems.</li>
        <li><strong>k-Nearest Neighbors (kNN):</strong> Assigns a class based on the majority class among the k closest data points.</li>
        <li><strong>Support Vector Machines (SVM):</strong> Finds the optimal boundary (hyperplane) that best separates different classes.</li>
        <li><strong>Decision Tree Classification:</strong> Uses a tree-like structure to make decisions and assign labels.</li>
        <li><strong>Naive Bayes:</strong> Probabilistic classifier based on Bayes’ theorem; effective for text classification and spam detection.</li>
        <li><strong>Neural Network Classification:</strong> Uses artificial neural networks to model complex patterns for multi-class and binary problems.</li>
      </ul>
      
    </div>
  </section>


  <section id="logistic-regression">

  <h4>2.2 Logistic Regression</h4>
  <br>

  <b>Idea:</b>
  <p>
    Logistic Regression models the probability of a <b>binary outcome</b> (e.g., 0/1, No/Yes) from input features by passing a linear combination of the features through a
    <em>sigmoid</em> (logistic) function so the output is between 0 and 1. The goal is to
    <b><font color="red">maximize the likelihood of the observed labels (equivalently, minimize cross-entropy loss)</font></b>.
  </p>

  <b>Equation:</b>
  <p style="text-align: center;">
    \( p(y=1 \mid \mathbf{x}) = \sigma\!\big(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n\big) \),
    where \( \sigma(t) = \dfrac{1}{1 + e^{-t}} \).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li><em>y</em> ∈ {0, 1} is the class label (0 = negative class, 1 = positive class)</li>
    <li>\(\beta_0\) = intercept</li>
    <li>\(\beta_1, \beta_2, \ldots, \beta_n\) = feature coefficients</li>
    <li><em>x</em><sub>1</sub>, …, <em>x</em><sub>n</sub> = input features</li>
    <li>\(p(y=1 \mid \mathbf{x})\) = predicted probability of class 1</li>
  </ul>

  <br>

  <center>
    <img src="images/logistic1.png" width="800px" height="300px" alt="Sigmoid mapping a linear score to probability">
  </center>

  <br>

  <h4>How Logistic Regression Works</h4>

  <!-- Put this once in your page (head or before </body>) if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Gather feature vectors \((x_1,\ldots,x_n)\) and binary labels \(y \in \{0,1\}\).</li>
    <li><strong>Model the Probability</strong> – Use \( p_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}) \) to map scores to probabilities.</li>
    <li><strong>Fit the Model</strong> – Choose \(\boldsymbol{\beta}\) to <em>maximize likelihood</em> (or minimize <em>cross-entropy</em>).</li>
    <li><strong>Predict</strong> – Output probabilities \(p_i\); convert to class labels with a threshold (commonly 0.5).</li>
    <li><strong>Evaluate</strong> – Use accuracy, precision/recall, F1, ROC-AUC, and log loss.</li>
  </ol>

  <h4>Interpreting the Coefficients</h4>
  <ul>
    <li><strong>Log-odds:</strong> Logistic Regression is linear in the <em>log-odds</em>:
      \( \log \dfrac{p}{1-p} = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n \).</li>
    <li><strong>Odds ratio:</strong> A one-unit increase in \(x_j\) multiplies the odds by \(e^{\beta_j}\) (holding others fixed).</li>
    <li><strong>Sign:</strong> \(\beta_j &gt; 0\) increases the probability (for higher \(x_j\)); \(\beta_j &lt; 0\) decreases it.</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Scalar form:</b></p>
  <p>
    $$ p(y=1 \mid \mathbf{x}) = \sigma\!\big(\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n\big), \quad
       \sigma(t) = \frac{1}{1+e^{-t}}. $$
    $$ \text{logit}(p) = \log\frac{p}{1-p} = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n. $$
  </p>

  <p><b>Matrix form:</b></p>
  <p>
    $$ \mathbf{p} = \sigma(\mathbf{X}\boldsymbol{\beta}), \quad
       \mathbf{X} =
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1n} \\
        1 & x_{21} & \cdots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mn}
      \end{bmatrix}, \quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n
      \end{bmatrix}. $$
  </p>

  <h4>Objective (Maximum Likelihood / Cross-Entropy)</h4>
  <p>
    For samples \((\mathbf{x}_i, y_i)\) with \(y_i \in \{0,1\}\), the (negative) log-likelihood (a.k.a. cross-entropy loss) is
  </p>
  <p>
    $$ \mathcal{L}(\boldsymbol{\beta})
       = -\sum_{i=1}^{m}\Big[\, y_i \log p_i + (1-y_i)\log(1-p_i) \,\Big],
       \quad p_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}). $$
  </p>
  <p>
    In vector form, with \(\mathbf{p}=\sigma(\mathbf{X}\boldsymbol{\beta})\):
    $$ \mathcal{L}(\boldsymbol{\beta}) = -\big( \mathbf{y}^\top \log \mathbf{p} + (\mathbf{1}-\mathbf{y})^\top \log (\mathbf{1}-\mathbf{p}) \big). $$
  </p>

  <h4>Optimization (No Closed-Form “Normal Equation”)</h4>
  <p>
    The gradient and Hessian of the <em>negative</em> log-likelihood are
  </p>
  <p>
    $$ \nabla \mathcal{L}(\boldsymbol{\beta}) = \mathbf{X}^\top(\mathbf{p} - \mathbf{y}), \qquad
       \nabla^2 \mathcal{L}(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}, \;
       \mathbf{W} = \mathrm{diag}(p_i(1-p_i)). $$
  </p>
  <p>
    Setting the gradient to zero gives \( \mathbf{X}^\top(\mathbf{y}-\mathbf{p})=\mathbf{0} \), a nonlinear system with no closed-form solution.
    We solve it iteratively (e.g., <em>gradient descent</em>, <em>Newton–Raphson</em> / IRLS).
  </p>

  <br>

  <hr>

  <h4>Worked Example: Binary Outcome (Illustration)</h4>
  <p>Suppose we want to predict whether a house sells within 30 days (1 = Yes, 0 = No) using Size and Bedrooms.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>0</td></tr>
    <tr><td>1500</td><td>3</td><td>0</td></tr>
    <tr><td>2000</td><td>3</td><td>1</td></tr>
    <tr><td>2500</td><td>4</td><td>1</td></tr>
    <tr><td>3000</td><td>4</td><td>1</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1 & 1000 & 2 \\
      1 & 1500 & 3 \\
      1 & 2000 & 3 \\
      1 & 2500 & 4 \\
      1 & 3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      0 \\ 0 \\ 1 \\ 1 \\ 1
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Probability Model</h4>
  <p>
    For any coefficient vector \(\boldsymbol{\beta}\), compute scores \( \mathbf{z} = \mathbf{X}\boldsymbol{\beta} \) and probabilities
    \( \mathbf{p} = \sigma(\mathbf{z}) \), with \( \sigma(t)=\dfrac{1}{1+e^{-t}} \).
  </p>

  <h4>Step 3: Fit by Maximum Likelihood</h4>
  <p>
    Minimize the cross-entropy:
    $$ \min_{\boldsymbol{\beta}} \; -\sum_{i=1}^{5}\Big[\, y_i \log p_i + (1-y_i)\log(1-p_i) \,\Big], \quad p_i=\sigma(\mathbf{x}_i^\top \boldsymbol{\beta}). $$
    Use an iterative optimizer (e.g., gradient descent or IRLS) to obtain \(\hat{\boldsymbol{\beta}}\).
  </p>

  <h4>Step 4: Predictions &amp; Decisions</h4>
  <p>
    Compute \( \hat{\mathbf{p}} = \sigma(\mathbf{X}\hat{\boldsymbol{\beta}}) \).
    Classify with a threshold (e.g., predict 1 if \( \hat{p}_i \ge 0.5 \), else 0).
  </p>

  <h4>Step 5: Evaluation</h4>
  <p>
    Common metrics: accuracy, precision, recall, F1, ROC-AUC, and log loss.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li>Outputs calibrated probabilities in \([0,1]\), not raw scores.</li>
    <li>Linear decision boundary in feature space (after the logit transform).</li>
    <li>No closed-form solution; solved by iterative optimization.</li>
    <li>For regularization, use <b>Ridge (L2)</b> or <b>Lasso (L1)</b> penalties to improve generalization.</li>
  </ul>

</section>

  

  
  

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
