<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 3: Supervised Learning - Classification</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    
    <li>
      <a href="#classification"><i class="fas fa-gamepad"></i>2. Classification</a>
      <div class="subsections">
        <a href="#logistic-regression">2.1 Logistic Regression</a>
        <a href="#knn">2.2 k-Nearest Neighbors (kNN)</a>
        <a href="#svm">2.3 Support Vector Machines (SVM)</a>
        <a href="#dt">2.4 Decision Tree </a>
        <a href="#nb">2.5 Naive Bayes </a>
        <a href="#nn">2.6 Neural Network </a>
      </div>
    </li>

    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 3: Supervised Learning - Classification</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>

    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li>1. Regression</li>
       <li><b>2. Classification</b></li>
    </ul>
    
  </section>
    

  <section id="supervised-cla">
    
    <div id="classification">
      <h3>2. Classification</h3>
      <p>Classification algorithms <b><font color="red">predicte categories</font> </b> (e.g., spam detection, image recognition).
      Common methods include:
      </p>
      
      <ul>
        <li><strong>Logistic Regression:</strong> Models the probability that an input belongs to a certain category; suitable for binary and multiclass problems.</li>
        <li><strong>k-Nearest Neighbors (kNN):</strong> Assigns a class based on the majority class among the k closest data points.</li>
        <li><strong>Support Vector Machines (SVM):</strong> Finds the optimal boundary (hyperplane) that best separates different classes.</li>
        <li><strong>Decision Tree Classification:</strong> Uses a tree-like structure to make decisions and assign labels.</li>
        <li><strong>Naive Bayes:</strong> Probabilistic classifier based on Bayes’ theorem; effective for text classification and spam detection.</li>
        <li><strong>Neural Network Classification:</strong> Uses artificial neural networks to model complex patterns for multi-class and binary problems.</li>
      </ul>
      
    </div>
  </section>


  <section id="logistic-regression">

  <h4>2.1 Logistic Regression</h4>
  <br>

  <b>Idea:</b>
  <p>
    Logistic Regression models the probability of a <b>binary outcome</b> (e.g., 0/1, No/Yes) from input features by passing a linear combination of the features through a
    <em>sigmoid</em> (logistic) function so the output is between 0 and 1. The goal is to
    <b><font color="red">maximize the likelihood of the observed labels (equivalently, minimize cross-entropy loss)</font></b>.
  </p>

  <b>Equation:</b>
  <p style="text-align: center;">
    \( p(y=1 \mid \mathbf{x}) = \sigma\!\big(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n\big) \),
    where \( \sigma(t) = \dfrac{1}{1 + e^{-t}} \).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li><em>y</em> ∈ {0, 1} is the class label (0 = negative class, 1 = positive class)</li>
    <li>\(\beta_0\) = intercept</li>
    <li>\(\beta_1, \beta_2, \ldots, \beta_n\) = feature coefficients</li>
    <li><em>x</em><sub>1</sub>, …, <em>x</em><sub>n</sub> = input features</li>
    <li>\(p(y=1 \mid \mathbf{x})\) = predicted probability of class 1</li>
  </ul>

  <br>

  <center>
    <img src="images/logistic1.png" width="800px" height="300px" alt="Sigmoid mapping a linear score to probability">
  </center>

  <br>

  <h4>How Logistic Regression Works</h4>

  <!-- Put this once in your page (head or before </body>) if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Gather feature vectors \((x_1,\ldots,x_n)\) and binary labels \(y \in \{0,1\}\).</li>
    <li><strong>Model the Probability</strong> – Use \( p_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}) \) to map scores to probabilities.</li>
    <li><strong>Fit the Model</strong> – Choose \(\boldsymbol{\beta}\) to <em>maximize likelihood</em> (or minimize <em>cross-entropy</em>).</li>
    <li><strong>Predict</strong> – Output probabilities \(p_i\); convert to class labels with a threshold (commonly 0.5).</li>
    <li><strong>Evaluate</strong> – Use accuracy, precision/recall, F1, ROC-AUC, and log loss.</li>
  </ol>

  <h4>Interpreting the Coefficients</h4>
  <ul>
    <li><strong>Log-odds:</strong> Logistic Regression is linear in the <em>log-odds</em>:
      \( \log \dfrac{p}{1-p} = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n \).</li>
    <li><strong>Odds ratio:</strong> A one-unit increase in \(x_j\) multiplies the odds by \(e^{\beta_j}\) (holding others fixed).</li>
    <li><strong>Sign:</strong> \(\beta_j &gt; 0\) increases the probability (for higher \(x_j\)); \(\beta_j &lt; 0\) decreases it.</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Scalar form:</b></p>
  <p>
    $$ p(y=1 \mid \mathbf{x}) = \sigma\!\big(\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n\big), \quad
       \sigma(t) = \frac{1}{1+e^{-t}}. $$
    $$ \text{logit}(p) = \log\frac{p}{1-p} = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n. $$
  </p>

  <p><b>Matrix form:</b></p>
  <p>
    $$ \mathbf{p} = \sigma(\mathbf{X}\boldsymbol{\beta}), \quad
       \mathbf{X} =
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1n} \\
        1 & x_{21} & \cdots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mn}
      \end{bmatrix}, \quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n
      \end{bmatrix}. $$
  </p>

  <h4>Objective (Maximum Likelihood / Cross-Entropy)</h4>
  <p>
    For samples \((\mathbf{x}_i, y_i)\) with \(y_i \in \{0,1\}\), the (negative) log-likelihood (a.k.a. cross-entropy loss) is
  </p>
  <p>
    $$ \mathcal{L}(\boldsymbol{\beta})
       = -\sum_{i=1}^{m}\Big[\, y_i \log p_i + (1-y_i)\log(1-p_i) \,\Big],
       \quad p_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}). $$
  </p>
  <p>
    In vector form, with \(\mathbf{p}=\sigma(\mathbf{X}\boldsymbol{\beta})\):
    $$ \mathcal{L}(\boldsymbol{\beta}) = -\big( \mathbf{y}^\top \log \mathbf{p} + (\mathbf{1}-\mathbf{y})^\top \log (\mathbf{1}-\mathbf{p}) \big). $$
  </p>

  <h4>Optimization (No Closed-Form “Normal Equation”)</h4>
  <p>
    The gradient and Hessian of the <em>negative</em> log-likelihood are
  </p>
  <p>
    $$ \nabla \mathcal{L}(\boldsymbol{\beta}) = \mathbf{X}^\top(\mathbf{p} - \mathbf{y}), \qquad
       \nabla^2 \mathcal{L}(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}, \;
       \mathbf{W} = \mathrm{diag}(p_i(1-p_i)). $$
  </p>
  <p>
    Setting the gradient to zero gives \( \mathbf{X}^\top(\mathbf{y}-\mathbf{p})=\mathbf{0} \), a nonlinear system with no closed-form solution.
    We solve it iteratively (e.g., <em>gradient descent</em>, <em>Newton–Raphson</em> / IRLS).
  </p>

  <br>

  <hr>

  <h4>Worked Example: Binary Outcome (Illustration)</h4>
  <p>Suppose we want to predict whether a house sells within 30 days (1 = Yes, 0 = No) using Size and Bedrooms.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>0</td></tr>
    <tr><td>1500</td><td>3</td><td>0</td></tr>
    <tr><td>2000</td><td>3</td><td>1</td></tr>
    <tr><td>2500</td><td>4</td><td>1</td></tr>
    <tr><td>3000</td><td>4</td><td>1</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1 & 1000 & 2 \\
      1 & 1500 & 3 \\
      1 & 2000 & 3 \\
      1 & 2500 & 4 \\
      1 & 3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      0 \\ 0 \\ 1 \\ 1 \\ 1
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Probability Model</h4>
  <p>
    For any coefficient vector \(\boldsymbol{\beta}\), compute scores \( \mathbf{z} = \mathbf{X}\boldsymbol{\beta} \) and probabilities
    \( \mathbf{p} = \sigma(\mathbf{z}) \), with \( \sigma(t)=\dfrac{1}{1+e^{-t}} \).
  </p>

  <h4>Step 3: Fit by Maximum Likelihood</h4>
  <p>
    Minimize the cross-entropy:
    $$ \min_{\boldsymbol{\beta}} \; -\sum_{i=1}^{5}\Big[\, y_i \log p_i + (1-y_i)\log(1-p_i) \,\Big], \quad p_i=\sigma(\mathbf{x}_i^\top \boldsymbol{\beta}). $$
    Use an iterative optimizer (e.g., gradient descent or IRLS) to obtain \(\hat{\boldsymbol{\beta}}\).
  </p>

  <h4>Step 4: Predictions &amp; Decisions</h4>
  <p>
    Compute \( \hat{\mathbf{p}} = \sigma(\mathbf{X}\hat{\boldsymbol{\beta}}) \).
    Classify with a threshold (e.g., predict 1 if \( \hat{p}_i \ge 0.5 \), else 0).
  </p>

  <h4>Step 5: Evaluation</h4>
  <p>
    Common metrics: accuracy, precision, recall, F1, ROC-AUC, and log loss.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li>Outputs calibrated probabilities in \([0,1]\), not raw scores.</li>
    <li>Linear decision boundary in feature space (after the logit transform).</li>
    <li>No closed-form solution; solved by iterative optimization.</li>
    <li>For regularization, use <b>Ridge (L2)</b> or <b>Lasso (L1)</b> penalties to improve generalization.</li>
  </ul>

</section>

  
<section id="knn">

  <h4>2.2 k-Nearest Neighbors (kNN)</h4>
  <br>

  <b>Idea:</b>
  <p>
    k-Nearest Neighbors is a <b>non-parametric, instance-based</b> method. To make a prediction for a new point,
    it looks at the <b>k closest training examples</b> and combines their labels/values:
    majority vote for classification, average for regression.
    The goal is to <b><font color="red">use local neighborhoods to predict</font></b> rather than fitting a global line or plane.
  </p>

  <b>Equation (Distance & Rules):</b>
  <p style="text-align:center;">
    Euclidean distance:
    \[
      d(\mathbf{x}_i,\mathbf{x}_q) \;=\; \sqrt{\sum_{j=1}^{n} (x_{ij} - x_{qj})^2}
    \]
    (Common alternatives: Manhattan \( \sum_j |x_{ij}-x_{qj}| \), Minkowski.)
  </p>
  <p><b>Prediction (Classification):</b> \(\hat{y} = \mathrm{mode}\{\,y_{i_1},\ldots,y_{i_k}\,\}\). Weighted variant:
    \(\hat{p} = \dfrac{\sum_{r=1}^{k} w_r \, y_{i_r}}{\sum_{r=1}^{k} w_r}\), with \(w_r = \dfrac{1}{d_r + \varepsilon}\).
  </p>
  <p><b>Prediction (Regression):</b> \(\hat{y} = \dfrac{1}{k}\sum_{r=1}^{k} y_{i_r}\). Weighted:
    \(\hat{y} = \dfrac{\sum_{r=1}^{k} w_r \, y_{i_r}}{\sum_{r=1}^{k} w_r}\).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li><em>k</em> = number of neighbors (hyperparameter)</li>
    <li>Distance metric (Euclidean by default)</li>
    <li>Weights (uniform or distance-weighted)</li>
  </ul>

  <br>

  <center>
    <img src="images/knn1.png" width="800px" height="300px" alt="kNN neighborhoods around a query point">
  </center>

  <br>

  <h4>How kNN Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Keep the training examples in memory (lazy learning; no parameter fitting).</li>
    <li><strong>Choose k, distance, weights</strong> – Often selected via cross-validation.</li>
    <li><strong>Find neighbors</strong> – For a query point \( \mathbf{x}_q \), compute distances to all training points and pick the k smallest.</li>
    <li><strong>Aggregate</strong> – Classification: majority vote; Regression: average (optionally distance-weighted).</li>
    <li><strong>Evaluate</strong> – Use accuracy/F1/AUC for classification, or MSE/RMSE/MAE for regression.</li>
  </ol>

  <h4>Interpreting the Parameters</h4>
  <ul>
    <li><strong>k:</strong> Small k → low bias, high variance (sensitive/noisy). Large k → higher bias, smoother predictions.</li>
    <li><strong>Distance weighting:</strong> Nearer points get more influence (e.g., \(w=1/(d+\varepsilon)\)).</li>
    <li><strong>Feature scaling:</strong> <b>Standardize</b> features; otherwise large-scale features dominate distance.</li>
  </ul>

  <br>

  <h4>Model & Notation</h4>
  <p>
    Given training set \(\{(\mathbf{x}_i, y_i)\}_{i=1}^m\) and a query \(\mathbf{x}_q\), let
    \(i_1,\ldots,i_k\) index the k nearest neighbors by a chosen distance \(d(\cdot,\cdot)\).
  </p>
  <p>
    <b>kNN-Classification:</b>
    \(\hat{y} = \mathrm{mode}\{y_{i_1},\ldots,y_{i_k}\}\), or probability
    \(\hat{p} = \frac{\sum_r w_r \, y_{i_r}}{\sum_r w_r}\).
  </p>
  <p>
    <b>kNN-Regression:</b>
    \(\hat{y} = \frac{1}{k}\sum_r y_{i_r}\) (uniform), or
    \(\hat{y} = \frac{\sum_r w_r \, y_{i_r}}{\sum_r w_r}\) (distance-weighted).
  </p>

  <h4>Objective</h4>
  <p>
    kNN has <b>no explicit training objective</b> (no coefficients to fit). It is a <em>lazy learner</em>:
    all computation happens at prediction time. The “objective” is implicit—use local neighborhoods to minimize local error.
  </p>

  <br>

  <hr>

  <h4>Worked Example: Predicting House Price (kNN-Regression)</h4>
  <p>Use the same data as before. Predict the price for a query house with <b>Size = 2200</b> sq.ft and <b>Bedrooms = 3</b>.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
    <tr><td>1000</td><td>2</td><td>200,000</td></tr>
    <tr><td>1500</td><td>3</td><td>280,000</td></tr>
    <tr><td>2000</td><td>3</td><td>340,000</td></tr>
    <tr><td>2500</td><td>4</td><td>400,000</td></tr>
    <tr><td>3000</td><td>4</td><td>460,000</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\), \(\mathbf{y}\), and the query \(\mathbf{x}_q\)</h4>
  <p>
    \(\mathbf{X} = \begin{bmatrix}
      1000 & 2\\
      1500 & 3\\
      2000 & 3\\
      2500 & 4\\
      3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} = \begin{bmatrix}
      200000\\ 280000\\ 340000\\ 400000\\ 460000
    \end{bmatrix},\quad
    \mathbf{x}_q = \begin{bmatrix}2200 & 3\end{bmatrix}.
  \)
  </p>

  <h4>Step 2: Distances to the Query (Euclidean)</h4>
  <p>
    \( d_i = \sqrt{(\text{Size}_i - 2200)^2 + (\text{Bed}_i - 3)^2} \).
  </p>
  <table border="1" cellpadding="6">
    <tr><th>Training point</th><th>(Size, Beds)</th><th>Distance to (2200,3)</th></tr>
    <tr><td>1</td><td>(1000, 2)</td><td>≈ 1200.00</td></tr>
    <tr><td>2</td><td>(1500, 3)</td><td>700.00</td></tr>
    <tr><td>3</td><td>(2000, 3)</td><td>200.00</td></tr>
    <tr><td>4</td><td>(2500, 4)</td><td>≈ 300.00</td></tr>
    <tr><td>5</td><td>(3000, 4)</td><td>≈ 800.00</td></tr>
  </table>

  <h4>Step 3: Choose k and Neighbors</h4>
  <p>
    Let \(k=3\). The 3 nearest neighbors are points #3 (340,000), #4 (400,000), and #2 (280,000).
  </p>

  <h4>Step 4: Predict (Regression)</h4>
  <p>
    <b>Uniform kNN:</b>
    \(\hat{y} = \frac{340{,}000 + 400{,}000 + 280{,}000}{3} = 340{,}000.\)
  </p>
  <p>
    <b>Distance-weighted kNN</b> (with \(w_i = 1/(d_i+\varepsilon)\)):
    \[
      \hat{y}
      \approx \frac{ \frac{340{,}000}{200} + \frac{400{,}000}{300} + \frac{280{,}000}{700} }
      { \frac{1}{200} + \frac{1}{300} + \frac{1}{700} }
      \;\approx\; 351{,}707.
    \]
  </p>

  <p class="note">
    <b>Scaling matters:</b> Size is measured in thousands while Bedrooms is small, so distance is dominated by Size.
    In practice, <b>standardize</b> features before kNN.
  </p>

  <br>

  <h4>Classification Note (Same Setup)</h4>
  <p>
    If the target were binary (e.g., Sold in 30 days: 0/1), the same neighbors would vote.
    With labels \([1,1,0]\) among the 3 nearest, the majority is <b>1 (Yes)</b>, and a simple probability estimate is \(2/3 \approx 0.67\).
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Non-parametric, lazy:</b> No training fit; predicts via local neighborhoods.</li>
    <li><b>Hyperparameters:</b> k, distance metric, and weighting—choose via cross-validation.</li>
    <li><b>Feature scaling is essential.</b> Consider standardization or normalization.</li>
    <li><b>Computational cost:</b> Naïve prediction is \(O(mn)\) per query; KD-trees/ball trees/ANN speed this up.</li>
    <li><b>Curses in high-D:</b> Distances become less informative as dimension grows (curse of dimensionality).</li>
  </ul>

</section>


  <section id="svm">

  <h4>2.3 Support Vector Machines (SVM)</h4>
  <br>

  <b>Idea:</b>
  <p>
    Support Vector Machines are margin-based classifiers. They find a decision boundary that <b>maximizes the margin</b>
    (distance) between classes while allowing some misclassifications when needed. The goal is to
    <b><font color="red">separate classes with the widest possible margin</font></b> (for robustness), and optionally use
    <em>kernels</em> to separate data that are not linearly separable.
  </p>

  <b>Equation (Linear SVM decision function):</b>
  <p style="text-align:center;">
    Decision score: \(\; f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b \;\) &nbsp;&nbsp;&nbsp;
    Prediction: \(\; \hat{y} = \mathrm{sign}\!\big(f(\mathbf{x})\big)\).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(\mathbf{w}\) = weight vector; \(b\) = bias (intercept)</li>
    <li>\(\mathbf{x} = (x_1,\ldots,x_n)\) = feature vector</li>
    <li>\(\hat{y} \in \{-1,+1\}\) = predicted class label</li>
    <li>For non-linear SVM, replace inner products \(\mathbf{x}_i^\top \mathbf{x}_j\) with a kernel \(K(\mathbf{x}_i,\mathbf{x}_j)\)</li>
  </ul>

  <br>

  <center>
    <img src="images/svm1.png" width="800px" height="300px" alt="SVM maximum-margin separator with support vectors">
  </center>

  <br>

  <h4>How SVM Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Feature vectors \(\mathbf{x}_i\) with labels \(y_i \in \{-1,+1\}\).</li>
    <li><strong>Choose linear or kernel SVM</strong> – Linear for high-dimensional but roughly linear data; kernels (RBF/poly) for curved boundaries.</li>
    <li><strong>Fit the model</strong> – Optimize a margin objective with regularization parameter \(C\) (trade-off: margin size vs. training errors).</li>
    <li><strong>Predict</strong> – Use the sign of the decision score \(f(\mathbf{x})\). (Scores are margins, not probabilities.)</li>
    <li><strong>Evaluate</strong> – Accuracy, precision/recall, F1, ROC-AUC; optionally calibrate probabilities (e.g., Platt scaling).</li>
  </ol>

  <h4>Interpreting the Model</h4>
  <ul>
    <li><strong>Support vectors</strong> – Training points on/inside the margin; they alone determine the boundary.</li>
    <li><strong>Margin</strong> – Width \(= \dfrac{2}{\|\mathbf{w}\|}\); maximizing margin improves robustness.</li>
    <li><strong>Parameter \(C\)</strong> – Larger \(C\) penalizes misclassification more (narrower margin, lower bias, higher variance). Smaller \(C\) allows a wider margin (higher bias, lower variance).</li>
    <li><strong>Kernel</strong> – Implicitly maps features to a higher-dimensional space: \(K(\mathbf{x},\mathbf{z}) = \phi(\mathbf{x})^\top \phi(\mathbf{z})\).</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Primal (soft-margin, linear SVM):</b></p>
  <p>
    $$ \min_{\mathbf{w},\,b,\,\boldsymbol{\xi}\ge 0} \;\; \frac{1}{2}\|\mathbf{w}\|_2^2 + C \sum_{i=1}^{m} \xi_i
       \quad \text{s.t.}\quad y_i \big(\mathbf{w}^\top \mathbf{x}_i + b\big) \ge 1 - \xi_i. $$
  </p>

  <p><b>Unconstrained hinge-loss form:</b></p>
  <p>
    $$ \min_{\mathbf{w},\,b} \;\; \frac{1}{2}\|\mathbf{w}\|_2^2 \;+\; C \sum_{i=1}^{m} \max\!\big(0,\, 1 - y_i(\mathbf{w}^\top \mathbf{x}_i + b)\big). $$
  </p>

  <p><b>Dual with kernels (decision rule):</b></p>
  <p>
    $$ \max_{\boldsymbol{\alpha}} \;\; \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j K(\mathbf{x}_i,\mathbf{x}_j),
       \quad 0 \le \alpha_i \le C,\; \sum_i \alpha_i y_i = 0, $$
    $$ f(\mathbf{x}) = \sum_{i \in \mathcal{SV}} \alpha_i y_i\, K(\mathbf{x}_i,\mathbf{x}) + b. $$
  </p>

  <br>

  <hr>

  <h4>Worked Example: “Sold in 30 days” (Binary)</h4>
  <p>We reuse the small dataset; set \(y = -1\) for “No”, \(y = +1\) for “Yes”.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>-1</td></tr>
    <tr><td>1500</td><td>3</td><td>-1</td></tr>
    <tr><td>2000</td><td>3</td><td>+1</td></tr>
    <tr><td>2500</td><td>4</td><td>+1</td></tr>
    <tr><td>3000</td><td>4</td><td>+1</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1000 & 2 \\
      1500 & 3 \\
      2000 & 3 \\
      2500 & 4 \\
      3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      -1 \\ -1 \\ +1 \\ +1 \\ +1
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Choose model &amp; hyperparameters</h4>
  <ul>
    <li>Scale features (recommended).</li>
    <li>Linear SVM with \(C=1\) (baseline). Try RBF kernel with \(\gamma\) tuned via cross-validation if the boundary is curved.</li>
  </ul>

  <h4>Step 3: Fit</h4>
  <p>
    Train the SVM on \((\mathbf{X},\mathbf{y})\). The support vectors will be the examples closest to the boundary (in this toy set, likely near the transition around 2000–2500 sq.ft).
  </p>

  <h4>Step 4: Predict</h4>
  <p>
    For a new house \(\mathbf{x}_q=(2200,3)\), compute \(f(\mathbf{x}_q)\). If \(f(\mathbf{x}_q)&gt;0\) predict <b>+1 (Yes)</b>, else <b>-1 (No)</b>.
  </p>

  <h4>Step 5: Evaluation</h4>
  <p>
    Report accuracy, precision/recall, F1, ROC-AUC. For probability-like outputs, calibrate scores (e.g., Platt scaling) after training.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Maximum-margin:</b> promotes robustness to small perturbations.</li>
    <li><b>Support vectors only:</b> the decision boundary depends on a subset of training points.</li>
    <li><b>\(C\) controls bias–variance:</b> large \(C\) fits training more tightly; small \(C\) widens the margin.</li>
    <li><b>Kernels:</b> RBF, polynomial, and others enable non-linear boundaries via the kernel trick.</li>
    <li><b>Scaling is essential:</b> SVMs are sensitive to feature scales.</li>
    <li><b>Outputs are margins, not probabilities:</b> calibrate if you need probabilities.</li>
  </ul>

</section>

  
  

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
