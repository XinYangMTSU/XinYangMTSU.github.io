<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 3: Supervised Learning - Regression</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */(no penalty) 
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    
    <li>
      <a href="#regression"><i class="fas fa-gamepad"></i>2. Regression</a>
      <div class="subsections">
        <a href="#linear-regression">2.1 Linear Regression</a>
        <a href="#ridge-regression">2.2 Ridge Regression</a>
        <a href="#lasso-regression">2.3 Lasso Regression</a>
      </div>
    </li>
    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 3: Supervised Learning - Regression</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>

    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li><b>1. Regression</b></li>
       <li>2. Classification</li>
    </ul>
    
  </section>
    
 
  <section id="supervised-reg">
    
  <div id="regression">  
      <h2>2. Regression</h2>
      <p>Regression algorithms <b><font color="red">predict numeric values</font></b> (e.g., house prices, temperature).
      Common methods include:
      </p>
      <ul>
        <li><strong>Linear Regression:</strong> Predicts a continuous outcome based on a linear relationship between input features and the target variable.</li>
        <li><strong>Ridge Regression (L2 Regularization): Linear regression with a penalty for large coefficients to prevent overfitting.</strong></li>
        <li><strong>Lasso Regression (L1 Regularization): Similar to ridge, but can shrink some coefficients to zero, performing feature selection.</strong></li>
      </ul>
  </div>

  </section>


    <section id="linear-regression">
      
      <h4>2.1 Linear Regression</h4>
      <br>
      <b>Idea:</b>
      <p>
        Fits a straight line (in two dimensions) or a hyperplane (in higher dimensions) that best describes the relationship between 
        input features and the target variable. 
        The goal is to <b><font color="red">minimize the difference between the predicted values and the actual values</font></b>.
      </p>
    
      <b>Equation:</b>

      <p style="text-align: center;">
          y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + &hellip; + β<sub>p</sub>x<sub>p</sub> + ε
      </p>

    <p><strong>Where:</strong></p>
    <ul>
      <li><em>y</em> = predicted output</li>
      <li>β<sub>0</sub> = intercept (value of <em>y</em> when all <em>x</em> are zero). Baseline prediction when all features are zero (unpenalized)</li>
      <li>β<sub>1</sub>, β<sub>2</sub>, …, β<sub>p</sub> = coefficients (weights for each feature). Effect of feature \(x_j\) on \(y\) when others are held fixed.</li>
      <li><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub>p</sub> = input features</li>
      <li><strong>Sparsity</strong> – If \(\beta_j = 0\), feature \(x_j\) is effectively removed by the model.</li>
      <li><i>\( p \)</i> means the number of input feature.</li>
      <li><em>ε</em> = error term (difference between predicted and actual values)</li>
    </ul>

  <br>
    
  <center>
    <img src="images/regression1.png" width="800px" height="300px">
  </center>

  <h4>Interactive: Linear Regression</h4>
  <ul>
    <li><a href="https://xinyangmtsu.github.io/3350/LR_2D.html" target="_blank">2D Linear Regression: OLS Best-Fit Line</a></li>
    <li><a href="https://xinyangmtsu.github.io/3350/LR_3D.html" target="_blank">3D Linear Regression: OLS Best-Fit Plane</a></li>
  </ul>
      
      
    <br>
    
    <h4>How Linear Regression Works</h4>
    
    <!-- Put this once in your page (head or before </body>) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
<ol>
  <li><strong>Collect Data</strong> – Gather pairs of input features (\(x_1, x_2, \ldots, x_p\)) and the corresponding output (\(y\)).</li>
  <li><strong>Define the Model</strong> – Assume a linear relationship:
    \[
    \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p
    \]
    where the \(\beta\)'s are unknown parameters.</li>
  <li><strong>Estimate Parameters by Minimizing Error</strong> – Ordinary Least Squares (OLS) chooses the values of \(\beta_0, \beta_1, \ldots, \beta_p\) that minimize the sum of squared errors:
    \[
    \mathrm{SSE} = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
    \]
    This ensures the predictions \(\hat{y}_i\) are as close as possible to the actual values \(y_i\).</li>
</ol>
      
    <br>
    
  <h4>Model & Notation</h4>
  <p>Scalar form:</p>
  <p>$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon $$</p>

  <p>Matrix form:</p>
  <p>
    $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
    where
    $$ \mathbf{X} = 
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1p} \\
        1 & x_{21} & \cdots & x_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mp}
      \end{bmatrix},\quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
      \end{bmatrix},\quad
      \mathbf{y} =
      \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_m
      \end{bmatrix}.
    $$
  </p>


<h4>Objective Function</h4>

<b>What is an Objective Function?</b>
<ul>
  <li>In <strong>mathematics, optimization, and machine learning</strong>, the objective function is the function we want to 
      <strong>optimize</strong> (minimize or maximize).</li>
  <li>It represents the <em>goal</em> of the learning or fitting process.</li>
</ul>
<br>
<b>In Linear Regression:</b>
<ul>
  <li>The goal is to make predictions \(\hat{y}_i\) close to the true values \(y_i\).</li>
  <li>The <strong>Sum of Squared Errors (SSE)</strong> is defined as the objective function.</li>
</ul>

<p><b>Formula (scalar form):</b></p>
<p style="text-align: center;">
  \[
  \mathrm{SSE}(\beta) = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
  \]
</p>

<p><b>Where:</b></p>
<ul>
  <li>\(y_i\) = actual observed value</li>
  <li>\(\hat{y}_i\) = predicted value (model output)</li>
  <li>\(m\) = number of data points</li>
</ul>

<p><b>Vector/Matrix Notation:</b></p>
<p style="text-align: center;">
  \[
  \mathrm{SSE}(\beta) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
  \]
</p>

<p><b>Where:</b></p>
<ul>
  <li>\(\mathbf{y}\) = vector of observed values</li>
  <li>\(\mathbf{X}\) = design matrix (input features)</li>
  <li>\(\boldsymbol{\beta}\) = vector of coefficients (model parameters)</li>
</ul>

<p><b>Residual Vector Interpretation:</b></p>
<p>
If
\[
\mathbf{r} =
\begin{bmatrix}
  r_1 \\ r_2 \\ \vdots \\ r_m
\end{bmatrix},
\quad \text{then} \quad
\mathbf{r}^\top \mathbf{r} =
\begin{bmatrix}
  r_1 & r_2 & \dots & r_m
\end{bmatrix}
\begin{bmatrix}
  r_1 \\ r_2 \\ \vdots \\ r_m
\end{bmatrix}
= r_1^2 + r_2^2 + \cdots + r_m^2
\]
</p>

<p>This shows that \(\mathbf{r}^\top \mathbf{r}\) is exactly the <strong>Sum of Squared Errors (SSE)</strong>.</p>
      
 <!--   -->

    <p>Residual Vector:
      $$
      \mathbf{r} \;=\; \mathbf{y} \;-\; \mathbf{X}\,\boldsymbol{\beta}
      $$
    </p>

    <p>Then:
      $$
      \mathbf{r}^\top \mathbf{r}
      \;=\;
      \sum_{i=1}^{m} \bigl(y_i - \hat{y}_i\bigr)^2
      $$
    </p>

    <br>


   <h4>Solving for \( \beta \): The Closed-Form OLS Solution in Linear Regression</h4>

<p>After defining the objective function (SSE), the next step is to solve for the coefficients 
\(\boldsymbol{\beta}\) that minimize this function. This is done using the 
<strong>Ordinary Least Squares (OLS)</strong> method.</p>

<p><b>Step 1: Express SSE in matrix form</b></p>
<p>
  $$
  \mathrm{SSE} = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
  $$
</p>

<p><b>Step 2: Expand the quadratic form</b></p>
<p>
  $$
  =\;\mathbf{y}^\top \mathbf{y}
  \;-\;\mathbf{y}^\top \mathbf{X}\boldsymbol{\beta}
  \;-\;(\mathbf{X}\boldsymbol{\beta})^\top \mathbf{y}
  \;+\;(\mathbf{X}\boldsymbol{\beta})^\top (\mathbf{X}\boldsymbol{\beta})
  $$
</p>

<p><b>Step 3: Simplify using transpose rules</b></p>
<ul>
  <li>Transpose rules:\((AB)^\top = B^\top A^\top\). So, we get:</li>

  $$
        (\mathbf{X}\boldsymbol{\beta})^\top\mathbf{y}
        \;=\; \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
  $$
  
  $$
  (\mathbf{X}\boldsymbol{\beta})^\top (\mathbf{X}\boldsymbol{\beta})
  = \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
  $$
  
  <li>A scalar equals its transpose. So, we get:</li> 
   
   $$
        \mathbf{y}^\top \mathbf{X} \boldsymbol{\beta}
        = (\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta})^\top
        = \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y}
    $$

<li>Therefore:</li>
      
<p>
   $$
        - \mathbf{y}^\top\mathbf{X}\boldsymbol{\beta} \;-\; (\mathbf{X}\boldsymbol{\beta})^\top\mathbf{y}
        \;=\; - \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} \;-\; \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
        \;=\; -2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
    $$
</p>

<li>Resulting SSE:</li>
<p>
  $$
  \mathrm{SSE}
  = \mathbf{y}^\top \mathbf{y}
  - 2\,\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y}
  + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
  $$
</p>
</ul>
      
<p><b>Step 4: Differentiate with respect to \(\boldsymbol{\beta}\)</b></p>

To minimize the SSE, we compute the derivative of the SSE with respect to \(\boldsymbol{\beta}\), which gives us:
<p>
  $$
  \frac{\partial \,\mathrm{SSE}}{\partial \boldsymbol{\beta}}
  = -2\,\mathbf{X}^\top \mathbf{y}
  + 2\,\mathbf{X}^\top \mathbf{X}\,\boldsymbol{\beta}
  $$
</p>

<p><b>Step 5: Set gradient = 0 and solve</b></p>
<p>
  $$
  -2\,\mathbf{X}^\top \mathbf{y} + 2\,\mathbf{X}^\top \mathbf{X}\,\hat{\boldsymbol{\beta}} \;=\; \mathbf{0}
  $$
  $$
   \;\;\Longrightarrow\;\;
  $$
  $$
  \mathbf{X}^\top \mathbf{X}\,\hat{\boldsymbol{\beta}} = \mathbf{X}^\top \mathbf{y}
  $$
</p>

<p><b>Step 6: Closed-form solution (Normal Equation)</b></p>
<p>
  $$
  \boxed{
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1}\,\mathbf{X}^\top \mathbf{y}
  }
  $$
</p>

<p><b>Where:</b></p>
<ul>
  <li>\(\hat{\boldsymbol{\beta}}\): estimated coefficients (parameters)</li>
  <li>\(\mathbf{X}^\top \mathbf{X}\): feature covariance matrix</li>
  <li>\(\mathbf{X}^\top \mathbf{y}\): correlation between features and target</li>
</ul>

<h4>Key Points:</h4>
<ul>
  <li>The <strong>Normal Equation</strong> provides a closed-form solution for the coefficients that minimize SSE.</li>
  <li>Computing the inverse of \(\mathbf{X}^\top \mathbf{X}\) can be expensive for large datasets.
  For very large datasets, more efficient methods like gradient descent are often used.</li>
</ul>     

<!-- -->
<!-- -->
  <hr>

  <h4>Worked Example: Predicting House Price</h4>
  <p>Model: 
    $$ \text{Price} = \beta_0 + \beta_1 \cdot \text{Size} + \beta_2 \cdot \text{Bedrooms} + \epsilon $$
  </p>

  <h4>Data</h4>

  <center>  
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
    <tr><td>1000</td><td>2</td><td>200,000</td></tr>
    <tr><td>1500</td><td>3</td><td>280,000</td></tr>
    <tr><td>2000</td><td>3</td><td>340,000</td></tr>
    <tr><td>2500</td><td>4</td><td>400,000</td></tr>
    <tr><td>3000</td><td>4</td><td>460,000</td></tr>
  </table>
  </center>  
      
  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1 & 1000 & 2 \\
      1 & 1500 & 3 \\
      1 & 2000 & 3 \\
      1 & 2500 & 4 \\
      1 & 3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Compute \( \mathbf{X}^\top \mathbf{X} \) and \( \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  In this step, we calculate two important matrices that will help us solve for the coefficients \( \boldsymbol{\beta} \):
</p>
<ul>
  <li><b>\( \mathbf{X}^\top \mathbf{X} \)</b>: The matrix product of the transpose of the design matrix \( \mathbf{X} \) and \( \mathbf{X} \) itself.</li>
  <li><b>\( \mathbf{X}^\top \mathbf{y} \)</b>: The matrix product of the transpose of \( \mathbf{X} \) and the observed values \( \mathbf{y} \).</li>
</ul>

<h4>Transpose of \( \mathbf{X} \)</h4>
<p>
  The transpose of the design matrix \( \mathbf{X} \), denoted \( \mathbf{X}^\top \), is obtained by flipping the rows and columns of \( \mathbf{X} \):
</p>
<p>
  $$ 
  \mathbf{X}^\top =
  \begin{bmatrix}
  1 & 1 & 1 & 1 & 1 \\
  1000 & 1500 & 2000 & 2500 & 3000 \\
  2 & 3 & 3 & 4 & 4
  \end{bmatrix}
  $$
</p>

<h4>Matrix Multiplication: \( \mathbf{X}^\top \mathbf{X} \)</h4>
<p>
  Now, multiply \( \mathbf{X}^\top \) by \( \mathbf{X} \) to get the matrix product \( \mathbf{X}^\top \mathbf{X} \):
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  1 & 1 & 1 & 1 & 1 \\
  1000 & 1500 & 2000 & 2500 & 3000 \\
  2 & 3 & 3 & 4 & 4
  \end{bmatrix}
  \begin{bmatrix}
  1 & 1000 & 2 \\
  1 & 1500 & 3 \\
  1 & 2000 & 3 \\
  1 & 2500 & 4 \\
  1 & 3000 & 4
  \end{bmatrix}
  $$
</p>
<p>
  The result of multiplying these matrices gives us the following:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 & 34500 \\
  16 & 34500 & 54
  \end{bmatrix}
  $$
</p>

<h4>Matrix Multiplication: \( \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  Next, multiply \( \mathbf{X}^\top \) by \( \mathbf{y} \) to get the matrix product \( \mathbf{X}^\top \mathbf{y} \):
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1 & 1 & 1 & 1 & 1 \\
  1000 & 1500 & 2000 & 2500 & 3000 \\
  2 & 3 & 3 & 4 & 4
  \end{bmatrix}
  \begin{bmatrix}
  200000 \\
  280000 \\
  340000 \\
  400000 \\
  460000
  \end{bmatrix}
  $$
</p>
<p>
  The result of this multiplication gives the following vector:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1680000 \\
  3680000000 \\
  5700000
  \end{bmatrix}
  $$
</p>

<h4>Summary of Step 2 Results:</h4>
<ul>
  <li><b>\( \mathbf{X}^\top \mathbf{X} \)</b>: 
    $$ 
    \begin{bmatrix}
    5 & 10000 & 16 \\
    10000 & 22500000 & 34500 \\
    16 & 34500 & 54
    \end{bmatrix}
    $$
  </li>
  <li><b>\( \mathbf{X}^\top \mathbf{y} \)</b>: 
    $$ 
    \begin{bmatrix}
    1680000 \\
    3680000000 \\
    5700000
    \end{bmatrix}
    $$
  </li>
</ul>

<p>These matrices will help us solve for the optimal coefficients in the next step using the Normal Equation:</p>
<p>
  $$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
</p>


  <h4>Step 3: Solve for \( \hat{\boldsymbol{\beta}} \)</h4>
<p>
  In this step, we solve for the optimal coefficients \( \hat{\boldsymbol{\beta}} \) using the **Normal Equation**:
</p>
<p>
  $$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
</p>

<h4>Computing \( (\mathbf{X}^\top \mathbf{X})^{-1} \)</h4>
<p>
  The first part of the equation is computing the inverse of \( \mathbf{X}^\top \mathbf{X} \). We already have \( \mathbf{X}^\top \mathbf{X} \) from Step 2:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 & 34500 \\
  16 & 34500 & 54
  \end{bmatrix}
  $$
</p>
<p>
  Using matrix algebra (or a calculator for matrix inversion), we compute the inverse of \( \mathbf{X}^\top \mathbf{X} \):
</p>
<p>
  $$
  (\mathbf{X}^\top \mathbf{X})^{-1} =
  \begin{bmatrix}
  0.000111 \, & -4.44 \times 10^{-6} \, & -0.00018 \\
  -4.44 \times 10^{-6} \, & 4.44 \times 10^{-11} \, & 6.44 \times 10^{-7} \\
  -0.00018 \, & 6.44 \times 10^{-7} \, & 0.000057
  \end{bmatrix}
  $$
</p>

<h4>Matrix Multiplication: \( (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  Now, multiply the inverse of \( \mathbf{X}^\top \mathbf{X} \) by \( \mathbf{X}^\top \mathbf{y} \). We have \( \mathbf{X}^\top \mathbf{y} \) from Step 2:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1680000 \\
  3680000000 \\
  5700000
  \end{bmatrix}
  $$
</p>
<p>
  Multiply the two matrices to obtain the vector of coefficients \( \hat{\boldsymbol{\beta}} \):
</p>
<p>
  $$
  \hat{\boldsymbol{\beta}} =
  \begin{bmatrix}
  0.000111 \, & -4.44 \times 10^{-6} \, & -0.00018 \\
  -4.44 \times 10^{-6} \, & 4.44 \times 10^{-11} \, & 6.44 \times 10^{-7} \\
  -0.00018 \, & 6.44 \times 10^{-7} \, & 0.000057
  \end{bmatrix}
  \begin{bmatrix}
  1680000 \\
  3680000000 \\
  5700000
  \end{bmatrix}
  $$
</p>
<p>
  The result of this multiplication gives us the estimated coefficients:
</p>
<p>
  $$ \hat{\beta}_0 = 64{,}000,\quad \hat{\beta}_1 \approx 114.67,\quad \hat{\beta}_2 \approx 13{,}333.33 $$
</p>

<h4>Final Model</h4>
<p style="text-align:center; font-size:1.1em;">
  $$ \widehat{\text{Price}} = 64{,}000 + 114.67 \times \text{Size} + 13{,}333.33 \times \text{Bedrooms} $$
</p>

<p>
  Now we have the final model for predicting the house price. This model can be used to predict the price of houses based on their size (in square feet) and the number of bedrooms.
</p>

  <h4>Step 5: Predictions \((\hat{\mathbf{y}})\) & Errors</h4>
  <p>
    $$ 
    \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      205,333.33 \\
      276,000.00 \\
      333,333.33 \\
      404,000.00 \\
      461,333.33
    \end{bmatrix},\quad
    \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      -5333.33 \\ 4000.00 \\ 6666.67 \\ -4000.00 \\ -1333.33
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 6: Sum of Squared Errors (SSE)</h4>
  <p>
    $$
      \mathrm{SSE} = \sum_{i=1}^{5}(y_i - \hat{y}_i)^2 \;\approx\; 1.0667 \times 10^8.
    $$
  </p>

  <h4>Interpretation</h4>
  <ul>
    <li><strong>Intercept \(\beta_0\)</strong>: Baseline price when Size and Bedrooms are zero (a theoretical anchor).</li>
    <li><strong>\(\beta_1\)</strong>: Each additional sq.ft adds about \$114.67 to the price, holding Bedrooms fixed.</li>
    <li><strong>\(\beta_2\)</strong>: Each additional bedroom adds about \$13,333.33, holding Size fixed.</li>
  </ul>

    <h4>Why is SSE Large?</h4>
<p>A large SSE doesn't necessarily mean that the model is bad; it just means that the values you're predicting (house prices) are large. This is why you should scale the SSE to interpret it better, such as calculating the <b>Mean Squared Error (MSE)</b> or <b>Root Mean Squared Error (RMSE)</b>. These measures take the scale of the data into account:</p>

<p><b>Mean Squared Error (MSE):</b></p>
<p>
    $$ \text{MSE} = \frac{\text{SSE}}{m} = \frac{1}{m} \sum_{i=1}^{m}(y_i - \hat{y}_i)^2 $$
</p>
<p>Where <i>m</i> is the number of data points (or samples). MSE gives you the average squared error per data point. For this example, the MSE is approximately <b>21,333,333.33</b>.</p>

<p><b>Root Mean Squared Error (RMSE):</b></p>
<p>
    $$ \text{RMSE} = \sqrt{\text{MSE}} $$
</p>
<p>RMSE takes the square root of MSE, giving you the error in the same units as the original data (house prices, in this case), making it easier to interpret. For this example, the RMSE is approximately <b>4,618.80</b>.</p>

<p>These metrics give you an idea of how much error you’re getting on average per prediction (in terms of the original units of the data). A smaller MSE or RMSE indicates better performance of the model.</p>

<p>
   <b>Coefficient of Determination (\(𝑅^2\))</b>
</p>
<p>
  $$ 
  R^2 \;=\; 1 \;-\; \frac{\sum_{i=1}^{m}\bigl(y_i - \hat{y}_i\bigr)^2}
                         {\sum_{i=1}^{m}\bigl(y_i - \bar{y}\bigr)^2}
  $$
</p>


<h4>Range &amp; Meaning</h4>
<ul>
  <li><strong>\( R^2 \) = 1.0</strong> — Perfect fit: model explains 100% of the variance in \(y\).</li>
  <li><strong>\( R^2 \) = 0.0</strong> — Model explains 0% of the variance: as good as predicting the mean.</li>
  <li><strong>\( R^2 \) &lt; 0</strong> — Model is worse than predicting the mean: poor predictive performance.</li>
</ul>

<!-- MathJax loader (optional, only needed for the \(y\) math) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>

<br>
      
</section>

<section id="ridge-regression">
    
      <h4>2.2 Ridge Regression</h4>

    <b>Idea:</b>
<p>
   
    Ridge Regression extends Ordinary Least Squares (OLS) by adding an <b><font color="red">L2 penalty</font></b> on the size of the coefficients. 
    This penalty discourages overly large weights, helping to reduce <b>overfitting</b> and improve <b>generalization</b>, 
    especially when features are correlated or when the number of features \( p \) is close to or greater than the number of training samples \( m \).
    <br><br>
    In Ridge Regression, a penalty is an extra cost you add to the training objective for using large coefficient values. 
    It’s also called <b><font color="red">regularization</font></b>. By penalizing large coefficients, L2 regularization reduces reliance on single features and improves generalization.
    <br><br>

    <h4>When to use Regularization:</h4>
    <ul>
      <li>Preventing Overfitting</li>
      <li>High Dimensionality (\(p &gg; m\))</li>
      <li>Multicollinearity (features are highly correlated)</li>
    </ul>
    
  <br><br>
    Ridge Regression (L2-regularized linear regression) fits a hyperplane like OLS but adds a penalty on the size of the coefficients to reduce overfitting and handle multicollinearity.
    The goal is to <b><font color="red">minimize the squared error while shrinking coefficients</font></b>.
</p>

<b>Equation:</b>
<p style="text-align: center;">
  \(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}\), &nbsp; and we choose \(\boldsymbol{\beta}\) to minimize
  \[
    \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^{p} \beta_j^2
  \]
 
</p>

    <p style="text-align:center;">
       
    \[
      \;\;\Longrightarrow\;\;
      
      \underbrace{\sum_{i=1}^{m}\bigl(y_i - \hat{y}_i\bigr)^2}_{\text{SSE}}
      \;+\;
      \underbrace{\lambda \sum_{j=1}^{p} \beta_j^2}_{\text{L2 penalty}}
    \]
       
  </p>

   <p style="text-align:center;">
    \[

    \;\;\Longrightarrow\;\;
    
    \underbrace{\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2}_{\text{sum of squared residuals}}
      \;+\;
      \underbrace{\lambda \|\boldsymbol{\beta}_{1:p}\|_2^2}_{\text{sum of squared coefficients}}
     
  \]

     (note: the intercept \(\beta_0\) is typically <i>not</i> penalized).
  </p>

<p><strong>Where:</strong></p>
<ul>
  <li>\( \lambda \ge 0 \): &lambda; is the regularization strength parameter, 
    it controls the amount of regularization applied</li>
  <li>\(  \lambda = 0 &rArr; Ridge = OLS \) (no penalty) </li>
  <li> Small &lambda; &rArr; weak shrinkage &rArr; coefficients close to OLS values </li>
  <li> Large &lambda; &rArr; strong shrinkage &rArr; coefficients shrink toward zero, coefficients much smaller, model more regularized. </li>
  <li>\( \beta_1, \dots, \beta_p \) = coefficients for features</li>
  <li>\( \mathbf{X} \) = design matrix </li>
  <li>\( \mathbf{y} \) = targets</li>
  <li>\( p \) means the number of input feature </li>
</ul>

<br>


<h4 style="color:#ff79c6; margin-top:0;">What does <span style="white-space:nowrap;">\(\|\cdot\|_2\)</span> mean?</h4>

<p>
    The notation <strong>\(\|\cdot\|_2\)</strong> means the <b><font color="red">L2 norm</font></b> (also called the
    <b><font color="red">Euclidean norm</font></b> or <b>2-norm</b>).
  </p>

  <ul>
    <li>
      <strong>Double bars</strong> <span style="white-space:nowrap;">\(\|\cdot\|\)</span> denote a
      <em>norm</em> in general.
    </li>
    <li>
      The <strong>subscript 2</strong> indicates the <em>L2 norm</em>.
    </li>
  </ul>

  <p><strong>Definition:</strong></p>
  <p style="margin-left:1rem;">
    For a vector \(\mathbf{v} = (v_1,\dots,v_n)\), the L2 norm is
  </p>
  <p style="text-align:center;">
    \[
      \|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}
    \]
  </p>

  <p><strong>Squared L2 norm (sum of squares):</strong></p>
  <p style="text-align:center;">
    \[
      \|\mathbf{v}\|_2^2 = \sum_{i=1}^{n} v_i^2
    \]
  </p>

<br>

<!-- Ridge Shrinkage Illustration — smaller L2 circle -->
<svg viewBox="0 0 800 480" role="img" aria-labelledby="ridgeTitle ridgeDesc"
     style="max-width:100%;height:auto;background:#1e1f22;border-radius:12px;box-shadow:0 4px 24px rgba(255,121,198,.12)">
  <title id="ridgeTitle">Ridge Shrinkage Illustration</title>
  <desc id="ridgeDesc">
    Axes for beta1 and beta2, a circular L2 constraint ball centered at the origin, OLS-centered SSE ellipses,
    and the ridge solution at the tangency point between an ellipse and the L2 circle.
  </desc>

  <defs>
    <marker id="arrow-pink" viewBox="0 0 10 10" refX="8" refY="5"
            markerWidth="7" markerHeight="7" orient="auto-start-reverse">
      <path d="M0 0 L10 5 L0 10 z" fill="#ff79c6"></path>
    </marker>
    <style>
      .axis { stroke:#ff79c6; stroke-width:2; marker-end:url(#arrow-pink); }
      .axisLabel { fill:#ffb3de; font: 500 13px Roboto, Arial, sans-serif; }
      .note { fill:#ffb3de; font: 500 12px Roboto, Arial, sans-serif; }
      .label { fill:#ff79c6; font: 600 13px Roboto, Arial, sans-serif; }
      .ellipse { fill:none; stroke:#8be9fd; stroke-width:3; }
      .ellipse--wide { fill:none; stroke:#8be9fd; stroke-opacity:.45; stroke-width:2; stroke-dasharray:6 6; }
      .l2 { fill:none; stroke:#ff79c6; stroke-width:3; }
      .guide { stroke:#9aa0a6; stroke-width:1.5; stroke-dasharray:6 6; }
    </style>
  </defs>

  <!-- Axes -->
  <g id="coords">
    <line x1="60"  y1="240" x2="760" y2="240" class="axis" />
    <line x1="400" y1="440" x2="400" y2="40"  class="axis" />
    <text x="752" y="226" class="axisLabel">β₁</text>
    <text x="410" y="54"  class="axisLabel">β₂</text>
    <circle cx="400" cy="240" r="2.5" fill="#ff79c6"/>
    <text x="408" y="254" class="note">origin</text>
  </g>

  <!-- Smaller L2 ball (reduced radius to 70) -->
  <g id="l2-ball">
    <circle cx="400" cy="240" r="70" class="l2"/>
    <text x="210" y="356" class="label">L2 constraint: ‖β‖₂ ≤ t</text>
  </g>

  <!-- OLS (unregularized) -->
  <g id="ols">
    <text x="560" y="160" text-anchor="middle" dominant-baseline="middle"
          style="font:700 20px/1 Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="572" y="180" class="label">OLS (unregularized)</text>
  </g>

  <!-- Shrinkage direction (origin → OLS) -->
  <g id="shrink">
    <line x1="400" y1="240" x2="560" y2="160" class="guide"/>
    <text x="492" y="206" class="note">shrinkage direction</text>
  </g>

  <!-- SSE contours -->
  <g id="sse">
    <ellipse cx="560" cy="160" rx="110" ry="72" class="ellipse"
             transform="rotate(-25 560 160)"/>
    <ellipse cx="560" cy="160" rx="150" ry="98" class="ellipse--wide"
             transform="rotate(-25 560 160)"/>
    <ellipse cx="560" cy="160" rx="190" ry="124" class="ellipse--wide"
             transform="rotate(-25 560 160)"/>
    <text x="600" y="90" class="label">SSE contours</text>
  </g>

  <!-- Ridge point (moved inward to sit on circle radius 70) -->
  <g id="ridge">
    <circle cx="459.6" cy="210.2" r="5.5" fill="#50fa7b" stroke="#1e1f22" stroke-width="1.5"/>
    <text x="474" y="228" class="label" style="fill:#50fa7b;">Ridge solution (tangent)</text>
  </g>

  <!-- Legend -->
  <g id="legend">
    <rect x="72" y="64" width="215" height="88" rx="10" ry="10"
          fill="rgba(33,17,27,.75)" stroke="#402138"/>
    <circle cx="92" cy="88" r="7" fill="none" stroke="#ff79c6" stroke-width="3"/>
    <text x="110" y="92" class="note">L2 constraint ball</text>

    <line x1="85" y1="114" x2="99" y2="114" class="ellipse"/>
    <text x="110" y="118" class="note">SSE (loss) contour</text>

    <text x="88" y="140" style="font:700 16px Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="110" y="142" class="note">OLS estimate</text>
  </g>
</svg>

  

 
<br>

<h4>How Ridge Regression Works</h4>
<ol>
  <li><strong>Collect Data</strong> – Gather features \((x_1,\dots,x_p)\) and outputs \(y\).</li>
  <li><strong>(Recommended) Standardize Features</strong> – Put features on comparable scales so the penalty treats them fairly.</li>
  <li><strong>Choose \(\lambda\)</strong> – Typically via cross-validation.</li>
  <li><strong>Fit the Model</strong> – Minimize squared error + L2 penalty to get \(\hat{\boldsymbol{\beta}}\).</li>
  <li><strong>Evaluate</strong> – Check train/validation error; tune \(\lambda\) to balance bias–variance.</li>
</ol>

<h4>Model & Notation</h4>
<p><b>Scalar form:</b></p>
<p>
  $$
   y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \epsilon 
  $$
</p>

<p><b>Matrix form:</b></p>
<p>
  $$
   \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
  $$
   where

  $$
   \mathbf{X} =
  \begin{bmatrix}
    1 & x_{11} & \cdots & x_{1p} \\
    1 & x_{21} & \cdots & x_{2p} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{m1} & \cdots & x_{mp}
  \end{bmatrix},\;
  \boldsymbol{\beta} =
  \begin{bmatrix}
    \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
  \end{bmatrix},\;
  \mathbf{y} =
  \begin{bmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_m
  \end{bmatrix}.
  $$
</p>

<h4>Objective (Ridge / L2)</h4>
<p>
  Ridge minimizes
  \[
    \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^{p}\beta_j^2
    \;=\;
    \|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2 \;+\; \lambda\,\|\boldsymbol{\beta}_{1:p}\|_2^2
  \]
  
In vector form with a penalty matrix
  \( \mathbf{P} = \mathrm{diag}(0,1,\dots,1) \) (no penalty on \( \beta_0 \))
  , 
  \( \mathbf{P} \in \mathbb{R}^{(p+1)\times(p+1)}   \text{is a diagonal matrix whose diagonal entries are } [0,\,1,\,1,\,\dots,\,1] \)
  
  
  
  \[
    \min_{\boldsymbol{\beta}} \; (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
    \;+\; \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
  \]
</p>


  <p>
  \( \beta^\top P \beta \) is a quadratic form.
  Since \( P \) is diagonal, multiplying out gives:
</p>

<p>
  \[
    \beta^\top P \beta
    = 0 \cdot \beta_0^2
    + 1 \cdot \beta_1^2
    + 1 \cdot \beta_2^2
    + \dots
    + 1 \cdot \beta_p^2
  \]
</p>

<p>
  That’s exactly:
</p>

<p>
  \[
    \sum_{j=1}^p \beta_j^2
  \]
</p>
  

<h4>Normal Equation (Ridge)</h4>
 <div class="step">
    <p><b>Expand the objective:</b></p>
    <p>
      $$
      J(\boldsymbol{\beta})
      = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">

    <p><b>Distribute (FOIL) the SSE part and use transpose rules:</b></p>

<p>
  $$
  (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
  = \underbrace{\mathbf{y}^\top\mathbf{y}}_{\text{First}}
    - \underbrace{\mathbf{y}^\top\mathbf{X}\boldsymbol{\beta}}_{\text{Outer}}
    - \underbrace{\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}}_{\text{Inner}}
    + \underbrace{\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}}_{\text{Last}}
  $$
</p>

<p>
  Using the fact that the Outer and Inner terms are equal scalars, we combine them:
</p>

<p>
  $$
  = \mathbf{y}^\top\mathbf{y}
    - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
    + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}.
  $$
</p>
    
    <p>So
      $$
      J(\boldsymbol{\beta}) =
      \mathbf{y}^\top\mathbf{y}
      - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
      + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">
    <p><b>Take gradient w.r.t. \( \boldsymbol{\beta} \) and set to zero:</b></p>
    <p>
      $$
      \frac{\partial J}{\partial \boldsymbol{\beta}}
      = -2\,\mathbf{X}^\top\mathbf{y}
        + 2\,\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
        + 2\lambda\,\mathbf{P}\,\boldsymbol{\beta}
      \;=\; \mathbf{0}.
      $$
    </p>
    <p>
      Rearranging:
      $$
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})\,\hat{\boldsymbol{\beta}}
      = \mathbf{X}^\top\mathbf{y}.
      $$
    </p>
    <p><b>Closed form (if invertible):</b>
      $$
      \boxed{\;
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      =
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
      \;}
      $$
    </p>
  </div>

<br>

<hr>

<h4>Worked Example: Predicting House Price (Ridge)</h4>
<p>Use the same data as in 2.1 and let \( \lambda = 100 \). We do <b>not</b> penalize the intercept, so
\(\mathbf{P}=\mathrm{diag}(0,1,1)\).</p>

<h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
<p>
  $$
  \mathbf{X} =
  \begin{bmatrix}
    1 & 1000 & 2 \\
    1 & 1500 & 3 \\
    1 & 2000 & 3 \\
    1 & 2500 & 4 \\
    1 & 3000 & 4
  \end{bmatrix},\quad
  \mathbf{y} =
  \begin{bmatrix}
    200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
  \end{bmatrix}.
  $$
</p>

<h4>Step 2: Compute \( \mathbf{X}^\top \mathbf{X} + \lambda \mathbf{P} \) and \( \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  From the ridge regression section, 
  $$ 
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 & 34500 \\
  16 & 34500 & 54
  \end{bmatrix}, 
  
  \quad
  
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1680000 \\ 3680000000 \\ 5700000
  \end{bmatrix}.
  $$
</p>
<p>
  With \( \lambda = 100 \) and 
  
  \( \mathbf{P}=
   \begin{bmatrix}
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
  \):
</p>
<p>
  \[
  \mathbf{X}^\top \mathbf{X} + \lambda \mathbf{P} \;=\;
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 + 100 & 34500 \\
  16 & 34500 & 54 + 100
  \end{bmatrix}
  =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500010 & 34500 \\
  16 & 34500 & 154
  \end{bmatrix}.
  \]
</p>

<h4>Step 3: Solve for \( \hat{\boldsymbol{\beta}}_{\text{ridge}} \)</h4>
<p>
  \[
    \hat{\boldsymbol{\beta}}_{\text{ridge}}
    = (\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
    \;\approx\;
    \begin{bmatrix}
      79,962.23 \\
      127.95 \\
      40.01
    \end{bmatrix}.
  \]
</p>

<h4>Final Ridge Model (\(\lambda=100\))</h4>
<p style="text-align:center; font-size:1.1em;">
  \( \widehat{\text{Price}} = 79{,}962.23 \;+\; 127.95 \times \text{Size} \;+\; 40.01 \times \text{Bedrooms} \)
</p>

<h4>Step 4: Predictions \((\hat{\mathbf{y}})\) & Errors</h4>
<p>
  $$
  \hat{\mathbf{y}} \approx
  \begin{bmatrix}
    207,997.12 \\
    272,014.56 \\
    335,992.00 \\
    400,009.44 \\
    463,986.88
  \end{bmatrix},
  
  \quad
  
  \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
  \begin{bmatrix}
    -7,997.12 \\
    7,985.44 \\
    4,008.00 \\
    -9.44 \\
    -3,986.88
  \end{bmatrix}.
  $$
</p>

<h4>Step 5: Training Error (for reference)</h4>
<p>
  $$
    \mathrm{SSE} \approx 1.5968 \times 10^8,\quad
    \mathrm{MSE} \approx 3.1936 \times 10^7,\quad
    \mathrm{RMSE} \approx 5{,}651.20.
  $$
</p>

<h4>Key Points</h4>
<ul>
  <li>Setting \(\lambda=0\) recovers OLS; larger \(\lambda\) shrinks coefficients (reduces variance, increases bias).</li>
  <li>Ridge helps when features are highly correlated (\(\mathbf{X}^\top\mathbf{X}\) is ill-conditioned).</li>
  <li>Do not penalize the intercept; standardize features before ridge to make penalty comparable across features.</li>
  <li>Choose \(\lambda\) via cross-validation for best generalization.</li>
</ul>

  </section>

  <section id="lasso-regression">
    
      <h4>2.3 Lasso Regression</h4>

<b>Idea:</b>

    <p>
    Lasso Regression extends Ordinary Least Squares (OLS) by adding an 
    <b><font color="red">L1 penalty</font></b> on the absolute values of the coefficients. 
    This penalty encourages sparsity in the model — that is, it tends to drive some coefficients 
    exactly to zero, effectively performing <b>feature selection</b> while fitting the model.
    <br><br>
    In Lasso Regression, the penalty is the sum of the absolute values of the coefficients 
    (excluding the intercept). By penalizing the magnitude of coefficients, L1 regularization 
    both reduces overfitting and simplifies the model by eliminating less important features.
  </p>

  <h4>When to use Regularization:</h4>
  <ul>
    <li>Preventing Overfitting</li>
    <li>High Dimensionality (\(p \gg m\))</li>
    <li>When you suspect many features are irrelevant or redundant</li>
    <li>For automatic feature selection (Lasso can set coefficients to zero)</li>
  </ul>

 <p>
  Lasso Regression (L1-regularized linear regression) fits a hyperplane like OLS but adds a penalty on the
  <em>absolute size</em> of coefficients to prevent overfitting and to perform <b>feature selection</b>.
  The goal is to <b><font color="red">minimize the squared error while pushing unimportant coefficients exactly to zero</font></b>
  (sparse model).
</p>

  <b>Equation:</b>
  <p style="text-align:center;">
    \(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}\), &nbsp; and we choose \(\boldsymbol{\beta}\) to minimize:
    \[
      \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^{p} |\beta_j|
    \]
  </p>

  <p style="text-align:center;">
    \[
      \;\;\Longrightarrow\;\;
      \underbrace{\sum_{i=1}^{m} \bigl(y_i - \hat{y}_i\bigr)^2}_{\text{SSE}}
      \;+\;
      \underbrace{\lambda \sum_{j=1}^{p} |\beta_j|}_{\text{L1 penalty}}
    \]
  </p>

  <p style="text-align:center;">
    \[
      \;\;\Longrightarrow\;\;
      \underbrace{\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2}_{\text{sum of squared residuals}}
      \;+\;
      \underbrace{\lambda \|\boldsymbol{\beta}_{1:p}\|_1}_{\text{sum of absolute coefficients}}
    \]
    (note: the intercept \(\beta_0\) is typically <i>not</i> penalized).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(\lambda \ge 0\): controls the amount of regularization</li>
    <li>\(\lambda = 0 \Rightarrow\) Lasso = OLS (no penalty)</li>
    <li>Small \(\lambda\) → weak shrinkage → coefficients close to OLS values</li>
    <li>Large \(\lambda\) → strong shrinkage → more coefficients set exactly to zero</li>
    <li>\(\beta_1, \dots, \beta_p\) = coefficients for features</li>
    <li>\(\mathbf{X}\) = design matrix</li>
    <li>\(\mathbf{y}\) = targets</li>
  </ul>

  <br>

  <h4 style="color:#ff79c6; margin-top:0;">What does <span style="white-space:nowrap;">\(\|\cdot\|_1\)</span> mean?</h4>
  <p>
    The notation <strong>\(\|\cdot\|_1\)</strong> means the <b><font color="red">L1 norm</font></b> 
    (also called the <b>Manhattan norm</b> or <b>taxicab norm</b>).
  </p>

  <ul>
    <li><strong>Double bars</strong> \(\|\cdot\|\) denote a norm in general.</li>
    <li>The subscript 1 indicates the L1 norm.</li>
  </ul>

  <p><strong>Definition:</strong></p>
  <p style="margin-left:1rem;">
    For a vector \(\mathbf{v} = (v_1, \dots, v_n)\), the L1 norm is
  </p>
  <p style="text-align:center;">
    \[
      \|\mathbf{v}\|_1 = \sum_{i=1}^n |v_i|
    \]
  </p>

  <br>

 <!-- Lasso Constraint Illustration (ellipse above, rotated, adjusted up for tangent) -->
<svg viewBox="0 0 800 480" style="max-width:100%;height:auto;background:#1e1f22;border-radius:12px;box-shadow:0 4px 24px rgba(255,121,198,.12)">
  <title>Lasso Shrinkage Illustration</title>
  <desc>Diamond-shaped L1 constraint, rotated SSE ellipse above the diamond, tangent at the top corner point.</desc>
  <defs>
    <marker id="arrow-pink" viewBox="0 0 10 10" refX="8" refY="5"
            markerWidth="7" markerHeight="7" orient="auto-start-reverse">
      <path d="M0 0 L10 5 L0 10 z" fill="#ff79c6"></path>
    </marker>
    <style>
      .axis { stroke:#ff79c6; stroke-width:2; marker-end:url(#arrow-pink); }
      .axisLabel { fill:#ffb3de; font: 500 13px Roboto, Arial, sans-serif; }
      .ellipse { fill:none; stroke:#8be9fd; stroke-width:3; }
      .l1 { fill:none; stroke:#ff79c6; stroke-width:3; }
      .note { fill:#ffb3de; font: 500 12px Roboto, Arial, sans-serif; }
      .label { fill:#ff79c6; font: 600 13px Roboto, Arial, sans-serif; }
    </style>
  </defs>

  <!-- Axes -->
  <line x1="60" y1="240" x2="760" y2="240" class="axis"/>
  <line x1="400" y1="440" x2="400" y2="40" class="axis"/>
  <text x="752" y="226" class="axisLabel">β₁</text>
  <text x="410" y="54" class="axisLabel">β₂</text>

  <!-- L1 constraint diamond -->
  <polygon points="400,170 470,240 400,310 330,240" class="l1"/>
  <text x="280" y="360" class="label">L1 constraint: ‖β‖₁ ≤ t</text>

  <!-- SSE ellipses (moved up for tangency, rotated) -->
  <ellipse cx="400" cy="110" rx="90" ry="60" class="ellipse" transform="rotate(20 400 110)"/>
  <ellipse cx="400" cy="110" rx="150" ry="100" class="ellipse" style="stroke-opacity:.45; stroke-dasharray:6 6;" transform="rotate(20 400 110)"/>

  <!-- Tangency / Lasso solution point (top corner) -->
  <circle cx="400" cy="170" r="5.5" fill="#50fa7b"/>
  <text x="410" y="150" class="label" style="fill:#50fa7b;">Tangent point (corner → β₁ = 0)</text>
</svg>


<br>

<h4>How Lasso Regression Works</h4>

<!-- Keep MathJax loaded once on the page -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<ol>
  <li><strong>Collect Data</strong> – Gather features \((x_1, x_2, \ldots, x_p)\) and outputs \(y\).</li>
  <li><strong>Standardize Features</strong> – Put features on comparable scales so the penalty treats them fairly; don’t penalize the intercept.</li>
  <li><strong>Choose \(\lambda\)</strong> – Typically via cross-validation (try a grid; pick the best validation score).</li>
  <li><strong>Fit the Model</strong> – Solve the L1-regularized optimization; common solvers include <em>coordinate descent</em> and <em>LARS</em>.</li>
  <li><strong>Interpret</strong> – Some coefficients become exactly 0 ⇒ automatic feature selection, simpler model.</li>
</ol>

<h4>Interpreting the Coefficients</h4>
<ul>
  <li><strong>Intercept (\(\beta_0\))</strong> – Baseline prediction when all features are zero (unpenalized).</li>
  <li><strong>Coefficient (\(\beta_j\))</strong> – Effect of feature \(x_j\) on \(y\) when others are held fixed.</li>
  <li><strong>Sparsity</strong> – If \(\beta_j = 0\), feature \(x_j\) is effectively removed by the model.</li>
</ul>

<br>

<h4>Model & Notation</h4>
<p><b>Scalar form:</b></p>
<p>
  $$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon $$
</p>

<p><b>Matrix form:</b></p>
<p>
  $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
  where
  $$ \mathbf{X} =
    \begin{bmatrix}
      1 & x_{11} & \cdots & x_{1p} \\
      1 & x_{21} & \cdots & x_{2p} \\
      \vdots & \vdots & \ddots & \vdots \\
      1 & x_{m1} & \cdots & x_{mp}
    \end{bmatrix},\quad
    \boldsymbol{\beta} =
    \begin{bmatrix}
      \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      y_1 \\ y_2 \\ \vdots \\ y_m
    \end{bmatrix}.
  $$
</p>

<h4>Objective (L1-Regularized Least Squares)</h4>

    <p>
    Lasso minimizes:
    \[
      \mathrm{SSE}(\boldsymbol{\beta}) + \lambda \sum_{j=1}^p |\beta_j|
      \;=\;
      \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2
      \;+\; \lambda \|\boldsymbol{\beta}_{1:p}\|_1
    \]
    This penalty tends to produce exact zeros in \(\beta_j\) for large enough \(\lambda\).
  </p>
    

<h4>Optimization & Soft-Thresholding (No Closed-Form)</h4>
<p>
  A common solver is <em>coordinate descent</em>. With standardized features (mean 0, variance 1) and centered targets (so the intercept is separate), the update for each coefficient has a closed-form soft-threshold:
</p>
<p style="text-align:center;">
  $$ \beta_j \leftarrow \frac{1}{\|\mathbf{x}_j\|_2^2}\; S\!\left(\mathbf{x}_j^\top \bigl(\mathbf{y} - \sum_{k\ne j} \mathbf{x}_k \beta_k \bigr),\; \tfrac{\lambda}{2}\right), $$
  where \( S(t,\gamma) = \mathrm{sign}(t)\,\max(|t|-\gamma,\,0) \) is the <b>soft-threshold</b> operator.
</p>
<p>
  Intuition: if the correlation of feature \(j\) with the current residual is small, soft-thresholding sets \(\beta_j\) to <b>exactly zero</b>; otherwise it shrinks it toward zero.
</p>

<br>

<hr>

<h4>Worked Example: Predicting House Price (Lasso)</h4>
<p>Use the same data as in Section 2.1:</p>

<h4>Data</h4>
<center>
<table border="1" cellpadding="6">
  <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
  <tr><td>1000</td><td>2</td><td>200,000</td></tr>
  <tr><td>1500</td><td>3</td><td>280,000</td></tr>
  <tr><td>2000</td><td>3</td><td>340,000</td></tr>
  <tr><td>2500</td><td>4</td><td>400,000</td></tr>
  <tr><td>3000</td><td>4</td><td>460,000</td></tr>
</table>
</center>
    
<h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
<p>
  $$ 
  \mathbf{X} =
  \begin{bmatrix}
    1 & 1000 & 2 \\
    1 & 1500 & 3 \\
    1 & 2000 & 3 \\
    1 & 2500 & 4 \\
    1 & 3000 & 4
  \end{bmatrix},\quad
  \mathbf{y} =
  \begin{bmatrix}
    200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
  \end{bmatrix}.
  $$
</p>

<h4>Step 2: Preprocess (Recommended for Lasso)</h4>
<ul>
  <li>Standardize the non-intercept columns (Size, Bedrooms) to mean 0 and variance 1.</li>
  <li>Center \( \mathbf{y} \) (so the intercept can be estimated as the mean of \(y\)).</li>
  <li>Do <em>not</em> penalize the intercept.</li>
</ul>

<h4>Step 3: Fit via Coordinate Descent (Conceptual)</h4>
<ol>
  <li>Initialize all \(\beta_j=0\) (or from OLS).</li>
  <li>For \(j=1\) to \(n\): compute the partial residual, apply soft-threshold \(S(\cdot,\lambda/2)\), update \(\beta_j\).</li>
  <li>Repeat passes over features until coefficients stabilize.</li>
</ol>
<p>
  <b>What you’ll see:</b> with small \(\lambda\), coefficients are close to OLS; as \(\lambda\) increases, both shrink; if Bedrooms is less informative than Size,
  Lasso will often set \(\beta_{\text{Bedrooms}}\) to <b>exactly 0</b> first (sparser model).
</p>

<h4>Final Model (Example Form)</h4>
<p style="text-align:center; font-size:1.1em;">
  $$ \widehat{\text{Price}} \;=\; \hat{\beta}_0 \;+\; \hat{\beta}_{\text{Size}}\times \text{Size} \;+\; \hat{\beta}_{\text{Bedrooms}}\times \text{Bedrooms}, $$
  where for a sufficiently large \(\lambda\), you may get
  $$ \hat{\beta}_{\text{Bedrooms}} = 0 \quad\Rightarrow\quad
     \widehat{\text{Price}} \;=\; \hat{\beta}_0 \;+\; \hat{\beta}_{\text{Size}}\times \text{Size}. $$
</p>

<h4>Predictions \((\hat{\mathbf{y}})\) & Errors</h4>
<p>
  After fitting, compute \( \hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}} \), residuals \( \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \),
  and the usual metrics:
</p>
<p>
  $$ \mathrm{SSE} = \sum_{i=1}^{m}(y_i - \hat{y}_i)^2,\quad
     \mathrm{MSE} = \frac{\mathrm{SSE}}{m},\quad
     \mathrm{RMSE} = \sqrt{\mathrm{MSE}}. $$
</p>
<p>
  <em>Note:</em> Training error may increase slightly vs OLS (added penalty), but test error often improves thanks to reduced variance and feature selection.
</p>

<h4>Key Points</h4>
<ul>
  <li><b>Sparsity:</b> Lasso can set coefficients exactly to zero ⇒ built-in feature selection.</li>
  <li><b>No closed-form solution:</b> solved by iterative methods (coordinate descent, LARS); the L1 penalty is nondifferentiable at 0.</li>
  <li><b>Standardize features:</b> ensures fair penalization across features with different units.</li>
  <li><b>Choose \(\lambda\) by cross-validation:</b> small \(\lambda\) ≈ OLS; large \(\lambda\) ⇒ simpler, sparser models.</li>
  <li><b>Contrast with Ridge:</b> Ridge (L2) <em>shrinks</em> but rarely zeros; Lasso (L1) <em>shrinks and selects</em>.</li>
</ul>
      
  </div>

  </section>


  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
