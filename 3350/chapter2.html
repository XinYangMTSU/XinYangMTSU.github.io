<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 2: Supervised Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    <li>
      <a href="#supervised"><i class="fas fa-chalkboard-teacher"></i>2. Types of Supervised Learning</a>
    </li>
    
    <li>
      <a href="#regression"><i class="fas fa-gamepad"></i>3. Regression</a>
      <div class="subsections">
        <a href="#value-based">3.1 Linear Regression</a>
        <a href="#policy-based">3.2 Ridge Regression</a>
        <a href="#model-based">3.3 Lasso Regression</a>
      </div>
    </li>

      <li>
      <a href="#classification"><i class="fas fa-gamepad"></i>4. Regression</a>
      <div class="subsections">
        <a href="#value-based">4.1 Linear Regression</a>
        <a href="#policy-based">4.2 Ridge Regression</a>
        <a href="#model-based">4.3 Lasso Regression</a>
      </div>
    </li>
    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 2: Supervised Learning</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>
    
  </section>
    

  <section id="supervised">

    <h2>2. Types of Supervised Learning</h2>
    
    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li>1. Regression</li>
       <li>2. Classification</li>
    </ul>
         
  </section>

  
  <section id="supervised-reg">
    
  <div id="regression">
    
      <h2>3. Regression</h2>
      <p>Regression algorithms <b><font color="red">predict numeric values</font></b> (e.g., house prices, temperature).
      Common methods include:
      </p>
      <ul>
        <li><strong>Linear Regression:</strong> Predicts a continuous outcome based on a linear relationship between input features and the target variable.</li>
        <li><strong>Ridge Regression (L2 Regularization): Linear regression with a penalty for large coefficients to prevent overfitting.</strong></li>
        <li><strong>Lasso Regression (L1 Regularization): Similar to ridge, but can shrink some coefficients to zero, performing feature selection.</strong></li>
      </ul>

      <br>
    
      <h4>3.1 Linear Regression</h4>
      <br>
      <b>Idea:</b>
      <p>
        Fits a straight line (in two dimensions) or a hyperplane (in higher dimensions) that best describes the relationship between 
        input features and the target variable. 
        The goal is to <b><font color="red">minimize the difference between the predicted values and the actual values</font></b>.
      </p>
    
      <b>Equation:</b>

      <p style="text-align: center;">
          y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + &hellip; + β<sub>n</sub>x<sub>n</sub> + ε
      </p>

    <p><strong>Where:</strong></p>
    <ul>
      <li><em>y</em> = predicted output</li>
      <li>β<sub>0</sub> = intercept (value of <em>y</em> when all <em>x</em> are zero)</li>
      <li>β<sub>1</sub>, β<sub>2</sub>, …, β<sub>n</sub> = coefficients (weights for each feature)</li>
      <li><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub>n</sub> = input features</li>
      <li>ε = error term (difference between predicted and actual values)</li>
    </ul>

  <br>
    
  <center>
    <img src="images/regression1.png" width="800px" height="300px">
  </center>
    
    <br>
    
    <h4>How Linear Regression Works</h4>
    
    <!-- Put this once in your page (head or before </body>) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
<ol>
  <li><strong>Collect Data</strong> – Gather pairs of input features (\(x_1, x_2, \ldots, x_n\)) and the known output (\(y\)).</li>
  <li><strong>Fit the Model</strong> – Find the values of \(\beta_0, \beta_1, \ldots, \beta_n\) that make the predicted \(\hat{y}\) values as close as possible to the actual \(y\) values in the training data.</li>
  <li><strong>Minimize Error</strong> – Ordinary Least Squares (OLS) minimizes the sum of squared errors:</li>
</ol>

<p>$$ \mathrm{SSE} = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 $$</p>
<p>where \(y_i\) is the actual value and \(\hat{y}_i\) is the predicted value.</p>


    <h4>Interpreting the Coefficients</h4>
<ul>
  <li><strong>Intercept (β<sub>0</sub>)</strong> – The baseline prediction when all features are zero.</li>
  <li><strong>Coefficient (β<sub>j</sub>)</strong> – How much <em>y</em> changes when <em>x</em><sub>j</sub> increases by one unit, keeping all other features constant.</li>
  <li><strong>Positive β<sub>j</sub></strong> → <em>y</em> increases.</li>
  <li><strong>Negative β<sub>j</sub></strong> → <em>y</em> decreases.</li>
</ul>

    <br>
    
  <h4>Model & Notation</h4>
  <p>Scalar form:</p>
  <p>$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon $$</p>

  <p>Matrix form:</p>
  <p>
    $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
    where
    $$ \mathbf{X} = 
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1n} \\
        1 & x_{21} & \cdots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mn}
      \end{bmatrix},\quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n
      \end{bmatrix},\quad
      \mathbf{y} =
      \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_m
      \end{bmatrix}.
    $$
  </p>


<h4>Objective (Least Squares)</h4>
<p>The goal of Least Squares regression is to minimize the sum of squared errors (SSE), which represents the difference between the observed values <i>y<sub>i</sub></i> and the predicted values <i>\( \hat{y}_i \)</i>.</p>

<p><b>Formula:</b></p>
<p>
    $$ \mathrm{SSE} = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
       \;=\; (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}). $$
</p>

<p>Where:</p>
<ul>
    <li><i>\( y_i \)</i> is the actual observed value.</li>
    <li>  \( \hat{y}_i \) is the predicted value (the model's output).</li>
    <li><i>\( m \) </i> is the number of data points.</li>
</ul>

<p>You can rewrite this using vector notation:</p>
<p>
    $$ \text{SSE} = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) $$
</p>
<p>Where:</p>
<ul>
    <li><i>\( \mathbf{y} \)</i> is the vector of observed values.</li>
    <li><i>\( \mathbf{X} \)</i> is the design matrix (input features for the data).</li>
    <li><i>\( \mathbf{&#946;} \)</i> is the vector of coefficients (parameters of the model).</li>
</ul>
<p>The expression <i>(y - X&#946;)</i> is the vector of errors (the differences between observed and predicted values).</p>

<h4>Normal Equations (Derivation Result)</h4>
<p>Once we have the objective function (SSE), we need to find the values of <i>&#946;</i> that minimize this function. To do so, we take the derivative of the SSE with respect to <i>&#946;</i> and set it to zero to find the optimal coefficients.</p>

<p><b>Derivation:</b></p>
<p>To minimize the SSE, we compute the derivative of the SSE with respect to <i>&#946;</i>, which gives us:</p>
<p>
    $$ \frac{\partial \text{SSE}}{\partial \boldsymbol{\beta}} = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) $$
</p>
<p>We then set the derivative equal to zero:</p>
<p>
    $$ -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) = 0 $$
</p>
<p>Simplifying this:</p>
<p>
    $$ \mathbf{X}^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) = 0 $$
</p>
<p>Expanding this:</p>
<p>
    $$ \mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} = 0 $$
</p>
<p>Solving for <i>&#946;</i>, we get the Normal Equation:</p>
<p>
    $$ \mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} = \mathbf{X}^\top \mathbf{y} $$
</p>
<p>The solution to this equation gives the optimal values for the coefficients <i>&#946;</i>:</p>
<p>
    $$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
</p>
<p>Where:</p>
<ul>
    <li><i>&#946;<sup>&#770;</sup></i> is the vector of estimated coefficients (the model parameters).</li>
    <li><i>X<sup>&#8473;</sup> X</i> is the matrix of the transposed design matrix multiplied by the design matrix.</li>
    <li><i>X<sup>&#8473;</sup> y</i> is the matrix multiplication of the transposed design matrix with the observed values vector.</li>
</ul>

<h4>Key Points:</h4>
<ul>
    <li>The Normal Equation provides a closed-form solution for the coefficients <i>&#946;</i> that minimizes the sum of squared errors.</li>
    <li>Matrix inversion is required to solve for <i>&#946;</i>, which is computationally expensive for large datasets. For very large datasets, more efficient methods like gradient descent are often used.</li>
</ul>

    

  <hr>

  <h4>Worked Example: Predicting House Price</h4>
  <p>Model: 
    $$ \text{Price} = \beta_0 + \beta_1 \cdot \text{Size} + \beta_2 \cdot \text{Bedrooms} + \epsilon $$
  </p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
    <tr><td>1000</td><td>2</td><td>200,000</td></tr>
    <tr><td>1500</td><td>3</td><td>280,000</td></tr>
    <tr><td>2000</td><td>3</td><td>340,000</td></tr>
    <tr><td>2500</td><td>4</td><td>400,000</td></tr>
    <tr><td>3000</td><td>4</td><td>460,000</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1 & 1000 & 2 \\
      1 & 1500 & 3 \\
      1 & 2000 & 3 \\
      1 & 2500 & 4 \\
      1 & 3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Compute \( \mathbf{X}^\top \mathbf{X} \) and \( \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  In this step, we calculate two important matrices that will help us solve for the coefficients \( \boldsymbol{\beta} \):
</p>
<ul>
  <li><b>\( \mathbf{X}^\top \mathbf{X} \)</b>: The matrix product of the transpose of the design matrix \( \mathbf{X} \) and \( \mathbf{X} \) itself.</li>
  <li><b>\( \mathbf{X}^\top \mathbf{y} \)</b>: The matrix product of the transpose of \( \mathbf{X} \) and the observed values \( \mathbf{y} \).</li>
</ul>

<h4>Transpose of \( \mathbf{X} \)</h4>
<p>
  The transpose of the design matrix \( \mathbf{X} \), denoted \( \mathbf{X}^\top \), is obtained by flipping the rows and columns of \( \mathbf{X} \):
</p>
<p>
  $$ 
  \mathbf{X}^\top =
  \begin{bmatrix}
  1 & 1 & 1 & 1 & 1 \\
  1000 & 1500 & 2000 & 2500 & 3000 \\
  2 & 3 & 3 & 4 & 4
  \end{bmatrix}
  $$
</p>

<h4>Matrix Multiplication: \( \mathbf{X}^\top \mathbf{X} \)</h4>
<p>
  Now, multiply \( \mathbf{X}^\top \) by \( \mathbf{X} \) to get the matrix product \( \mathbf{X}^\top \mathbf{X} \):
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  1 & 1 & 1 & 1 & 1 \\
  1000 & 1500 & 2000 & 2500 & 3000 \\
  2 & 3 & 3 & 4 & 4
  \end{bmatrix}
  \begin{bmatrix}
  1 & 1000 & 2 \\
  1 & 1500 & 3 \\
  1 & 2000 & 3 \\
  1 & 2500 & 4 \\
  1 & 3000 & 4
  \end{bmatrix}
  $$
</p>
<p>
  The result of multiplying these matrices gives us the following:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 & 34500 \\
  16 & 34500 & 54
  \end{bmatrix}
  $$
</p>

<h4>Matrix Multiplication: \( \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  Next, multiply \( \mathbf{X}^\top \) by \( \mathbf{y} \) to get the matrix product \( \mathbf{X}^\top \mathbf{y} \):
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1 & 1 & 1 & 1 & 1 \\
  1000 & 1500 & 2000 & 2500 & 3000 \\
  2 & 3 & 3 & 4 & 4
  \end{bmatrix}
  \begin{bmatrix}
  200000 \\
  280000 \\
  340000 \\
  400000 \\
  460000
  \end{bmatrix}
  $$
</p>
<p>
  The result of this multiplication gives the following vector:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1680000 \\
  3680000000 \\
  5700000
  \end{bmatrix}
  $$
</p>

<h4>Summary of Step 2 Results:</h4>
<ul>
  <li><b>\( \mathbf{X}^\top \mathbf{X} \)</b>: 
    $$ 
    \begin{bmatrix}
    5 & 10000 & 16 \\
    10000 & 22500000 & 34500 \\
    16 & 34500 & 54
    \end{bmatrix}
    $$
  </li>
  <li><b>\( \mathbf{X}^\top \mathbf{y} \)</b>: 
    $$ 
    \begin{bmatrix}
    1680000 \\
    3680000000 \\
    5700000
    \end{bmatrix}
    $$
  </li>
</ul>

<p>These matrices will help us solve for the optimal coefficients in the next step using the Normal Equation:</p>
<p>
  $$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
</p>


  <h4>Step 3: Solve for \( \hat{\boldsymbol{\beta}} \)</h4>
<p>
  In this step, we solve for the optimal coefficients \( \hat{\boldsymbol{\beta}} \) using the **Normal Equation**:
</p>
<p>
  $$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
</p>

<h4>Computing \( (\mathbf{X}^\top \mathbf{X})^{-1} \)</h4>
<p>
  The first part of the equation is computing the inverse of \( \mathbf{X}^\top \mathbf{X} \). We already have \( \mathbf{X}^\top \mathbf{X} \) from Step 2:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{X} =
  \begin{bmatrix}
  5 & 10000 & 16 \\
  10000 & 22500000 & 34500 \\
  16 & 34500 & 54
  \end{bmatrix}
  $$
</p>
<p>
  Using matrix algebra (or a calculator for matrix inversion), we compute the inverse of \( \mathbf{X}^\top \mathbf{X} \):
</p>
<p>
  $$
  (\mathbf{X}^\top \mathbf{X})^{-1} =
  \begin{bmatrix}
  0.000111 \, & -4.44 \times 10^{-6} \, & -0.00018 \\
  -4.44 \times 10^{-6} \, & 4.44 \times 10^{-11} \, & 6.44 \times 10^{-7} \\
  -0.00018 \, & 6.44 \times 10^{-7} \, & 0.000057
  \end{bmatrix}
  $$
</p>

<h4>Matrix Multiplication: \( (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \)</h4>
<p>
  Now, multiply the inverse of \( \mathbf{X}^\top \mathbf{X} \) by \( \mathbf{X}^\top \mathbf{y} \). We have \( \mathbf{X}^\top \mathbf{y} \) from Step 2:
</p>
<p>
  $$
  \mathbf{X}^\top \mathbf{y} =
  \begin{bmatrix}
  1680000 \\
  3680000000 \\
  5700000
  \end{bmatrix}
  $$
</p>
<p>
  Multiply the two matrices to obtain the vector of coefficients \( \hat{\boldsymbol{\beta}} \):
</p>
<p>
  $$
  \hat{\boldsymbol{\beta}} =
  \begin{bmatrix}
  0.000111 \, & -4.44 \times 10^{-6} \, & -0.00018 \\
  -4.44 \times 10^{-6} \, & 4.44 \times 10^{-11} \, & 6.44 \times 10^{-7} \\
  -0.00018 \, & 6.44 \times 10^{-7} \, & 0.000057
  \end{bmatrix}
  \begin{bmatrix}
  1680000 \\
  3680000000 \\
  5700000
  \end{bmatrix}
  $$
</p>
<p>
  The result of this multiplication gives us the estimated coefficients:
</p>
<p>
  $$ \hat{\beta}_0 = 64{,}000,\quad \hat{\beta}_1 \approx 114.67,\quad \hat{\beta}_2 \approx 13{,}333.33 $$
</p>

<h4>Final Model</h4>
<p style="text-align:center; font-size:1.1em;">
  $$ \widehat{\text{Price}} = 64{,}000 + 114.67 \cdot \text{Size} + 13{,}333.33 \cdot \text{Bedrooms} $$
</p>

<p>
  Now we have the final model for predicting the house price. This model can be used to predict the price of houses based on their size (in square feet) and the number of bedrooms.
</p>

  <h4>Step 5: Predictions \((\hat{\mathbf{y}})\) & Errors</h4>
  <p>
    $$ 
    \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      205,333.33 \\
      276,000.00 \\
      333,333.33 \\
      404,000.00 \\
      461,333.33
    \end{bmatrix},\quad
    \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      -5333.33 \\ 4000.00 \\ 6666.67 \\ -4000.00 \\ -1333.33
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 6: Sum of Squared Errors (SSE)</h4>
  <p>
    $$
      \mathrm{SSE} = \sum_{i=1}^{5}(y_i - \hat{y}_i)^2 \;\approx\; 1.0667 \times 10^8.
    $$
  </p>

  <h4>Interpretation</h4>
  <ul>
    <li><strong>Intercept \(\beta_0\)</strong>: Baseline price when Size and Bedrooms are zero (a theoretical anchor).</li>
    <li><strong>\(\beta_1\)</strong>: Each additional sq.ft adds about \$114.67 to the price, holding Bedrooms fixed.</li>
    <li><strong>\(\beta_2\)</strong>: Each additional bedroom adds about \$13,333.33, holding Size fixed.</li>
  </ul>

    <h4>Why is SSE Large?</h4>
<p>A large SSE doesn't necessarily mean that the model is bad; it just means that the values you're predicting (house prices) are large. This is why you should scale the SSE to interpret it better, such as calculating the <b>Mean Squared Error (MSE)</b> or <b>Root Mean Squared Error (RMSE)</b>. These measures take the scale of the data into account:</p>

<p><b>Mean Squared Error (MSE):</b></p>
<p>
    $$ \text{MSE} = \frac{\text{SSE}}{m} $$
</p>
<p>Where <i>m</i> is the number of data points (or samples). MSE gives you the average squared error per data point. For this example, the MSE is approximately <b>21,333,333.33</b>.</p>

<p><b>Root Mean Squared Error (RMSE):</b></p>
<p>
    $$ \text{RMSE} = \sqrt{\text{MSE}} $$
</p>
<p>RMSE takes the square root of MSE, giving you the error in the same units as the original data (house prices, in this case), making it easier to interpret. For this example, the RMSE is approximately <b>4,618.80</b>.</p>

<p>These metrics give you an idea of how much error you’re getting on average per prediction (in terms of the original units of the data). A smaller MSE or RMSE indicates better performance of the model.</p>
<br>

    
      <h4>3.2 Ridge Regression</h4>
    
      <h4>3.3 Lasso Regression</h4>
      
  </div>

  </section>
  
  <section id="supervised-cla">
    
    <div id="classification">
      <h3>4. Classification</h3>
      <p>Classification algorithms <b><font color="red">predicte categories</font> </b> (e.g., spam detection, image recognition).
      Common methods include:
      </p>
      
      <ul>
        <li><strong>Logistic Regression:</strong> Models the probability that an input belongs to a certain category; suitable for binary and multiclass problems.</li>
        <li><strong>k-Nearest Neighbors (kNN):</strong> Assigns a class based on the majority class among the k closest data points.</li>
        <li><strong>Support Vector Machines (SVM):</strong> Finds the optimal boundary (hyperplane) that best separates different classes.</li>
        <li><strong>Decision Tree Classification:</strong> Uses a tree-like structure to make decisions and assign labels.</li>
        <li><strong>Naive Bayes:</strong> Probabilistic classifier based on Bayes’ theorem; effective for text classification and spam detection.</li>
        <li><strong>Neural Network Classification:</strong> Uses artificial neural networks to model complex patterns for multi-class and binary problems.</li>
      </ul>
      
    </div>
  </section>

  

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
