
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 2: Data Preprocessing & Feature Engineering</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }

    pre {
  position: relative;
}

pre .copy {
  position: absolute;
  top: 8px;
  right: 8px;
  padding: 4px 8px;
  background: #21111b;
  border: 1px solid #ff79c6;
  border-radius: 6px;
  color: #ffb3de;
  font-size: 0.8em;
  cursor: pointer;
}
pre .copy:hover {
  background: #ff79c6;
  color: #fff;
}

    
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#intro"><i class="fas fa-database"></i>1. What is Data Preprocessing?</a></li>
    <li><a href="#cleaning"><i class="fas fa-broom"></i>2. Data Cleaning</a></li>
    <li><a href="#transformation"><i class="fas fa-balance-scale"></i>3. Data Transformation</a></li>
    <li><a href="#feature-eng"><i class="fas fa-tools"></i>4. Feature Engineering</a></li>
    <li><a href="#feature-selection"><i class="fas fa-magic"></i>5. Feature Selection Techniques</a></li>
    <li><a href="#pipeline"><i class="fas fa-stream"></i>6. Pipelines for Preprocessing</a></li>
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 2: Data Preprocessing & Feature Engineering</h1>

  <section id="intro">
  <h3>1. What is Data Preprocessing?</h3>
  <p>
    <b>Data preprocessing</b> is the <b>entire set of steps</b> you take to prepare raw data so it can be effectively used in machine learning or analysis.
    It’s like the whole kitchen prep before cooking: washing, chopping, measuring, marinating.
  </p>

  <h4>It may include:</h4>
  <ul>
    <li><b>Data Cleaning:</b> remove duplicates, fix missing values, etc.</li>
    <li><b>Data Transformation:</b> scaling numbers, encoding categories, normalizing</li>
    <li><b>Feature Engineering:</b> creating new variables like BMI = weight/height²</li>
    <li><b>Feature Selection:</b> choosing the most important variables</li>
    <li><b>Dimensionality Reduction:</b> PCA, etc.</li>
  </ul>
</section>

  </section>

  <section id="cleaning">
    <h3>2. Data Cleaning</h3>
    
    <h4>What is Data Cleaning?</h4> 
    Data Cleaning (sometimes called data cleansing or data scrubbing) is the process of detecting and fixing errors, inconsistencies, and problems in raw data so it becomes accurate, consistent, and usable for analysis or modeling.
    Data cleaning is just one part of preprocessing.
    It specifically means fixing mistakes, inconsistencies, or errors in data.
    <br><br>
    Think of it like washing vegetables before cooking: the recipe (machine learning model or analysis) won’t work well if the ingredients (data) are dirty.
    <br>
    <h4>Typical Cleaning Steps: </h4>
  <ul>
    <li><b>Remove duplicates</b> – drop repeated rows/records.</li>
    <a href="https://xinyangmtsu.github.io/3350/duplicate.html" target="_blank">Removing Duplicates in Python</a>
    <br><br>
    <li><b>Handle missing values</b> – delete, impute, or flag them.</li>
    <a href="https://xinyangmtsu.github.io/3350/MissingValues.html" target="_blank">Handle Missing Values in Python</a>    
    <br><br>
    <li><b>Fix inconsistent formats</b> – e.g., “N/A”, “na”, “null” → standardize to NaN.
    unify values like “Male”, “M”, “male”.</li>
    <a href="https://xinyangmtsu.github.io/3350/Inconsistent.html" target="_blank">Fixing Inconsistent Formats in Python</a>
    <br><br>
  
    <!--
    <li><b>Correct typos or categories</b> – unify values like “Male”, “M”, “male”.</li>
    <li><b>Normalize units</b> – convert different units into the same scale (e.g., USD vs. EUR).</li>
    -->
    
    <li><b>Outlier handling</b> – investigate and correct extreme or invalid values.</li>
    <a href="https://xinyangmtsu.github.io/3350/Outlier.html" target="_blank">Outlier Handling in Python</a>
    <br><br>
    <li><b>Type conversion</b> – ensure columns have the right data type (dates, numbers).</li>
  </ul>
    
  </section>

  <section id="transformation">
    <h3>3. Data Transformation</h3>
    <h4>What is Data Transformation?</h4> 
    <p>Data Transformation is the process of converting cleaned data into a format that machine learning algorithms can understand and work with effectively.
       It goes beyond just “fixing mistakes” (cleaning). Instead, it’s about reshaping, scaling, or encoding the data so that models can learn from it correctly.</p>

    <h4>Common Data Transformation Techniques:</h4>

<h4>Feature Scaling</h4>
  <ul>
    <li><b>Standardization (Z-score):</b> mean = 0, variance = 1</li>
    <li><b>Min-Max Scaling:</b> rescale to [0, 1]</li>
    <li><b>Robust Scaling:</b> use median/IQR, less sensitive to outliers</li>
  </ul>

  <h4>Categorical Encoding</h4>
  <ul>
     <li><b>One-Hot Encoding:</b> Converts each category into binary columns</li>
      <li><b>Label Encoding:</b> Assigns an integer to each category (can mislead models if not ordinal)</li>
  </ul>

  <h4>Normalization</h4>
  <ul>
    <li><b>Normalization:</b> transform values to a common range or distribution 
      (e.g., log-transform skewed data).
    </li>
  </ul>
    
  </section>

  <section id="feature-eng">
    <h3>4. Feature Engineering</h3>
    <h4>What is Feature Engineering?</h4> 
    <p><b>Feature engineering</b> is the process of creating, modifying, or combining features (input variables) to improve the performance of machine learning models.
      Creating new features from existing ones can dramatically improve model performance.</p>

    <h4>Common Feature Engineering Techniques:</h4>

    <h4>Mathematical Transformations</h4>
  <ul>
    <li>Log, square root, polynomial terms (e.g., <code>age²</code>).</li>
  </ul>

  <h4>Interaction Features</h4>
  <ul>
    <li>Multiply/combine two features (e.g., <code>height * weight = body surface area</code>).</li>
  </ul>

  <h4>Date/Time Features</h4>
  <ul>
    <li>Extract year, month, weekday, hour, time since event, seasonality.</li>
  </ul>

  <h4>Domain-Specific Features</h4>
  <ul>
    <li>Finance: debt-to-income ratio.</li>
    <li>Healthcare: BMI = weight/height².</li>
    <li>Marketing: customer lifetime value.</li>
  </ul>

  <h4>Aggregations</h4>
  <ul>
    <li>Total spend per customer, average order value, counts of transactions.</li>
  </ul>

  <h4>Binning/Discretization</h4>
  <ul>
    <li>Convert continuous variables into categories (e.g., age groups).</li>
  </ul>
    
  </section>

  <section id="feature-selection">
    <h3>5. Feature Selection </h3>
     <h4>What is Feature Selection?</h4>
    <b>Feature selection</b> is the process of choosing the most important input variables (features) for building your machine learning model, while removing irrelevant or redundant ones.
    
    <h4>Common Feature Selection Methods</h4>

  <h4>Filter Methods (statistical tests, independent of model)</h4>
  <ul>
    <li>Correlation scores</li>
    <li>Chi-square test</li>
    <li>ANOVA F-test</li>
  </ul>

  <h4>Wrapper Methods (evaluate subsets with a model)</h4>
  <ul>
    <li>Recursive Feature Elimination (RFE)</li>
    <li>Forward/Backward Selection</li>
  </ul>

  <h4>Embedded Methods (built into algorithms)</h4>
  <ul>
    <li>Lasso (L1) regularization → shrinks coefficients to zero</li>
    <li>Tree-based feature importance (Random Forest, XGBoost)</li>
  </ul>
    
  </section>

  <section id="pipeline">
    <h3>6. Pipelines for Preprocessing</h3>
    <p>Scikit-learn’s <code>Pipeline</code> allows you to bundle preprocessing and modeling steps together, making your code cleaner and more reproducible.</p>
    <pre>
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipe = Pipeline([
  ('scaler', StandardScaler()),
  ('model', LogisticRegression())
])
pipe.fit(X_train, y_train)
    </pre>
  </section>

  <section id="references">
    <h3>References</h3>
    <ul>
      <li><a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank">Scikit-learn: Preprocessing</a></li>
      <li><a href="https://www.kaggle.com/learn/feature-engineering" target="_blank">Kaggle: Feature Engineering Course</a></li>
      <li><a href="https://www.analyticsvidhya.com/blog/2020/10/feature-engineering-techniques-explained/" target="_blank">AV: Feature Engineering</a></li>
    </ul>
  </section>


  <script>
  document.querySelectorAll("pre .copy").forEach(button => {
    button.addEventListener("click", () => {
      const code = button.nextElementSibling.innerText;
      navigator.clipboard.writeText(code).then(() => {
        const old = button.textContent;
        button.textContent = "Copied!";
        setTimeout(() => button.textContent = old, 1500);
      });
    });
  });
</script>

  
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>
</body>
</html>
