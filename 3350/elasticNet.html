
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Worked Example: OLS vs Ridge vs Lasso vs Elastic Net (High-Dimensional Breast Cancer)</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6;
      padding: 20px;
    }

    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
      max-width: 950px;
      margin-left: auto;
      margin-right: auto;
    }

    h1, h2, h4 {
      margin: 0 0 10px 0;
      color: #ffb3de;
      font-weight: 700;
    }

    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }

    a.anchor {
      text-decoration: none;
      margin-left: .5rem;
      opacity: .6;
      color: #ffb3de;
    }
    a.anchor:hover { opacity: 1; }

    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
      position: relative;
      max-width: 100%;
    }

    pre .copy {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      background: #21111b;
      border: 1px solid #ff79c6;
      border-radius: 6px;
      color: #ffb3de;
      font-size: 0.8em;
      cursor: pointer;
    }
    pre .copy:hover {
      background: #ff79c6;
      color: #fff;
    }

    ul.toc {
      padding-left: 18px;
      margin-top: 8px;
    }
    ul.toc li { margin: 4px 0; }
    ul.toc a { color: #ffb3de; }
  </style>
</head>

<body>

<section>
  <center>
    <h1>Worked Example: OLS vs Ridge vs Lasso vs Elastic Net</h1>
    <div>High-Dimensional Breast Cancer Data (Polynomial Expansion)</div>
  </center>

  <h4>Method: OLS, Ridge (L2), Lasso (L1), and Elastic Net (L1+L2) with PolynomialFeatures (degree = 3)</h4>

  <pre><button class="copy">Copy</button><code>
# ========================================================
# Breast Cancer Data — OLS vs Ridge vs Lasso vs Elastic Net (FAST, no CV)
# Polynomial feature expansion, train/test split, compare
# ========================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# Fixed hyperparameters 
RIDGE_ALPHA = 1000
LASSO_ALPHA = 0.008227241341700473
LASSO_MAX_ITER = 20000
ENET_ALPHA = 0.002121
ENET_L1_RATIO = 0.900
ENET_MAX_ITER = 4576

# 1) Load dataset
data = load_breast_cancer()
X, y = data.data, data.target   # binary labels (0/1)
n_features = X.shape[1]

# 2) Train / test split (before poly expansion!)
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=RANDOM_STATE
)

# 3) Polynomial feature expansion (degree=3)
# Includes x, x^2, x^3 and all cross-terms --> high dimensional
poly = PolynomialFeatures(degree=3)
X_train = poly.fit_transform(X_train_raw)  # (fit only on training data)
X_test  = poly.transform(X_test_raw)

print(f"Original features: {n_features} --> After expansion: {X_train.shape[1]}")

# 4) Helper function to evaluate models
def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)

    # --- Train ---
    y_pred_train = model.predict(X_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    rmse_train = np.sqrt(mse_train)
    r2_train = r2_score(y_train, y_pred_train)
    sse_train = np.sum((y_train - y_pred_train) ** 2)

    # --- Test ---
    y_pred_test = model.predict(X_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    rmse_test = np.sqrt(mse_test)
    r2_test = r2_score(y_test, y_pred_test)
    sse_test = np.sum((y_test - y_pred_test) ** 2)

    # Collect results
    results = pd.DataFrame([
        {"Model": name, "Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Model": name, "Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ])
    return results

# 5) Define models (no cross-validation; fixed params; fast)
ols = Pipeline([
    ("scaler", StandardScaler()),
    ("linreg", LinearRegression())
])

ridge = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=RIDGE_ALPHA, random_state=RANDOM_STATE))
])

lasso = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=LASSO_ALPHA, max_iter=LASSO_MAX_ITER, selection="cyclic", random_state=RANDOM_STATE))
])

elastic_net = Pipeline([
    ("scaler", StandardScaler()),
    ("elastic", ElasticNet(alpha=ENET_ALPHA, l1_ratio=ENET_L1_RATIO,
                           max_iter=ENET_MAX_ITER, selection="cyclic", random_state=RANDOM_STATE))
])

# 6) Evaluate models
results_df = pd.DataFrame(
    pd.concat([
        evaluate_model("OLS",         ols,         X_train, y_train, X_test, y_test),
        evaluate_model("Ridge",       ridge,       X_train, y_train, X_test, y_test),
        evaluate_model("Lasso",       lasso,       X_train, y_train, X_test, y_test),
        evaluate_model("Elastic Net", elastic_net, X_train, y_train, X_test, y_test),
    ])
).set_index(["Model", "Split"])

# 7) Display results + (echo) fixed hyperparameters and achieved iterations
print("\n=== Train/Test Metrics (high-dimensional case) ===")
print(results_df)

print("\nHyperparameters used (no CV):")
print(f"Ridge alpha:           {ridge.named_steps['ridge'].alpha:.6f}")
print(f"Lasso alpha:           {lasso.named_steps['lasso'].alpha:.6f}")
print(f"Elastic Net alpha:     {elastic_net.named_steps['elastic'].alpha:.6f}")
print(f"Elastic Net l1_ratio:  {elastic_net.named_steps['elastic'].l1_ratio:.3f}")

# After fit, report actual iterations taken (if available)
if hasattr(lasso.named_steps["lasso"], "n_iter_"):
    print(f"Lasso iterations (n_iter_):       {lasso.named_steps['lasso'].n_iter_}")
if hasattr(elastic_net.named_steps["elastic"], "n_iter_"):
    print(f"Elastic Net iterations (n_iter_): {elastic_net.named_steps['elastic'].n_iter_}")

# 8) Coefficient magnitudes
coef_ols         = ols.named_steps["linreg"].coef_
coef_ridge       = ridge.named_steps["ridge"].coef_
coef_lasso       = lasso.named_steps["lasso"].coef_
coef_elastic_net = elastic_net.named_steps["elastic"].coef_

plt.figure(figsize=(12, 5))
idx = np.arange(len(coef_ols))
plt.semilogy(idx, np.abs(coef_ols),         "o", markersize=3, label="OLS", alpha=0.7)
plt.semilogy(idx, np.abs(coef_ridge),       "s", markersize=3, label="Ridge", alpha=0.7)
plt.semilogy(idx, np.abs(coef_lasso),       "^", markersize=3, label="Lasso", alpha=0.7)
plt.semilogy(idx, np.abs(coef_elastic_net), "d", markersize=3, label="Elastic Net", alpha=0.7)
plt.title("Coefficient Magnitudes (OLS vs Ridge vs Lasso vs Elastic Net)")
plt.xlabel("Feature index")
plt.ylabel("Absolute coefficient (log scale)")
plt.legend()
plt.tight_layout()
plt.show()

# 9) Sparsity report
nz_lasso   = np.count_nonzero(coef_lasso)
nz_elastic = np.count_nonzero(coef_elastic_net)
total_coef = len(coef_lasso)

print(f"\nSparsity comparison:")
print(f"Lasso nonzero coefficients:       {nz_lasso} / {total_coef} ({nz_lasso/total_coef:.2%} nonzero)")
print(f"Elastic Net nonzero coefficients: {nz_elastic} / {total_coef} ({nz_elastic/total_coef:.2%} nonzero)")

# 10) Performance summary table
print("\n=== Performance Summary ===")
test_scores  = results_df.loc[(slice(None), "Test"),  "R^2"]
train_scores = results_df.loc[(slice(None), "Train"), "R^2"]

summary = pd.DataFrame({
    "Train R^2": train_scores.values,
    "Test R^2":  test_scores.values,
    "Overfitting (Train - Test)": train_scores.values - test_scores.values
}, index=["OLS", "Ridge", "Lasso", "Elastic Net"]).round(4)
print(summary)

# 11) Discussion
print("\n=== Analysis ===")
print("OLS: Overfits completely → poor test performance.")
print("Ridge: Smooth shrinkage, good test performance, keeps all features.")
print("Lasso: Good test performance with automatic feature selection (sparsity); can be unstable with highly correlated features.")
print("Elastic Net: Combines Ridge and Lasso → balances shrinkage and sparsity; more stable than Lasso when predictors are correlated.")
  </code></pre>

  <h4>Why Use Log Scale?</h4>
  The <b>semilogy()</b> function uses a logarithmic y-axis because:
  <ul>
    <li>OLS coefficients can vary by orders of magnitude (some might be 0.01, others 1000+).</li>
    <li>Log scale lets you see both small and large coefficients clearly on the same plot.</li>
    <li>Makes it easy to see the shrinkage effects of Ridge, Lasso, and Elastic Net.</li>
    <li>We cannot take the logarithm of negative numbers or zero.</li>
  </ul>
</section>

<script>
  document.querySelectorAll("pre .copy").forEach(button => {
    button.addEventListener("click", () => {
      const code = button.nextElementSibling.innerText;
      navigator.clipboard.writeText(code).then(() => {
        const old = button.textContent;
        button.textContent = "Copied!";
        setTimeout(() => button.textContent = old, 1500);
      });
    });
  });
</script>

</body>
</html>
