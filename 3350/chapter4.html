<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 4: Supervised Learning - Classification</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>

    table, tr, th, td {
      border: 1px solid
    }
    
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }



     :root{
    --bg:#0f172a;          /* slate-900 */
    --panel:#111827;       /* gray-900 */
    --muted:#94a3b8;       /* slate-400 */
    --text:#e5e7eb;        /* gray-200 */
    --cA:#22d3ee;          /* cyan-400 (Class A) */
    --cB:#a78bfa;          /* violet-400 (Class B) */
    --query:#fbbf24;       /* amber-300 */
    --accent:#34d399;      /* emerald-400 */
  }

  .wrap{max-width:900px;margin:40px auto;padding:0 20px}
  .muted{color:var(--muted)}
  .card{
    background:linear-gradient(180deg,rgba(255,255,255,.02),rgba(255,255,255,.01));
    border:1px solid rgba(148,163,184,.18);
    border-radius:14px;padding:18px
  }
  .row{display:flex;gap:18px;align-items:center;flex-wrap:wrap}
  .pill{
    display:inline-flex;align-items:center;gap:8px;
    padding:6px 12px;border-radius:999px;border:1px solid rgba(148,163,184,.25);
    background:rgba(2,6,23,.35);font-weight:600
  }
  .pill b{letter-spacing:.2px}
  .legend{display:flex;gap:14px;align-items:center;flex-wrap:wrap}
  .legend .key{display:inline-flex;align-items:center;gap:8px}
  .dot{width:12px;height:12px;border-radius:50%}
  .dot.a{background:var(--cA)}
  .dot.b{background:var(--cB)}
  .dot.q{background:var(--query);border:1px solid rgba(0,0,0,.25)}
  .slider{display:flex;align-items:center;gap:10px}
  input[type="range"]{width:200px}
  .notice{font-size:.95rem;color:var(--muted)}
  svg{width:100%;max-width:900px;height:auto;background:linear-gradient(180deg,rgba(255,255,255,.02),rgba(255,255,255,.01));border-radius:12px}
  .axis{stroke:#1f2937;stroke-width:1}
  .grid{stroke:#1f2937;stroke-width:.5;opacity:.5}
  .pt{stroke:rgba(0,0,0,.25);stroke-width:1}
  .pt.a{fill:var(--cA)}
  .pt.b{fill:var(--cB)}
  .pt.nn{stroke-width:3}
  .query{fill:var(--query);stroke:#000;stroke-width:1.2}
  .halo{fill:none;stroke-dasharray:6 6;stroke:#e5e7eb;opacity:.7}
  .label{fill:#e5e7eb;font-size:12px}
  .predict{
    border:1px solid rgba(52,211,153,.35);
    background:linear-gradient(180deg,rgba(16,185,129,.12),rgba(16,185,129,.06));
    border-radius:12px;padding:10px 12px;margin-left:auto
  }
    
    
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Supervised Learning?</a></li>
    
    <li>
      <a href="#classification"><i class="fas fa-gamepad"></i>2. Classification</a>
      <div class="subsections">
        <a href="#logistic-regression">2.1 Logistic Regression</a>
        <a href="#knn">2.2 K-Nearest Neighbors (KNN)</a>
        <a href="#svm">2.3 Support Vector Machines (SVM)</a>
        <a href="#dt">2.4 Decision Tree </a>
        <a href="#nb">2.5 Naive Bayes </a>
      </div>
    </li>

    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 4: Supervised Learning - Classification</h1>

  <section id="what-is-ml">
    <h2>1. What is Supervised Learning?</h2>
    <p>
      Supervised learning is a type of machine learning where you train a model using a labeled dataset — 
      meaning the training data includes both the <b><font color="red">input</font></b> and the 
      <b><font color="red">correct output</font></b>.
    </p>

    <h4>The goal:</h4>
    <ul>
      <Li>The model learns the relationship between <b><font color="red">inputs</font></b> (features) 
          and <b><font color="red">outputs</font></b> (labels).</Li>
      <li>Once trained, it can predict the output for new, unseen inputs.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Think of supervised learning like a student studying with a set of practice problems and the answer key. 
      The student uses these examples to learn patterns, then solves new problems without seeing the answers.
    </p>

    <h4>
      The two main types of supervised learning are:
    </h4>

    <ul>
       <li>1. Regression</li>
       <li><b>2. Classification</b></li>
    </ul>
    
  </section>
    

  <section id="supervised-cla">
    
    <div id="classification">
      <h3>2. Classification</h3>
      <p>Classification algorithms <b><font color="red">predicte categories</font> </b> (e.g., spam detection, image recognition).
      Common methods include:
      </p>
      
      <ul>
        <li><strong>Logistic Regression:</strong> Models the probability that an input belongs to a certain category; suitable for binary and multiclass problems.</li>
        <li><strong>K-Nearest Neighbors (KNN):</strong> Assigns a class based on the majority class among the k closest data points.</li>
        <li><strong>Support Vector Machines (SVM):</strong> Finds the optimal boundary (hyperplane) that best separates different classes.</li>
        <li><strong>Decision Tree Classification:</strong> Uses a tree-like structure to make decisions and assign labels.</li>
         <li><strong>Random Forests:</strong> An ensemble of multiple decision trees; combines their outputs to improve accuracy and reduce overfitting.</li>
        <li><strong>Naive Bayes:</strong> Probabilistic classifier based on Bayes’ theorem; effective for text classification and spam detection.</li>
    
        <!--
        <li><strong>Neural Network Classification:</strong> Uses artificial neural networks to model complex patterns for multi-class and binary problems.</li>
        -->
      </ul>
      
    </div>
  </section>


<section id="logistic-regression">
    
    <h3>2.1 Logistic Regression</h3>

    <!-- 1. Motivation -->
    <h4>1. Motivation</h4>
    <div class="card">
      <p><strong>Problem with Linear Regression for classification:</strong></p>
      <ul>
        <li>Linear Regression can predict any real number (e.g., −3.2 or 2.7).</li>
        <li>But classification tasks need <em>Yes/No</em> outputs (Admit/Reject, Spam/Not Spam, Disease/No Disease).</li>
        <li>We need outputs between <strong>0 and 1</strong> so we can interpret them as <strong>probabilities</strong>.</li>
      </ul>
    </div>

    <!-- 2. Idea -->
    <h4>2. Idea</h4>
    <p>Logistic Regression takes the linear model from Linear Regression:</p>
    <p class="eq">
      $$ z \;=\; \beta_0 \;+\; \beta_1 x_1 \;+\; \cdots \;+\; \beta_p x_p $$
    </p>
    <p>and passes it through the <em>sigmoid</em> (logistic) function:</p>
    <p class="eq">
      $$ \sigma(z) \;=\; \frac{1}{1 + e^{-z}} $$
    </p>
    <ul>
      <li>Output is always in \((0,1)\).</li>
      <li>Can be interpreted as the probability of class \(1\).</li>
    </ul>


     <!-- 8. Visualization -->
    <h4>The Sigmoid (Logistic) Function</h4>

    <ul>
      <li>1. Definition</li>
        <div class="card">
          <p class="eq">$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$</p>
          <ul>
            <li><strong>Input:</strong> any real number \( z \in (-\infty, +\infty) \).</li>
            <li><strong>Output:</strong> a number between 0 and 1.</li>
          </ul>
        </div>

      <li>2. Shape</li>
        <ul>
          <li>The graph looks like an S-shaped curve.</li>
          <li>As \( z \to +\infty \), \( \sigma(z) \to 1 \).</li>
          <li>As \( z \to -\infty \), \( \sigma(z) \to 0 \).</li>
          <li>At \( z = 0 \), \( \sigma(0) = 0.5 \).</li>
        </ul>
        <p>That’s why it’s perfect for mapping any real number into a probability.</p>

      <li>3. Why it’s used in Logistic Regression</li>
        <p>Logistic Regression first computes a linear combination:</p>
        <p class="eq">$$ z = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$</p>
        <p>But \(z\) can be any real number. To turn \(z\) into a probability, we pass it through the sigmoid:</p>
        <p class="eq">$$ P(y=1 \mid x) = \sigma(z) $$</p>
        <p>The result is always between 0 and 1 → interpretable as probability.</p>
      
    </ul>

    <canvas id="sigmoidCanvas" width="720" height="380" aria-label="Sigmoid function plot"></canvas>
    <center>
    <div class="caption">Sigmoid function \( \sigma(z) = \dfrac{1}{1 + e^{-z}} \)</div>
    </center>
  
  <script>
    // Draw a simple sigmoid plot on the canvas (z in [-8, 8])
    (function drawSigmoid() {
      const canvas = document.getElementById('sigmoidCanvas');
      if (!canvas || !canvas.getContext) return;
      const ctx = canvas.getContext('2d');
      const W = canvas.width, H = canvas.height;

      // Padding for axes
      const pad = { left: 60, right: 20, top: 20, bottom: 50 };
      const plotW = W - pad.left - pad.right;
      const plotH = H - pad.top - pad.bottom;

      // Domain & range
      const zMin = -8, zMax = 8;
      const pMin = 0,  pMax = 1;

      const xScale = z => pad.left + ( (z - zMin) / (zMax - zMin) ) * plotW;
      const yScale = p => pad.top + (1 - ( (p - pMin) / (pMax - pMin) )) * plotH; // invert y

      // Clear
      ctx.clearRect(0, 0, W, H);

      // Background
      ctx.fillStyle = '#ffffff';
      ctx.fillRect(0, 0, W, H);

      // Axes
      ctx.strokeStyle = '#cbd5e1';
      ctx.lineWidth = 1;

      // Border of plot area
      ctx.strokeRect(pad.left, pad.top, plotW, plotH);

      // Horizontal grid lines at p = 0, 0.5, 1
      const gridPs = [0, 0.5, 1];
      ctx.beginPath();
      gridPs.forEach(p => {
        const y = yScale(p);
        ctx.moveTo(pad.left, y);
        ctx.lineTo(pad.left + plotW, y);
      });
      ctx.stroke();

      // Vertical grid lines at z = -6, -3, 0, 3, 6
      const gridZs = [-6, -3, 0, 3, 6];
      ctx.beginPath();
      gridZs.forEach(z => {
        const x = xScale(z);
        ctx.moveTo(x, pad.top);
        ctx.lineTo(x, pad.top + plotH);
      });
      ctx.stroke();

      // Labels
      ctx.fillStyle = '#374151';
      ctx.font = '12px Roboto, sans-serif';
      // y labels
      gridPs.forEach(p => {
        const y = yScale(p);
        ctx.fillText(p.toFixed(1), 28, y + 4);
      });
      // x labels
      gridZs.forEach(z => {
        const x = xScale(z);
        ctx.fillText(z.toString(), x - 6, pad.top + plotH + 18);
      });

      // Axis titles
      ctx.font = '13px Roboto, sans-serif';
      ctx.fillText('z', pad.left + plotW / 2, H - 18);
      ctx.save();
      ctx.translate(18, pad.top + plotH / 2);
      ctx.rotate(-Math.PI / 2);
      ctx.fillText('Probability  p = σ(z)', 0, 0);
      ctx.restore();

      // Sigmoid curve
      ctx.strokeStyle = '#2563eb';
      ctx.lineWidth = 2;
      ctx.beginPath();
      const steps = 600;
      for (let i = 0; i <= steps; i++) {
        const t = i / steps;
        const z = zMin + t * (zMax - zMin);
        const p = 1 / (1 + Math.exp(-z));
        const x = xScale(z);
        const y = yScale(p);
        if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
      }
      ctx.stroke();

      // Mark p=0.5 at z=0
      const x0 = xScale(0);
      const y05 = yScale(0.5);
      ctx.fillStyle = '#1f2937';
      ctx.beginPath();
      ctx.arc(x0, y05, 3.5, 0, 2 * Math.PI);
      ctx.fill();

      // Dashed helpers to axes
      ctx.setLineDash([5, 4]);
      ctx.strokeStyle = '#9ca3af';
      ctx.beginPath();
      ctx.moveTo(x0, y05);
      ctx.lineTo(x0, pad.top + plotH);
      ctx.moveTo(x0, y05);
      ctx.lineTo(pad.left, y05);
      ctx.stroke();
      ctx.setLineDash([]);
    })();
  </script>

  <br>

  Logistic regression can handle both binary classification (using the sigmoid function to output probabilities for two classes) and multi-class classification 
  (using the softmax function to output probability distributions across multiple classes).
  <br>
    <!-- 3. Equation -->
    <h4>3. Equation</h4>
    <p class="eq">
      $$ P(y=1 \mid x) \;=\; \frac{1}{\,1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}\,} $$
    </p>
    <p class="center small">If the probability \(\ge 0.5\) → predict \(1\); otherwise \(0\).</p>

    <!-- 4. Interpretation -->
    <!-- 
    <h4>4. Interpretation of Coefficients</h4>
    <p>Each coefficient \(\beta_j\) changes the <strong>log-odds</strong> of the outcome:</p>
    <p class="eq">
      $$ \log\!\left(\frac{p}{1-p}\right) \;=\; \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$
    </p>
    <p>If you exponentiate \(\beta_j\), you get an <strong>odds ratio</strong>:</p>
    <ul>
      <li>\(e^{\beta_j} &gt; 1\): increases chance of the event.</li>
      <li>\(e^{\beta_j} &lt; 1\): decreases chance of the event.</li>
    </ul>
    -->
  
    <!-- 5. Objective -->
    <h4>4. Objective Function</h4>
  
    <p>Instead of minimizing squared error, Logistic Regression uses <strong>Maximum Likelihood Estimation (MLE)</strong>.</p>

  <h4>Maximum Likelihood Estimation (MLE)</h4>
  <p><strong>Idea:</strong> Given some observed data, MLE asks: "What parameter values would make this data we observed most likely to occur?"
  </p>
   
  <p>Goal: Find \(\beta\) that maximizes \( \ell(\boldsymbol{\beta}) \) the log-likelihood:</p>
   <div style="text-align:center; margin:6px 0;">
      $$ \ell(\boldsymbol{\beta}) \;=\; \sum_{i=1}^{m} \Big[\, y_i \log p_i \;+\; (1-y_i)\log(1-p_i) \,\Big]. $$
    </div>

  <strong>or</strong>
  
  <p>Equivalently, We minimize the negative log-likelihood, also called log loss or cross-entropy:</p>

 <div style="text-align:center; margin:6px 0;">
          $$ J(\boldsymbol{\beta}) \,=\, -\ell(\boldsymbol{\beta}) \,=\, -\sum_{i=1}^{m} \Big[\, y_i \log p_i \;+\; (1-y_i)\log(1-p_i) \,\Big]. $$
        </div>

  In machine learning, we typically frame everything as minimization problems, so we minimize the negative log-likelihood.
  
  <h4>Solving the MLE Objective Function for Logistic Regression</h4>

  The MLE problem for logistic regression <b>cannot be solved analytically</b> (no closed-form solution exists). 
  Instead, we use <b>iterative optimization methods</b>. Gradient descent is the basic algorithm used to 
  solve this type of optimization problem, but in practice, more efficient solvers are often preferred.
  <br><br>
  In scikit-learn's LogisticRegression, common solvers include LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno), 
  liblinear (coordinate descent), newton-cg (Newton's method with conjugate gradient), 
  sag/saga (stochastic average gradient), which iteratively update parameters to 
  minimize the negative log-likelihood.
  
  <!--
  <h4>How it works?</h4>

  <ul>
    <li>1. Logistic Regression Setup</li>

      <div style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:14px 16px; margin:12px 0;">
        <div style="text-align:center; margin:6px 0;">
          $$ p_i \;=\; P(y_i=1 \mid x_i;\,\boldsymbol{\beta}) \;=\; \sigma(z_i)
             \;=\; \frac{1}{1+e^{-z_i}}, \qquad z_i \;=\; x_i^{\top}\boldsymbol{\beta}. $$
        </div>
        <p style="margin:6px 0 0;">with binary outcomes \(y_i \in \{0,1\}\), \(i=1,\dots,m\).</p>
    </div>

    <strong>Bernoulli form: </strong>
    
    <p style="margin:6px 0;">Each label \(y_i \in \{0,1\}\) has</p>
      <div style="text-align:center; margin:8px 0;">
        $$ 
        P(y_i \mid x_i;\boldsymbol{\beta}) \;=\;
        \begin{cases}
          p_i, & y_i=1,\\[4pt]
          1-p_i, & y_i=0
        \end{cases}
        \;=\; p_i^{\,y_i}\,(1-p_i)^{\,1-y_i}.
        $$
      </div>
    
      <p><em>(The exponents \(y_i\) and \(1-y_i\) act like on/off switches.)</em></p>
    <br>
    
    <li>2. Form the likelihood:</li>

     <strong>Independence across samples (i.i.d.)</strong>
      <p style="margin:6px 0;">Assuming examples are independent given their features and \(\boldsymbol{\beta}\), the joint probability of the whole dataset is the product of per-sample probabilities:</p>
      
    <div style="text-align:center; margin:8px 0;">
        $$ L(\boldsymbol{\beta}) \;=\; \prod_{i=1}^{m} P(y_i \mid x_i;\boldsymbol{\beta}). $$
      </div>
    
     <strong>Plug the Bernoulli form in</strong>

    <div style="text-align:center; margin:6px 0;">
      $$ L(\boldsymbol{\beta}) \;=\; \prod_{i=1}^{m} \; p_i^{\,y_i}\; (1-p_i)^{\,1-y_i}. $$
    </div>

      <ul style="padding-left:1.1rem; margin:12px 0;">
    <li style="margin:6px 0;">
      <strong>\(L(\theta)\) — the likelihood:</strong>
      a score (function of the parameters \(\theta\)) that tells you how plausible \(\theta\) is
      given the data you observed.
    </li>

    <li style="margin:6px 0;">
      <strong>\(\displaystyle \prod_{i=1}^{m}\) — “product from \(i=1\) to \(m\)”:</strong>
      multiply one term for each data point in the dataset.
    </li>

    <li style="margin:6px 0;">
      <strong>\(p(y_i \mid x_i;\,\theta)\):</strong>
      the model’s probability (or probability density) of observing label \(y_i\) for example \(i\),
      given its features \(x_i\) and parameters \(\theta\).
    </li>
  </ul>

  <div style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:14px 16px; margin:12px 0;">
    <div style="text-align:center; margin:6px 0;">
      <strong>Note.</strong> This product form assumes examples are independent (given their features); that’s why we can multiply their individual probabilities.
    </div>
  </div>
  <br>
    
    <li>3. Log-Likelihood (Take logs: turns products into sums (easier, more stable))</li>
   
    <p>Apply <em>log</em>(·) to both sides:</p>
    
    <div style="text-align:center; margin:10px 0; background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px;">
    $$ \ell(\theta) 
        \;=\; \log L(\theta) \;=\; \sum_{i=1}^{m} \log\, p\!\left(y_i \mid x_i;\,\theta\right) 
        \;=\; \sum_{i=1}^{m} \log\!\Big( p_i^{\,y_i}\,(1-p_i)^{\,1-y_i} \Big).
      
      $$
  </div>

  <p style="margin-top:10px;">We used the identity</p>
  <div style="text-align:center; margin:8px 0;">
    $$ \log\!\Bigg(\prod_{i} a_i\Bigg) \;=\; \sum_{i} \log a_i. $$
  </div>

    <h4>Why do this? (three big reasons)</h4>
  <ul style="padding-left:1.1rem; margin:10px 0;">
    <li style="margin:8px 0;">
      <strong>Numerical stability:</strong> Products of many small probabilities underflow to 0; sums of logs don’t.
    </li>

    <li style="margin:8px 0;">
      <strong>Optimization convenience:</strong> sums are easier to differentiate:
      <div style="text-align:center; margin:8px 0;">
        $$ \nabla_{\theta}\,\ell(\theta) \;=\; \sum_{i=1}^{m} \nabla_{\theta}\,\log p\!\left(y_i \mid x_i;\,\theta\right), $$
      </div>
      so the gradient is just an additive sum of per-example gradients.
    </li>

    <li style="margin:8px 0;">
      <strong>Same optimum:</strong> \(\log(\cdot)\) is strictly increasing, so
      <div style="text-align:center; margin:8px 0;">
        $$ \arg\max_{\theta}\, L(\theta) \;=\; \arg\max_{\theta}\, \ell(\theta). $$
      </div>
      Maximizing the product or the sum of logs gives the same best \(\theta\).
    </li>
  </ul>
  
     
    <p>(Equivalently, minimize \(-\ell(\boldsymbol{\beta})\), the <em>cross-entropy loss</em>.)</p>
    
  <br>

    <li>4. Expand each term using \( \log(a^b)=b\log a \)</li>
    
  <div style="text-align:center; margin:8px 0;">
    $$ \log\!\Big( p_i^{\,y_i}\,(1-p_i)^{\,1-y_i} \Big)
       \;=\; y_i \log p_i \;+\; (1-y_i)\log(1-p_i). $$
  </div>

    <li>5. Combine terms</li>
    
  <div style="text-align:center; margin:8px 0; background:#eef6ff; border:1px solid #c7e0ff; border-radius:10px; padding:12px;">
    $$ \boxed{ \;
       \ell(\boldsymbol{\beta})
       \;=\; \sum_{i=1}^{m} \Big[\, y_i \log p_i \;+\; (1-y_i)\log(1-p_i) \,\Big]
       \; } $$
  </div>

  <p style="margin-top:10px; text-align:center;">
    where \( \; p_i = \sigma(x_i^{\top}\boldsymbol{\beta}) = \dfrac{1}{1+e^{-x_i^{\top}\boldsymbol{\beta}}} \; \).
  </p>

    <br>
    
     <div style="background:#fff7ed; border:1px solid #fed7aa; border-radius:10px; padding:12px 14px; margin-top:10px;">
    There’s no closed-form solution for \( \boldsymbol{\beta} \) in logistic regression, so we use iterative optimization.
  </div>

    <br>
    
    <li>6. Optimize (MLE): choose parameters that maximize the log-likelihood</li>

    <div style="text-align:center; margin:8px 0;">
      $$ \hat{\theta} \;=\; \arg\max_{\theta}\; \ell(\theta). $$
    </div>

    
     <li>7. Minimize the Cross-Entropy Loss</li>

    
    <p style="margin:0 0 6px;"><strong>Equivalence:</strong> maximizing the log-likelihood is the same as minimizing its negative.</p>
    
    <div style="text-align:center; margin:6px 0;">
      $$ \arg\max_{\theta}\,\ell(\theta) \;=\; \arg\min_{\theta}\,\big(-\ell(\theta)\big). $$
    </div>

    <p style="margin:6px 0;">The negative log-likelihood equals the cross-entropy loss:</p>
  <div style="text-align:center; margin:8px 0; background:#eef6ff; border:1px solid #c7e0ff; border-radius:10px; padding:12px;">
    $$ -\ell(\boldsymbol{\beta}) \;=\; \sum_{i=1}^{m}
       \Big[ -y_i \log p_i \;-\; (1-y_i)\log(1-p_i) \Big],
       \qquad p_i=\sigma\!\big(x_i^{\top}\boldsymbol{\beta}\big). $$
  </div>
    
  <p><strong>Cross-entropy</strong> is just the <em>negative log-likelihood</em> for binary logistic regression.</p>

  <div style="text-align:center; margin:10px 0; background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px;">
    $$ \min_{\boldsymbol{\beta}}\; -\ell(\boldsymbol{\beta})
       \;=\; \min_{\boldsymbol{\beta}} \sum_{i=1}^{m}
       \Big[\,-y_i \log p_i \;-\; (1-y_i)\log(1-p_i)\,\Big], \qquad
       p_i=\sigma(x_i^{\top}\boldsymbol{\beta}). $$
  </div>

  <p style="margin-top:10px;">Often we use the <em>average</em> loss (same minimizer):</p>
  <div style="text-align:center; margin:8px 0;">
    $$ \min_{\boldsymbol{\beta}}\; \frac{1}{m}\sum_{i=1}^{m}
       \Big[\,-y_i \log p_i \;-\; (1-y_i)\log(1-p_i)\,\Big]. $$
  </div>

  <div style="background:#eef6ff; border:1px solid #c7e0ff; border-radius:10px; padding:12px 14px; margin:12px 0;">
    <strong>Why it’s equivalent:</strong> \(\log(\cdot)\) is strictly increasing, so maximizing the log-likelihood
    \(\ell(\boldsymbol{\beta})\) is the same as minimizing its negative, i.e., the cross-entropy loss.
  </div>
    
    
    
  </ul>


  <h4>How do we find \( \boldsymbol{\beta} \)?</h4>

  <p>We optimize the log-likelihood (or minimize cross-entropy). There is no closed-form solution, 
    so we use iterative methods.</p>

  <h5>How the Iteration Works?</h5>
  
  <div style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px 14px;">
    <p style="margin:0 0 8px;">
      Start with an initial guess for the coefficients \( \boldsymbol{\beta} \), compute predictions, compare to the true labels, and update \( \boldsymbol{\beta} \) in the improving direction (e.g., gradient or Newton/IRLS step).
    </p>
    <p style="margin:0;">
      Repeat this update–evaluate cycle until improvement is negligible—i.e., the log-likelihood no longer increases (equivalently, cross-entropy no longer decreases) and the parameters have converged.
      <strong>"Parameters have converged"</strong> means your coefficient vector \( \boldsymbol{\beta} \) has essentially stopped changing and you’re at (or extremely near) the optimum.
    </p>
    </p>
  </div>
  
  <p><strong>Two common choices:</strong></p>

  <ul>
    <li>(i) Gradient Descent  </li>
    <li>(ii) Newton’s Method </li>
  </ul> 


    <h4>Practical, often-used choice: <em>Newton / IRLS</em></h4>
  
    <p>Uses curvature (second-order info) for fast convergence on tabular data.</p>
  
    <div style="text-align:center; margin:8px 0;">
      $$ \mathbf{p}=\sigma(\mathbf{X}\boldsymbol{\beta}),\quad
         \nabla \ell(\boldsymbol{\beta})=\mathbf{X}^{\top}(\mathbf{y}-\mathbf{p}),\quad
         \nabla^2\ell(\boldsymbol{\beta})=-\mathbf{X}^{\top}\mathbf{W}\mathbf{X},\;
         \mathbf{W}=\mathrm{diag}\!\big(p_i(1-p_i)\big). $$
      $$ \text{Newton update:}\qquad
         \boldsymbol{\beta}_{\text{new}}
         = \boldsymbol{\beta} - \big(\nabla^2\ell(\boldsymbol{\beta})\big)^{-1}\nabla\ell(\boldsymbol{\beta})
         = \boldsymbol{\beta} + (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^{\top}(\mathbf{y}-\mathbf{p}). $$
      $$ \text{IRLS form (solve WLS each iteration):}\qquad
         \tilde{\mathbf{z}}=\mathbf{X}\boldsymbol{\beta}+\mathbf{W}^{-1}(\mathbf{y}-\mathbf{p}),\quad
         (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})\,\boldsymbol{\beta}_{\text{new}}=\mathbf{X}^{\top}\mathbf{W}\,\tilde{\mathbf{z}}. $$
    </div>
  
    <pre>Initialize β ← 0
repeat
  p ← σ(X β)
  W ← diag(p ∘ (1 − p))
  z ← Xβ + W⁻¹ (y − p)
  Solve: (Xᵀ W X) β_new = Xᵀ W z
  β ← β_new
until convergence</pre>
  
    <ul style="margin:6px 0 0 1rem;">
      <li><strong>Pros:</strong> few iterations, reliable near optimum.</li>
      <li><strong>When:</strong> small–medium number of features; standard choice in many GLM implementations.</li>
      <li><strong>Tip:</strong> standardize features; optionally add L2 regularization for stability.</li>
    </ul>

  <p><em>Note:</em> Gradient Descent is simpler but usually slower and needs step-size tuning; 
    Newton/IRLS (or quasi-Newton like L-BFGS) is typically preferred for logistic regression on tabular data.</p>
-->

    <!-- 6. Example -->
  <!-- 
    <h4>8. Real Example: Fitting \( \boldsymbol{\beta} \) on the Student Data</h4>
  
    <p>Imagine predicting if a student is admitted based on GPA and test score.</p>
    <table aria-label="Example dataset for admission">
      <thead>
        <tr><th>GPA</th><th>Test Score</th><th>Admitted (y)</th></tr>
      </thead>
      <tbody>
        <tr><td>3.0</td><td>600</td><td>0</td></tr>
        <tr><td>3.8</td><td>700</td><td>1</td></tr>
        <tr><td>4.0</td><td>720</td><td>1</td></tr>
        <tr><td>2.8</td><td>580</td><td>0</td></tr>
      </tbody>
    </table>

    <p class="eq">
      $$ P(\text{Admit}=1) \;=\; \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{GPA} + \beta_2 \cdot \text{TestScore})}} $$
    </p>

  -->
  <!--
    <div class="tip">
      If for a new student (GPA = 3.6, Score = 680) we calculate \(p = 0.72\): that’s a 72% chance of admission → predict <strong>Admitted = 1</strong>.
    </div>
  -->

  <!--
  <div class="card" style="background:#fff7ed; border:1px solid #fed7aa; border-radius:12px; padding:12px 14px; margin:12px 0;">
    <p><strong>Why regularize?</strong> With only four points, the classes are (nearly) perfectly separable, so the unregularized MLE can diverge (coefficients \(\to\pm\infty\)). To get a stable, finite solution we add a small L2 penalty and standardize features.</p>
  </div>

  <h5>Step 1 — Standardize features (z-scores)</h5>
  <p>Compute z-scores for GPA and Test Score so each has mean 0 and standard deviation 1:</p>
  <div class="eq" style="text-align:center; margin:8px 0;">
    $$ \tilde{\text{GPA}}=\frac{\text{GPA}-\bar{\text{GPA}}}{s_{\text{GPA}}}, \qquad
       \tilde{\text{Score}}=\frac{\text{Score}-\bar{\text{Score}}}{s_{\text{Score}}}. $$
  </div>
  <p class="small" style="color:#6b7280; margin-top:6px;">
    For this tiny dataset, a reasonable summary is \(\bar{\text{GPA}}\approx3.40\), \(s_{\text{GPA}}\approx0.59\); \(\bar{\text{Score}}\approx650\), \(s_{\text{Score}}\approx70.2\).
  </p>

  <h5>Step 2 — Fit a regularized logistic regression</h5>
  <p>Use L2 (ridge) regularization with a small strength (e.g., \(\lambda=1\), not penalizing the intercept). In terms of standardized features:</p>
  <div class="eq" style="text-align:center; margin:8px 0;">
    $$ p=\sigma\!\big(\beta_0 + \beta_1\,\tilde{\text{GPA}} + \beta_2\,\tilde{\text{Score}}\big). $$
  </div>

  <div class="card" style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px 14px; margin:12px 0;">
    <p style="margin:0 0 6px;"><strong>Illustrative fitted coefficients (standardized features, small L2):</strong></p>
    <table aria-label="Estimated coefficients" style="width:100%; border-collapse:collapse;">
      <thead>
        <tr>
          <th style="text-align:left; border-bottom:1px solid #e5e7eb; padding:6px;">Coefficient</th>
          <th style="text-align:left; border-bottom:1px solid #e5e7eb; padding:6px;">Estimate (≈)</th>
          <th style="text-align:left; border-bottom:1px solid #e5e7eb; padding:6px;">Odds ratio per +1&nbsp;SD</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="padding:6px;">\( \beta_0 \) (intercept)</td>
          <td style="padding:6px;">\(\;\;0.00\)</td>
          <td style="padding:6px;">—</td>
        </tr>
        <tr>
          <td style="padding:6px;">\( \beta_1 \) (GPA, z-score)</td>
          <td style="padding:6px;">\(\;0.73\)</td>
          <td style="padding:6px;">\(e^{0.73}\approx 2.08\)</td>
        </tr>
        <tr>
          <td style="padding:6px;">\( \beta_2 \) (Score, z-score)</td>
          <td style="padding:6px;">\(\;0.74\)</td>
          <td style="padding:6px;">\(e^{0.74}\approx 2.10\)</td>
        </tr>
      </tbody>
    </table>
    <p class="small" style="color:#6b7280; margin-top:8px;">Values depend on the exact regularization strength and tiny-sample quirks; standardization makes coefficients comparable.</p>
  </div>

  <h5>Step 3 — Predict a new student</h5>
  <p>For GPA = 3.6 and Score = 680:</p>
  <div class="eq" style="text-align:center; margin:8px 0;">
    $$ \tilde{\text{GPA}} \approx \frac{3.6-3.4}{0.59}\approx 0.34, \qquad
       \tilde{\text{Score}} \approx \frac{680-650}{70.2}\approx 0.43. $$
    $$ z \approx 0.00 + 0.73(0.34) + 0.74(0.43) \approx 0.56, \qquad
       p=\sigma(z)\approx \frac{1}{1+e^{-0.56}}\approx 0.64. $$
  </div>
  <div class="tip" style="background:#ecfdf5; border:1px solid #a7f3d0; border-radius:10px; padding:10px 12px; margin:10px 0;">
    With a 0.5 threshold, we predict <strong>Admitted = 1</strong> (about 64% probability). Thresholds can be tuned for precision/recall trade-offs.
  </div>

  <details style="margin-top:8px;">
    <summary style="cursor:pointer;">Show coefficients on <em>raw</em> features (illustrative)</summary>
    <p style="margin:8px 0 0;">Using raw GPA and Score with L2 \((\lambda=1)\) can yield a numerically different but equivalent model after scaling (example values):</p>
    <div class="eq" style="text-align:center; margin:8px 0;">
      $$ p=\sigma\!\big(\beta_0 + \beta_1\,\text{GPA} + \beta_2\,\text{Score}\big), \quad
         \beta_0\approx -87.16,\; \beta_1\approx 0.0011,\; \beta_2\approx 0.134. $$
    </div>
    <p class="small" style="color:#6b7280;">Different scaling changes the numerical size of coefficients but not the predicted probabilities for the same data.</p>
  </details>

  <div class="card" style="background:#f9fafb; border:1px solid #e5e7eb; border-radius:12px; padding:12px 14px; margin:12px 0;">
    <strong>Takeaways:</strong>
    <ul style="margin:6px 0 0 1rem;">
      <li>Use <strong>regularization</strong> (e.g., L2) on tiny or separable datasets to obtain finite, stable estimates.</li>
      <li><strong>Standardize</strong> features to make coefficients comparable and optimization smoother.</li>
      <li>Interpret \(e^{\beta_j}\) as the <strong>odds ratio</strong> per 1 SD (standardized) or per 1 unit (raw) increase in that feature.</li>
    </ul>
  </div>
    -->

   <h4>5. Evaluation</h4>

  <div class="two-col" style="margin-top:10px">
<div class="box">
<div class="mini"><strong>Common Metrics</strong></div>
<ul style="margin:8px 0 0 18px">
<li>Accuracy: Accuracy measures the percentage of predictions that were correct.
  <div style='text-align:center; margin-top:4px;'>$$ \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} $$</div></li>

<li>Precision (PPV): Precision measures how many of your positive predictions were actually correct. 
<div style='text-align:center; margin-top:4px;'>$$ \text{Precision} = \frac{TP}{TP + FP} = \frac{\text{Correct Positive Predictions}}{\text{All Positive Predictions}} $$</div></li>
<li>Recall (TPR / Sensitivity): Recall measures how many of the actual positive cases you successfully identified
  <div style='text-align:center; margin-top:4px;'>$$ \text{Recall} = \frac{TP}{TP + FN} = \frac{\text{Correct Positive Predictions}}{\text{All Actual Positives}}$$</div></li>

<li>Specificity (TNR): Specificity measures how many of the actual negative cases you successfully identified.
  <div style='text-align:center; margin-top:4px;'> $$ \text{Specificity} = \frac{TN}{TN + FP}  = \frac{\text{Correct Negative Predictions}}{\text{All Actual Negatives}}$$</div></li>
<li>F1 Score: The F1 score is the harmonic mean of Precision and Recall, combining both into a single number.
  It is useful when you want to balance precision and recall.
  <div style='text-align:center; margin-top:4px;'>$$ F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $$</div></li>
</ul>
</div>


    <h4>Confusion Matrix</h4>

  A confusion matrix is a simple table that shows how well a classification model’s 
  predictions match the actual outcomes. It breaks predictions into four categories:
    
    <ul>
      <li><b>True Positive (TP)</b>: Model predicted Positive, and it was actually Positive.</li>
      <li><b>False Positive (FP)</b>: Model predicted Positive, but it was actually Negative.</li>
      <li><b>True Negative (TN)</b>: Model predicted Negative, and it was actually Negative.</li>
      <li><b>False Negative (FN)</b>: Model predicted Negative, but it was actually Positive.</li>
    </ul>
    <br>
    <center>
            <table style="border: 1px solid">
                <tr>
                    <th></th>
                    <th>Predicted Positive</th>
                    <th>Predicted Negative</th>
                </tr>
                <tr style="border: 1px solid">
                    <td>Actual Positive</td>
                    <td><strong>TP</strong> (True Positive)</td>
                    <td><strong>FN</strong> (False Negative)</td>
                </tr>
                <tr style="border: 1px solid">
                    <td>Actual Negative</td>
                    <td><strong>FP</strong> (False Positive)</td>
                    <td><strong>TN</strong> (True Negative)</td>
                </tr>
            </table>
    </center>
    
  <br>
  <!--
  <h4>From Sigmoid (Logistic) Function to Log-Odds</h4>
  <p>We start from the sigmoid and derive the log-odds step by step.</p>

  <ol style="padding-left:1.2rem;">
    <li>
      <strong>Start with the sigmoid function</strong>
      <div style="text-align:center; margin:6px 0;">
        $$ p \;=\; \sigma(z) \;=\; \frac{1}{1 + e^{-z}} $$
      </div>
    </li>

    <li>
      <strong>Invert the fraction</strong> — take reciprocals of both sides:
      <div style="text-align:center; margin:6px 0;">
        $$ \frac{1}{p} \;=\; 1 + e^{-z} $$
      </div>
    </li>

    <li>
      <strong>Subtract 1 from both sides</strong> and simplify:
      <div style="text-align:center; margin:6px 0;">
        $$ \frac{1}{p} - 1 \;=\; e^{-z}
        \quad\Longrightarrow\quad
        \frac{1-p}{p} \;=\; e^{-z} $$
      </div>
    </li>

    <li>
      <strong>Take natural log</strong>:
      <div style="text-align:center; margin:6px 0;">
        $$ \ln\!\left(\frac{1-p}{p}\right) \;=\; -z $$
      </div>
    </li>

    <li>
      <strong>Rearrange</strong> — multiply by \(-1\):
      <div style="text-align:center; margin:6px 0;">
        $$ \ln\!\left(\frac{p}{1-p}\right) \;=\; z $$
      </div>
    </li>
  </ol>

  <div style="border:1px solid #e5e7eb; background:#f9fafb; padding:12px 14px; border-radius:10px; margin-top:10px;">
    <strong>Final Result (Log-Odds / Logit)</strong>
    <div style="text-align:center; margin-top:6px;">
      $$ \text{Log-Odds} \;=\; \ln\!\left(\frac{p}{1-p}\right) \;=\; z \;=\; \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$
    </div>
  </div>

  <br>
  <hr>
  -->
  

  <!-- 7. Key Points -->
    <h4>6. Key Points</h4>
    <ul>
      <li>Logistic Regression outputs <strong>probabilities</strong> in \([0,1]\), not raw continuous values.</li>
      <li>A threshold (commonly 0.5) converts probability → class.</li>
      <li>Uses <strong>MLE</strong>, not Ordinary Least Squares.</li>
      <li>Linear decision boundary in feature space (after the logit transform).</li>
      <li>No closed-form solution; solved by iterative optimization.</li>
      <li>Works best when the relationship between features and <em>log-odds</em> is approximately linear.</li>
      <li>We can also add L1 (Lasso) or L2 (Ridge) penalty terms to logistic regression, which help prevent overfitting and improve generalization by shrinking or selecting coefficients</li>
    </ul>

</section>

  
<section id="knn">

  <h4>2.2 K-Nearest Neighbors (KNN)</h4>
  <br>

  <b>Idea:</b>
  <p>
    k-Nearest Neighbors is a <b>non-parametric, instance-based</b> method. 
    <br><br>
    <b>Non-parametric</b> means it doesn't assume any fixed form for the decision boundary and doesn't learn a fixed set of parameters during training.
    <br>
    <b>Instance-based</b> means it stores all the training data and makes predictions by directly comparing new points to those stored examples.
    <br>
    k is a <b>hyperparameter</b> that controls how many neighbors to consider, and it's typically tuned using techniques like grid search with cross-validation to find the optimal value.
    <br>
    <br>
    To make a prediction for a new point,
    it looks at the <b>k closest training examples</b> and combines their labels/values:
    <font color="red">majority vote</font> for classification, <font color="red">average</font> for regression.
    The goal is to <b><font color="red">use local neighborhoods to predict</font></b> rather than fitting a global line or plane.
  </p>

  <p>Given a new input \(x\), find its <b>\(k\) nearest training samples</b> under a chosen distance metric and aggregate their targets.</p>
  <p class="eq" style="text-align:center;">
    Classification: \(\;\hat{y}(x) = \operatorname{mode}\big(\{y_{(1)},\dots,y_{(k)}\}\big)\)
  </p>
  <ul>
    <li>\(y_{(i)}\) denotes the label/target of the \(i\)-th nearest neighbor to \(x\).</li>
    <li><b>mode</b>: The most frequently occurring class among the k nearest neighbors</li>
  </ul>
  
  <br>
  
  <b>Distance Metrics:</b>
  <p>Common choices for numeric features:</p>
  <div class="eq" style="text-align:center;">
    Minkowski (order \(p\)): 
    $$ d_p(x,x')=\Big(\sum_{j=1}^{n} |x_j - x'_j|^{p}\Big)^{1/p} $$
  </div>
  <ul>
    <li>\(p=2\): Euclidean, $$ d_2(x,x') = \sqrt{ \sum_{j=1}^{n} (x_j - x'_j)^2 } $$ </li>
    <li>\(p=1\): Manhattan, $$ d_1(x,x') = \sum_{j=1}^{n} |x_j-x'_j| $$ </li>
    <li>\(p\to\infty\): Chebyshev, $$ d_\infty(x,x')=\max_j |x_j-x'_j| $$ </li>
  </ul>
  <p>
    <strong>Important:</strong> Scale/standardize features for distance to make sense; otherwise features with large units dominate.
  </p>
  
  <p><strong>Where:</strong></p>
  <ul>
    <li><em>k</em> = number of neighbors (hyperparameter)</li>
    <li>Distance metric (Euclidean by default)</li>
    <li>Weights (uniform or distance-weighted)</li>
    <ul>
        <li>Uniform: All k neighbors get equal votes</li>
        <li>Distance-weighted: Closer neighbors have more influence (often inverse distance)</li>
    </ul>
  </ul>
  

  <br>

  <h4>Interpreting the Parameters</h4>
  <ul>
    <li><strong>k:</strong> Small k → low bias, high variance (sensitive/noisy). Large k → higher bias, smoother predictions.</li>
    <li><strong>Distance weighting:</strong> Nearer points get more influence (e.g., \(w=1/(d+\varepsilon)\)).</li>
    <li><strong>Feature scaling:</strong> <b>Standardize</b> features; otherwise large-scale features dominate distance.</li>
  </ul>
  <br>

  <div class="wrap">
  <h4>k-Nearest Neighbors (KNN) — Interactive Figure</h4>
  <p class="muted">Move the slider to change <em>k</em>. The dashed circle shows the distance to the \(k\)-th nearest neighbor around the new data \( x^* \).</p>

  <div class="card row">
    <div class="legend">
      <span class="key"><span class="dot a"></span><span>Class A</span></span>
      <span class="key"><span class="dot b"></span><span>Class B</span></span>
      <span class="key"><span class="dot q"></span><span>New Data \( x^* \)</span></span>
      <span class="slider">
        <label for="k">k:</label>
        <input id="k" type="range" min="1" max="11" step="2" value="5" />
        <span id="kval" class="pill"><span>k</span><b>5</b></span>
      </span>
    </div>
    <div id="pred" class="predict pill">Predicted: <b id="predClass">—</b></div>
  </div>

  <!-- Plot (600x420 viewBox; coordinates use pixels as data units) -->
  <svg id="plot" viewBox="0 0 600 420" aria-label="KNN figure">
    <!-- Grid -->
    <g stroke-linecap="round">
      <line class="axis" x1="40" y1="380" x2="580" y2="380"/>
      <line class="axis" x1="40" y1="40"  x2="40"  y2="380"/>
      <!-- vertical grid -->
      <g class="grid">
        <line x1="120" y1="40" x2="120" y2="380"/>
        <line x1="200" y1="40" x2="200" y2="380"/>
        <line x1="280" y1="40" x2="280" y2="380"/>
        <line x1="360" y1="40" x2="360" y2="380"/>
        <line x1="440" y1="40" x2="440" y2="380"/>
        <line x1="520" y1="40" x2="520" y2="380"/>
      </g>
      <!-- horizontal grid -->
      <g class="grid">
        <line x1="40" y1="320" x2="580" y2="320"/>
        <line x1="40" y1="260" x2="580" y2="260"/>
        <line x1="40" y1="200" x2="580" y2="200"/>
        <line x1="40" y1="140" x2="580" y2="140"/>
        <line x1="40" y1="80"  x2="580" y2="80"/>
      </g>
    </g>

    <!-- Neighborhood halo (updated by JS) -->
    <circle id="halo" class="halo" cx="300" cy="200" r="1"></circle>

    <!-- Query point -->
    <g id="queryGroup">
      <circle class="query" cx="300" cy="200" r="6"></circle>
      <text class="label" x="310" y="194">New Data \( x^* \)</text>
    </g>

    <!-- Points container -->
    <g id="points"></g>
  </svg>

  <p class="notice">Note: The dashed circle radius equals the distance to the \(k\)-th nearest neighbor. Highlighted points are the \(k\) neighbors; majority vote determines the predicted class.</p>
</div>

<script>
/* --- Data: two classes around different regions --- */
const pts = [
  // Class A (cyan)
  {x:100,y:320,c:'A'},{x:120,y:280,c:'A'},{x:150,y:300,c:'A'},{x:160,y:250,c:'A'},
  {x:190,y:290,c:'A'},{x:130,y:230,c:'A'},{x:170,y:210,c:'A'},{x:210,y:260,c:'A'},
  // a few spread
  {x:250,y:230,c:'A'},{x:220,y:190,c:'A'},

  // Class B (violet)
  {x:480,y:120,c:'B'},{x:450,y:90,c:'B'},{x:520,y:160,c:'B'},{x:500,y:200,c:'B'},
  {x:430,y:150,c:'B'},{x:460,y:210,c:'B'},{x:520,y:240,c:'B'},
  // a couple closer to center
  {x:360,y:180,c:'B'},{x:380,y:220,c:'B'}
];

// Query point (center-ish)
const query = {x:300, y:200};
const kSlider = document.getElementById('k');
const kVal = document.getElementById('kval').querySelector('b');
const predClassEl = document.getElementById('predClass');
const pointsG = document.getElementById('points');
const halo = document.getElementById('halo');

/* --- Draw all points --- */
function renderPoints(nearestIdx = new Set()){
  pointsG.innerHTML = '';
  pts.forEach((p,i)=>{
    const g = document.createElementNS('http://www.w3.org/2000/svg','g');
    const dot = document.createElementNS('http://www.w3.org/2000/svg','circle');
    dot.setAttribute('cx', p.x);
    dot.setAttribute('cy', p.y);
    dot.setAttribute('r', 6);
    dot.setAttribute('class', `pt ${p.c==='A'?'a':'b'} ${nearestIdx.has(i)?'nn':''}`);
    // outline nearest with white halo under
    if(nearestIdx.has(i)){
      const halo = document.createElementNS('http://www.w3.org/2000/svg','circle');
      halo.setAttribute('cx', p.x); halo.setAttribute('cy', p.y);
      halo.setAttribute('r', 9);
      halo.setAttribute('fill','none');
      halo.setAttribute('stroke','#ffffff');
      halo.setAttribute('stroke-opacity','.85');
      halo.setAttribute('stroke-width','2');
      g.appendChild(halo);
    }
    g.appendChild(dot);
    pointsG.appendChild(g);
  });
}

/* --- Compute k-nearest neighbors and update plot --- */
function update(){
  const k = parseInt(kSlider.value,10);
  kVal.textContent = k;

  // distances
  const withD = pts.map((p,idx)=>({
    idx,
    d2: (p.x - query.x)**2 + (p.y - query.y)**2
  })).sort((a,b)=>a.d2-b.d2);

  const nn = withD.slice(0, k);
  const idxSet = new Set(nn.map(o=>o.idx));
  renderPoints(idxSet);

  // circle radius = distance to kth neighbor
  const r = Math.sqrt(nn[nn.length-1].d2);
  halo.setAttribute('cx', query.x);
  halo.setAttribute('cy', query.y);
  halo.setAttribute('r', r);

  // majority vote
  let a=0,b=0;
  nn.forEach(o => pts[o.idx].c==='A' ? a++ : b++);
  const pred = (a>=b) ? 'Class A' : 'Class B';
  predClassEl.textContent = `${pred} (vote ${a}:${b})`;
  predClassEl.style.color = (a>=b) ? 'var(--cA)' : 'var(--cB)';
}

kSlider.addEventListener('input', update);
renderPoints(new Set()); // initial render
update();
</script>
  
  <hr>

  <h4>Key Points</h4>
  <ul>
    <li><b>Non-parametric, lazy:</b> No training fit; predicts via local neighborhoods.</li>
    <li><b>Hyperparameters:</b> k, distance metric, and weighting—choose via cross-validation.</li>
    <li><b>Feature scaling is essential.</b> Consider standardization or normalization.</li>
    <li><b>Curses in high-D:</b> Distances become less informative as dimension grows (curse of dimensionality).</li>
  </ul>

</section>


  <section id="svm">

  <h4>2.3 Support Vector Machines (SVM)</h4>
  <br>

  <b>Idea:</b>
  <p>
    Support Vector Machines are margin-based classifiers. They find a decision boundary that <b>maximizes the margin</b>
    (distance) between classes while allowing some misclassifications when needed. The goal is to
    <b><font color="red">separate classes with the widest possible margin</font></b> (for robustness), and optionally use
    <em>kernels</em> to separate data that are not linearly separable.
  </p>

  <p>Support Vector Machine (SVM) is a powerful classification algorithm that:</p>
  <ul>
    <li>Finds the <strong><font color="red">hyperplane</font></strong> (decision boundary) that best separates classes.</li>
    <li>Maximizes the <strong><font color="red">margin</font></strong> between classes — the distance from the hyperplane to the nearest data points.</li>
    <li>The nearest data points are called <strong><font color="red">support vectors</font></strong> — they "support" or define the decision boundary.</li>
    <li>Can handle non-linear classification using the <strong><font color="red">kernel trick</font></strong>.</li>
  </ul>
  <br>
    
  <h4>Mathematical Equation:</h4>

  <h4>Linear SVM (Linearly Separable Case)</h4>

     <p>For a binary classification problem with labels \( y_i \in \{-1, +1\} \) and feature vectors \( \mathbf{x}_i \), the decision boundary is a hyperplane:</p>
  
  <p class="eq">
    $$ \mathbf{w}^T \mathbf{x} + b = 0 $$
  </p>

  <p>where \( \mathbf{w} \) is the weight vector (normal to the hyperplane) and \( b \) is the bias term.</p>

  <div class="card">
    <p><strong>Classification Rule:</strong></p>
    <p class="eq">
      $$ \text{Predict } y = \begin{cases} 
      +1 & \text{if } \mathbf{w}^T \mathbf{x} + b \geq 0 \\
      -1 & \text{if } \mathbf{w}^T \mathbf{x} + b < 0
      \end{cases} $$
    </p>
  </div>

    <br>
  <p style="text-align:center;">
    Decision score: \(\; f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b \;\) &nbsp;&nbsp;&nbsp;
    Prediction: \(\; \hat{y} = \mathrm{sign}\!\big(f(\mathbf{x})\big)\).
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(\mathbf{w}\) = weight vector; \(b\) = bias (intercept)</li>
    <li>\(\mathbf{x} = (x_1,\ldots,x_n)\) = feature vector</li>
    <li>\(\hat{y} \in \{-1,+1\}\) = predicted class label</li>
    <li>For non-linear SVM, replace inner products \(\mathbf{x}_i^\top \mathbf{x}_j\) with a kernel \(K(\mathbf{x}_i,\mathbf{x}_j)\)</li>
  </ul>

  <br>

  <center>
    <svg viewBox="0 0 800 400" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .grid { stroke: #1f2937; stroke-width: 0.5; opacity: 0.3; }
      .point-a { fill: #22d3ee; stroke: #0891b2; stroke-width: 1.5; }
      .point-b { fill: #a78bfa; stroke: #7c3aed; stroke-width: 1.5; }
      .support-ring { stroke: #fbbf24; stroke-width: 3; fill: none; }
      .margin-line { stroke: #f97316; stroke-width: 2; stroke-dasharray: 8,4; opacity: 0.9; }
      .decision-boundary { stroke: #10b981; stroke-width: 3; }
      .margin-region { fill: #10b981; opacity: 0.1; }
      .label-text { fill: #e5e7eb; font-size: 14px; font-family: 'Roboto', Arial, sans-serif; font-weight: 500; }
      .title-text { fill: #ff79c6; font-size: 18px; font-family: 'Roboto', Arial, sans-serif; font-weight: 700; }
      .annotation { fill: #fbbf24; font-size: 13px; font-family: 'Roboto', Arial, sans-serif; font-weight: 600; }
      .legend-ring { stroke: #fbbf24; stroke-width: 3; fill: none; }
      .tick { stroke: #fbbf24; stroke-width: 2; }
    </style>
  </defs>

  <rect width="800" height="400" fill="#1a1d1f"/>

  <!-- Margin region + lines + boundary -->
  <polygon class="margin-region" points="0,322.5 800,22.5 800,82.5 0,382.5" />
  <line class="margin-line" x1="0" y1="322.5" x2="800" y2="22.5"/>   <!-- +1 -->
  <line class="margin-line" x1="0" y1="382.5" x2="800" y2="82.5"/>   <!-- -1 -->
  <line class="decision-boundary" x1="0" y1="352.5" x2="800" y2="52.5"/> <!-- 0 -->

  <!-- Class points -->
  <circle class="point-a" cx="120" cy="360" r="7"/>
  <circle class="point-a" cx="180" cy="350" r="7"/>
  <circle class="point-a" cx="150" cy="340" r="7"/>
  <circle class="point-a" cx="220" cy="345" r="7"/>
  <circle class="point-a" cx="200" cy="330" r="7"/>
  <circle class="point-a" cx="260" cy="340" r="7"/>
  <circle class="point-a" cx="240" cy="320" r="7"/>

  <circle class="point-b" cx="520" cy="140" r="7"/>
  <circle class="point-b" cx="580" cy="120" r="7"/>
  <circle class="point-b" cx="550" cy="110" r="7"/>
  <circle class="point-b" cx="620" cy="90"  r="7"/>
  <circle class="point-b" cx="600" cy="80"  r="7"/>
  <circle class="point-b" cx="660" cy="70"  r="7"/>
  <circle class="point-b" cx="640" cy="85"  r="7"/>

  <!-- Support Vectors (+1 side should be Class B / purple) -->
  <circle class="point-b" cx="280" cy="217.5" r="7"/>
  <circle class="point-b" cx="320" cy="202.5" r="7"/>
  <circle class="support-ring" cx="280" cy="217.5" r="11"/>
  <circle class="support-ring" cx="320" cy="202.5" r="11"/>

  <!-- Support Vectors (−1 side should be Class A / cyan) -->
  <circle class="point-a" cx="480" cy="202.5" r="7"/>
  <circle class="point-a" cx="440" cy="217.5" r="7"/>
  <circle class="support-ring" cx="480" cy="202.5" r="11"/>
  <circle class="support-ring" cx="440" cy="217.5" r="11"/>

  <!-- Margin = 1/||w|| bar -->
  <g>
    <line x1="400" y1="202.5" x2="390.1" y2="176.2" stroke="#fbbf24" stroke-width="2"/>
    <line class="tick" x1="394" y1="204.8" x2="406" y2="200.2"/>
    <line class="tick" x1="384" y1="178.4" x2="396" y2="174"/>
    <text class="annotation" x="370" y="162">margin = 1 / ||w||</text>
  </g>

  <!-- Total margin bar -->
  <g>
    <line x1="140.137" y1="269.949" x2="159.863" y2="322.551" stroke="#fbbf24" stroke-width="2"/>
    <line class="tick" x1="134.519" y1="272.055" x2="145.755" y2="267.842"/>
    <line class="tick" x1="154.245" y1="324.658" x2="165.481" y2="320.445"/>
    <text class="annotation" x="110" y="260">total margin = 2 / ||w||</text>
  </g>

  <!-- Labels -->
  <text class="title-text" x="250" y="30">SVM: Maximum-Margin Separator</text>
  <text class="label-text" x="95"  y="380" fill="#22d3ee">Class A</text>
  <text class="label-text" x="650" y="60"  fill="#a78bfa">Class B</text>
  <text class="annotation" x="210" y="290" font-size="12">w·x + b >= +1</text>
  <text class="annotation" x="565" y="70"  font-size="12">w·x + b <= -1</text>

  <!-- Decision boundary -->
  <text class="label-text" x="360" y="214" font-size="12" fill="#10b981">w·x + b = 0</text>

  <!-- Legend -->
  <circle class="legend-ring" cx="580" cy="365" r="11"/>
  <text class="label-text" x="595" y="370" font-size="12">Support Vectors (yellow ring)</text>
</svg>

  </center>

  <br>

  <h4>Margin and Support Vectors</h4>

    <p>The <strong>margin</strong> is the distance from the hyperplane to the nearest data points. The margin width is:</p>
  
  <p class="eq">
    $$ \text{Margin} = \frac{2}{\|\mathbf{w}\|} $$
  </p>

  <p>To maximize the margin, we minimize \( \|\mathbf{w}\| \) (or equivalently, minimize \( \frac{1}{2}\|\mathbf{w}\|^2 \)).</p>

  <h4>The Objective Function:</h4>

  <p>For linearly separable data, SVM solves:</p>

  <div class="card">
    <p class="eq">
      $$ \min_{\mathbf{w}, b} \quad \frac{1}{2}\|\mathbf{w}\|^2 $$
    </p>
    <p class="eq">
      $$ \text{subject to} \quad y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \text{for all } i $$
    </p>
    <p class="small center">The constraint ensures all points are correctly classified with at least margin 1.</p>
  </div>

    <p>
  <strong>Translation:</strong> "Find the values of 
  <strong>w</strong> (weights) and <strong>b</strong> (bias) that 
  <strong>minimize</strong> \( \frac{1}{2}\|\mathbf{w}\|^2 \)"
  </p>

  <br>

    

  <br>

  <!--
  <hr>
  <h4>Worked Example: “Sold in 30 days” (Binary)</h4>
  <p>We reuse the small dataset; set \(y = -1\) for “No”, \(y = +1\) for “Yes”.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>-1</td></tr>
    <tr><td>1500</td><td>3</td><td>-1</td></tr>
    <tr><td>2000</td><td>3</td><td>+1</td></tr>
    <tr><td>2500</td><td>4</td><td>+1</td></tr>
    <tr><td>3000</td><td>4</td><td>+1</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1000 & 2 \\
      1500 & 3 \\
      2000 & 3 \\
      2500 & 4 \\
      3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      -1 \\ -1 \\ +1 \\ +1 \\ +1
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Choose model &amp; hyperparameters</h4>
  <ul>
    <li>Scale features (recommended).</li>
    <li>Linear SVM with \(C=1\) (baseline). Try RBF kernel with \(\gamma\) tuned via cross-validation if the boundary is curved.</li>
  </ul>

  <h4>Step 3: Fit</h4>
  <p>
    Train the SVM on \((\mathbf{X},\mathbf{y})\). The support vectors will be the examples closest to the boundary (in this toy set, likely near the transition around 2000–2500 sq.ft).
  </p>

  <h4>Step 4: Predict</h4>
  <p>
    For a new house \(\mathbf{x}_q=(2200,3)\), compute \(f(\mathbf{x}_q)\). If \(f(\mathbf{x}_q)&gt;0\) predict <b>+1 (Yes)</b>, else <b>-1 (No)</b>.
  </p>
  
  <h4>Step 5: Evaluation</h4>
  <p>
    Report accuracy, precision/recall, F1, ROC-AUC. For probability-like outputs, calibrate scores (e.g., Platt scaling) after training.
  </p>
  -->
    
  <h4>Hyperparameters</h4>
    
  <p><strong><var>C</var></strong> (both kernels): Regularization.</p>
  <ul>
    <li>
      Small <strong><var>C</var></strong> &rarr; wider margin, more misclassifications allowed
      (less overfit).
    </li>
    <li>
      Large <strong><var>C</var></strong> &rarr; narrow margin, fewer training errors
      (risk overfit).
    </li>
  </ul>

    <br>
    
     <p><strong><var>kernel</var></strong>: Determines the decision boundary shape.</p>
  <ul>
    <li>
      <strong>linear</strong> &rarr; straight decision boundary (best for linearly separable data).
    </li>
    <li>
      <strong>rbf</strong> (Radial Basis Function) &rarr; curved/circular decision boundary (handles non-linear patterns).
    </li>
  </ul>
    
  <br>
    
  <h4>Key Points</h4>
  <ul>
    <li><b>Maximum-margin:</b> promotes robustness to small perturbations.</li>
    <li><b>Support vectors only:</b> the decision boundary depends on a subset of training points.</li>
    <li><b>\(C\) controls bias–variance:</b> large \(C\) fits training more tightly; small \(C\) widens the margin.</li>
    <li><b>Kernels:</b> RBF, polynomial, and others enable non-linear boundaries via the kernel trick.</li>
    <li><b>Scaling is essential:</b> SVMs are sensitive to feature scales.</li>
    <li><b>Outputs are margins, not probabilities:</b> calibrate if you need probabilities.</li>
  </ul>

</section>


  <section id="dt">

  <h4>2.4 Decision Tree (Classification)</h4>
  <br>

  <b>Idea:</b>
  <p>
    A Decision Tree predicts a class by asking a sequence of <b>if–then</b> questions on features.
    At each split, it chooses the feature/threshold that best <b><font color="red">separates the classes (reduces impurity)</font></b>.
    The final prediction comes from the class majority at a leaf.
  </p>

  <b>Split Criteria (Impurity):</b>
  <p style="text-align:center;">
    Common impurities for classification:
    <br>
    <b>Gini:</b>
    \( \displaystyle G(S) = 1 - \sum_{c} p_c^2 \)
    &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;
    <b>Entropy:</b>
    \( \displaystyle H(S) = - \sum_{c} p_c \log_2 p_c \)
  </p>
  <p style="text-align:center;">
    <b>Information Gain:</b>
    \( \displaystyle \mathrm{Gain}(S,\text{split})
       = \mathrm{Impurity}(S) - \sum_{v} \frac{|S_v|}{|S|}\,\mathrm{Impurity}(S_v) \)
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\( S \) = current set of training samples at a node; \( S_v \) = subset after a split option \(v\)</li>
    <li>\( p_c \) = fraction of class \(c\) in \(S\)</li>
    <li>Impurity = Gini or Entropy (choose one consistently)</li>
  </ul>

  <br>

  <center>
    <img src="images/tree1.png" width="800px" height="300px" alt="Decision tree splitting space into class-pure regions">
  </center>

  <br>

  <h4>How a Decision Tree Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Collect Data</strong> – Features \((x_1,\ldots,x_n)\) with class labels \(y\).</li>
    <li><strong>Choose a split</strong> – For each feature/threshold, compute impurity reduction; pick the best.</li>
    <li><strong>Recurse</strong> – Split the resulting subsets until a stopping rule (pure leaf, max depth, min samples).</li>
    <li><strong>Predict</strong> – A new sample follows the if–then path; output the leaf’s majority class (optionally class probabilities).</li>
    <li><strong>Prune/Tune</strong> – Limit depth or prune to avoid overfitting; evaluate with accuracy, precision/recall, F1, ROC-AUC.</li>
  </ol>

  <h4>Interpreting the Tree</h4>
  <ul>
    <li><strong>Rules:</strong> Each root-to-leaf path is a human-readable rule (e.g., <em>if Size &lt; 2000 then No</em>).</li>
    <li><strong>Feature importance:</strong> Features used near the root (large impurity drop) are typically more influential.</li>
    <li><strong>Probabilities:</strong> Leaf probabilities = class frequencies in that leaf.</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p>
    A tree consists of <b>internal nodes</b> (tests like \(x_j \le t\)) and <b>leaves</b> (class distributions).
    For a dataset \( \{(\mathbf{x}_i, y_i)\}_{i=1}^m \), a split at node \(N\) chooses feature \(j\) and threshold \(t\)
    that maximize \( \mathrm{Gain}(S_N, (j,t)) \).
  </p>

  <h4>Objective</h4>
  <p>
    Greedy, top-down growth: at each node, pick the split with the <b>largest information gain</b> (or Gini decrease).
    Overfitting is controlled via <b>pre-pruning</b> (max depth, min samples per leaf) or <b>post-pruning</b>
    (e.g., cost-complexity pruning with parameter \( \alpha \)).
  </p>

  <br>

  <hr>

  <h4>Worked Example: “Sold in 30 Days”</h4>
  <p>Binary labels: 1 = Yes, 0 = No. Features: Size (sq.ft), Bedrooms.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Sold in 30 days (y)</th></tr>
    <tr><td>1000</td><td>2</td><td>0</td></tr>
    <tr><td>1500</td><td>3</td><td>0</td></tr>
    <tr><td>2000</td><td>3</td><td>1</td></tr>
    <tr><td>2500</td><td>4</td><td>1</td></tr>
    <tr><td>3000</td><td>4</td><td>1</td></tr>
  </table>

  <h4>Step 1: Parent Impurity</h4>
  <p>
    Class counts: 3 ones, 2 zeros ⇒ \(p_1 = 3/5,\; p_0 = 2/5\).
  </p>
  <p>
    Gini:
    \( \displaystyle G(S) = 1 - (3/5)^2 - (2/5)^2 = 1 - 9/25 - 4/25 = 12/25 = 0.48 \).
    <br>
    Entropy:
    \( \displaystyle H(S) = -\tfrac{3}{5}\log_2\tfrac{3}{5} - \tfrac{2}{5}\log_2\tfrac{2}{5} \approx 0.971 \).
  </p>

  <h4>Step 2: Try a Split on Size</h4>
  <p><b>Threshold \( \text{Size} &lt; 2000 \):</b></p>
  <ul>
    <li>Left: \((1000,2,0), (1500,3,0)\) ⇒ all 0 ⇒ Gini = 0</li>
    <li>Right: \((2000,3,1), (2500,4,1), (3000,4,1)\) ⇒ all 1 ⇒ Gini = 0</li>
  </ul>
  <p>
    Weighted child Gini = \( \tfrac{2}{5}\cdot 0 + \tfrac{3}{5}\cdot 0 = 0\).
    <br>
    <b>Information Gain = 0.48 − 0 = 0.48</b> (perfect split).
  </p>

  <p><b>Threshold \( \text{Size} &lt; 2250 \) (for comparison):</b></p>
  <ul>
    <li>Left (3 items): labels 0,0,1 ⇒ \(G = 1 - (2/3)^2 - (1/3)^2 = 4/9 \approx 0.444\)</li>
    <li>Right (2 items): labels 1,1 ⇒ \(G = 0\)</li>
  </ul>
  <p>
    Weighted child Gini \(= \tfrac{3}{5}\cdot 0.444 + \tfrac{2}{5}\cdot 0 \approx 0.267\).
    <br>
    <b>Information Gain ≈ 0.48 − 0.267 = 0.213</b>.
  </p>

  <h4>Step 3: Stopping / Pruning</h4>
  <p>
    For this toy set, the split <em>Size &lt; 2000</em> yields pure leaves, so the tree can stop (depth = 1).
    In larger datasets, use max depth / min samples per leaf or post-pruning (\( \alpha \)) to avoid overfitting.
  </p>

  <h4>Final Tree (Rules)</h4>
  <p style="text-align:center; font-size:1.05em;">
    If \( \text{Size} &lt; 2000 \)\,: predict <b>No</b> (0); &nbsp; else: predict <b>Yes</b> (1).
  </p>

  <h4>Step 4: Predictions &amp; Evaluation</h4>
  <p>
    Apply the rule to training points (all classified correctly here). Report accuracy, precision/recall, F1; for calibrated probabilities, use leaf frequencies.
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Interpretable:</b> human-readable if–then rules.</li>
    <li><b>No scaling needed:</b> handles mixed feature types and non-linear boundaries.</li>
    <li><b>Can overfit:</b> control with depth/leaf constraints or pruning (\( \alpha \)).</li>
    <li><b>Unstable alone:</b> small data changes can alter splits; ensembles (Random Forests, Gradient Boosted Trees) improve stability and accuracy.</li>
  </ul>

</section>

  <section id="nb">

  <h4>2.5 Naive Bayes</h4>
  <br>

  <b>Idea:</b>
  <p>
    Naive Bayes is a fast, probabilistic classifier that applies <b>Bayes’ rule</b> with a strong (naive) assumption:
    <em>features are conditionally independent given the class</em>. It predicts the class with the highest posterior
    probability. The goal is to <b><font color="red">estimate class posteriors from simple per-feature models</font></b>.
  </p>

  <b>Equation (Posterior via Bayes’ Rule):</b>
  <p style="text-align:center;">
    $$ P(y=c \mid \mathbf{x}) \;\propto\; P(y=c)\,\prod_{j=1}^{n} P(x_j \mid y=c). $$
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\(y \in \{1,\ldots,C\}\) is the class label.</li>
    <li>\(\mathbf{x} = (x_1,\ldots,x_n)\) are features.</li>
    <li>\(P(y=c)\) is the class prior; \(P(x_j \mid y=c)\) is the class-conditional likelihood for feature \(j\).</li>
  </ul>

  <br>

  <center>
    <img src="images/nb1.png" width="800" height="300" alt="Naive Bayes: class-wise feature likelihoods multiply into posteriors">
  </center>

  <br>

  <h4>How Naive Bayes Works</h4>

  <!-- Include MathJax once globally on your page if not already included -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <ol>
    <li><strong>Choose a variant</strong> – Common choices:
      <ul>
        <li><b>Gaussian NB</b>: continuous features \(x_j \sim \mathcal{N}(\mu_{jc}, \sigma^2_{jc})\).</li>
        <li><b>Multinomial NB</b>: counts/frequencies (e.g., word counts in text) with Laplace/Lidstone smoothing.</li>
        <li><b>Bernoulli NB</b>: binary features (word present/absent).</li>
      </ul>
    </li>
    <li><strong>Estimate parameters</strong> – Priors \( \hat P(y=c)=n_c/m \). For Gaussian NB:
      \( \hat\mu_{jc}=\tfrac{1}{n_c}\sum_{i:y_i=c} x_{ij} \), 
      \( \hat\sigma^2_{jc}=\tfrac{1}{n_c}\sum_{i:y_i=c}(x_{ij}-\hat\mu_{jc})^2 \).</li>
    <li><strong>Score a new sample</strong> – Compute
      \( \log P(y=c) + \sum_j \log P(x_j \mid y=c) \) (use logs to avoid underflow).</li>
    <li><strong>Predict</strong> – Choose the class with the highest (log) posterior (MAP decision).</li>
    <li><strong>Evaluate</strong> – Accuracy, precision/recall, F1, ROC-AUC; log-loss if you care about probabilities.</li>
  </ol>

  <h4>Interpreting the Parameters</h4>
  <ul>
    <li><strong>Class priors</strong> \(P(y=c)\) reflect base rates of classes.</li>
    <li><strong>Per-class feature models</strong> capture how each feature behaves within a class
      (e.g., Gaussian mean/variance or multinomial word probabilities).</li>
    <li><strong>Smoothing (Multinomial/Bernoulli):</strong> add-\(\alpha\) (e.g., Laplace \(\alpha=1\)) prevents zero probabilities.</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p>
    <b>Gaussian NB likelihood:</b>
    \( \displaystyle P(x_j \mid y=c) = \mathcal{N}(x_j;\,\mu_{jc},\sigma^2_{jc})
      = \frac{1}{\sqrt{2\pi\sigma^2_{jc}}}\exp\!\left(-\frac{(x_j-\mu_{jc})^2}{2\sigma^2_{jc}}\right) \).
  </p>
  <p>
    <b>Multinomial NB (counts):</b>
    \( \displaystyle P(\mathbf{x}\mid y=c)=\frac{(\sum_w x_w)!}{\prod_w x_w!}\prod_w \theta_{wc}^{\,x_w} \),
    with \( \hat\theta_{wc}=\frac{N_{wc}+\alpha}{N_c+\alpha V} \).
  </p>

  <h4>Objective (MAP Rule)</h4>
  <p>
    Predict the class maximizing the posterior:
    $$ \hat y = \arg\max_c \; \log P(y=c) + \sum_{j=1}^{n} \log P(x_j \mid y=c). $$
  </p>

  <br>

  <hr>

  <h4>Worked Example: “Sold in 30 Days” (Gaussian NB)</h4>
  <p>Binary labels: 1 = Yes, 0 = No. Features: Size (sq.ft), Bedrooms.</p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>y</th></tr>
    <tr><td>1000</td><td>2</td><td>0</td></tr>
    <tr><td>1500</td><td>3</td><td>0</td></tr>
    <tr><td>2000</td><td>3</td><td>1</td></tr>
    <tr><td>2500</td><td>4</td><td>1</td></tr>
    <tr><td>3000</td><td>4</td><td>1</td></tr>
  </table>

  <h4>Step 1: Estimate Priors</h4>
  <p>
    \(P(y=0)=2/5=0.4\), &nbsp; \(P(y=1)=3/5=0.6\).
  </p>

  <h4>Step 2: Per-Class Gaussian Parameters</h4>
  <p><b>Class 0 (y=0):</b> points \((1000,2),(1500,3)\)</p>
  <ul>
    <li>Size: \(\mu_{0}=1250\), \(\sigma^2_{0}=62{,}500\).</li>
    <li>Bedrooms: \(\mu_{0}=2.5\), \(\sigma^2_{0}=0.25\).</li>
  </ul>
  <p><b>Class 1 (y=1):</b> points \((2000,3),(2500,4),(3000,4)\)</p>
  <ul>
    <li>Size: \(\mu_{1}=2500\), \(\sigma^2_{1}\approx 166{,}666.67\).</li>
    <li>Bedrooms: \(\mu_{1}\approx 3.6667\), \(\sigma^2_{1}\approx 0.2222\).</li>
  </ul>

  <h4>Step 3: Score a Query \(\mathbf{x}_q=(2200,\,3)\)</h4>
  <p>
    Compute class-conditional densities and multiply with priors (use logs in code).
  </p>
  <p>
    For \(y=0\): \(P(\text{Size}=2200\mid0)\approx 1.16\times10^{-6}\), \(P(\text{Beds}=3\mid0)\approx 0.484\).
    <br>
    Product \(\approx 5.6\times10^{-7}\); multiply prior \(0.4\) ⇒ \(\approx 2.3\times10^{-7}\).
  </p>
  <p>
    For \(y=1\): \(P(\text{Size}=2200\mid1)\approx 7.46\times10^{-4}\), \(P(\text{Beds}=3\mid1)\approx 0.311\).
    <br>
    Product \(\approx 2.32\times10^{-4}\); multiply prior \(0.6\) ⇒ \(\approx 1.39\times10^{-4}\).
  </p>

  <h4>Step 4: Predict</h4>
  <p>
    Compare posteriors: \(1.39\times10^{-4} \gg 2.3\times10^{-7}\) ⇒ predict <b>y = 1 (Yes)</b>.
  </p>

  <h4>Step 5: Evaluation</h4>
  <p>
    Report accuracy, precision/recall, F1; use <em>log-loss</em> for probability quality. NB is strong on small, high-dimensional,
    or text data (with Multinomial NB), but can degrade when features are highly correlated (independence assumption is violated).
  </p>

  <h4>Key Points</h4>
  <ul>
    <li><b>Simple and fast:</b> closed-form parameter estimates; scales well.</li>
    <li><b>Works well with little data:</b> especially Multinomial NB for text.</li>
    <li><b>Sensitive to independence violations:</b> correlated features can bias posteriors.</li>
    <li><b>Smoothing is essential</b> for Multinomial/Bernoulli to avoid zero probabilities.</li>
    <li><b>Use log-space</b> for products of probabilities to avoid underflow.</li>
  </ul>

</section>

  
  

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
