{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Because Colab's environment:\n",
        "#    Gets reset every time the session ends\n",
        "#    Deletes all files when you disconnect\n",
        "#So mounting Drive allows your work to be saved permanently."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFtO0UmgLelC",
        "outputId": "e1efc1a9-276a-4447-f743-da1fdef1e21f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/cifar10_data/\"\n",
        "import os\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "WmekE-IpLgt9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. Imports & Device\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision # a library that contains computer vision tools for PyTorch\n",
        "import torchvision.transforms as transforms # Transforms are functions to prepare images for training\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXiNxnq4lh8",
        "outputId": "331e0454-1f32-4e97-e53a-149c11a5b1d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. CIFAR-10 Dataset & DataLoaders\n",
        "#    CIFAR-10: 32×32 color images, 10 classes\n",
        "# ============================================================\n",
        "\n",
        "# Transform: convert to tensor + normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize each channel: mean & std for CIFAR-10\n",
        "    # Three Values (R, G, B)\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "# Download training & test sets\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=SAVE_DIR,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=SAVE_DIR,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders (you can adjust batch_size)\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Check number of images\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCPVYNoT4pDU",
        "outputId": "6e1893c2-c832-4081-dce9-bba1c0e8bd74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 50000\n",
            "Test dataset size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3. Define CNN (matches the lecture architecture)\n",
        "#  It takes 3×32×32 images and outputs probabilities for 10 classes.\n",
        "\n",
        "#    Input:  3×32×32\n",
        "#    Conv1:  3 -> 16 filters, 5×5, no padding\n",
        "#      Output size: 16×28×28\n",
        "#    Pool1: 2×2 -> 16×14×14\n",
        "#    Conv2: 16 -> 32 filters, 5×5\n",
        "#      Output size: 32×10×10\n",
        "#    Pool2: 2×2 -> 32×5×5\n",
        "#    FC:    32*5*5 -> 10 classes\n",
        "# ============================================================\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Formula: output_size = (input_size - kernel_size + 1) / stride\n",
        "        # in_channels: Takes 3 input channels (RGB)\n",
        "        # out_channels: Creates 16 output filters (feature maps)\n",
        "        # Conv layer 1: (3, 32, 32) -> (16, 28, 28)\n",
        "        # Width:  (32 - 5 + 1) / 1 = 28\n",
        "        # Height: (32 - 5 + 1) / 1 = 28\n",
        "        # Output: (16, 28, 28)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,\n",
        "                               kernel_size=5, stride=1, padding=0)\n",
        "\n",
        "\n",
        "\n",
        "        # in_channels: Takes 16 input channels\n",
        "        # out_channels: Creates 32 output filters (feature maps)\n",
        "        # Conv layer 2: (16, 14, 14) -> (32, 10, 10)\n",
        "        #Width:  (14 - 5 + 1) / 1 = 10\n",
        "        #Height: (14 - 5 + 1) / 1 = 10\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,\n",
        "                               kernel_size=5, stride=1, padding=0)\n",
        "\n",
        "        # Max pooling (2×2) after each conv\n",
        "        # Formula: output_size = input_size / stride\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layer:\n",
        "        # After two pools, image 3×32×32 --> 32×5×5 feature map:\n",
        "        # 3×32×32 --Conv1(5x5)--> 16×28×28 --Pool--> 16×14×14\n",
        "        # 16×14×14 --Conv2(5x5)--> 32×10×10 --Pool--> 32×5×5\n",
        "        self.fc = nn.Linear(32 * 5 * 5, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Conv1 + ReLU + Pool\n",
        "        # Width:  28 / 2 = 14\n",
        "        # Height: 28 / 2 = 14\n",
        "        x = self.pool(torch.relu(self.conv1(x)))   # (16, 14, 14)\n",
        "\n",
        "        # Conv2 + ReLU + Pool\n",
        "        #Width:  10 / 2 = 5\n",
        "        #Height: 10 / 2 = 5\n",
        "        x = self.pool(torch.relu(self.conv2(x)))   # (32, 5, 5)\n",
        "\n",
        "        # Flatten\n",
        "        # Converts 2D feature maps to 1D vector: 32*5*5 = 800 neurons\n",
        "        x = x.view(x.size(0), -1)                  # (32*5*5)\n",
        "        # Fully connected --> logits for 10 classes\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4GQRDJ5O5D",
        "outputId": "015575e5-71e0-4408-d885-f302ce752ff6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=800, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. Loss Function & Optimizer\n",
        "# ============================================================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# For demo: only a few epochs so it runs quickly in class\n",
        "num_epochs = 50\n"
      ],
      "metadata": {
        "id": "GupvEAqt5WBi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5. Training Loop\n",
        "# ============================================================\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # 1. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 4. Backprop\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Stats\n",
        "        batch_size = inputs.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        #scores, predicted = outputs.max(1)\n",
        "        predicted = outputs.argmax(dim=1)\n",
        "        total += batch_size\n",
        "\n",
        "        # How many predictions are correct?\n",
        "        is_correct = (predicted == labels)\n",
        "        num_correct_in_batch = is_correct.sum().item()\n",
        "        running_correct += num_correct_in_batch\n",
        "\n",
        "        # Print a mini status every ~200 batches\n",
        "        if (batch_idx + 1) % 200 == 0:\n",
        "            print(f\"  [Batch {batch_idx+1:3d}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_correct / total\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc*100:.2f}%\")\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "w39hzUjn5X4C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "    train_one_epoch(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ronmft6IBQb4",
        "outputId": "59c9f104-4deb-47c7-e56d-17bd8b311b45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Epoch 1/50 =====\n",
            "  [Batch 200] Loss: 1.5199\n",
            "  [Batch 400] Loss: 1.1879\n",
            "  [Batch 600] Loss: 1.3530\n",
            "Epoch 1 Train Loss: 1.4696 | Train Acc: 47.45%\n",
            "===== Epoch 2/50 =====\n",
            "  [Batch 200] Loss: 1.2057\n",
            "  [Batch 400] Loss: 1.4738\n",
            "  [Batch 600] Loss: 1.0567\n",
            "Epoch 2 Train Loss: 1.1666 | Train Acc: 59.22%\n",
            "===== Epoch 3/50 =====\n",
            "  [Batch 200] Loss: 0.7569\n",
            "  [Batch 400] Loss: 0.9955\n",
            "  [Batch 600] Loss: 0.9997\n",
            "Epoch 3 Train Loss: 1.0617 | Train Acc: 63.18%\n",
            "===== Epoch 4/50 =====\n",
            "  [Batch 200] Loss: 1.1755\n",
            "  [Batch 400] Loss: 1.0142\n",
            "  [Batch 600] Loss: 0.9914\n",
            "Epoch 4 Train Loss: 0.9892 | Train Acc: 65.73%\n",
            "===== Epoch 5/50 =====\n",
            "  [Batch 200] Loss: 0.7400\n",
            "  [Batch 400] Loss: 0.9936\n",
            "  [Batch 600] Loss: 0.7868\n",
            "Epoch 5 Train Loss: 0.9410 | Train Acc: 67.47%\n",
            "===== Epoch 6/50 =====\n",
            "  [Batch 200] Loss: 0.8945\n",
            "  [Batch 400] Loss: 0.7878\n",
            "  [Batch 600] Loss: 0.9196\n",
            "Epoch 6 Train Loss: 0.9037 | Train Acc: 68.75%\n",
            "===== Epoch 7/50 =====\n",
            "  [Batch 200] Loss: 0.9553\n",
            "  [Batch 400] Loss: 0.8824\n",
            "  [Batch 600] Loss: 1.0374\n",
            "Epoch 7 Train Loss: 0.8661 | Train Acc: 70.01%\n",
            "===== Epoch 8/50 =====\n",
            "  [Batch 200] Loss: 0.5301\n",
            "  [Batch 400] Loss: 0.8396\n",
            "  [Batch 600] Loss: 0.8655\n",
            "Epoch 8 Train Loss: 0.8460 | Train Acc: 70.49%\n",
            "===== Epoch 9/50 =====\n",
            "  [Batch 200] Loss: 0.6562\n",
            "  [Batch 400] Loss: 0.7629\n",
            "  [Batch 600] Loss: 1.0042\n",
            "Epoch 9 Train Loss: 0.8229 | Train Acc: 71.47%\n",
            "===== Epoch 10/50 =====\n",
            "  [Batch 200] Loss: 0.6319\n",
            "  [Batch 400] Loss: 0.6107\n",
            "  [Batch 600] Loss: 0.7790\n",
            "Epoch 10 Train Loss: 0.8016 | Train Acc: 72.29%\n",
            "===== Epoch 11/50 =====\n",
            "  [Batch 200] Loss: 0.6817\n",
            "  [Batch 400] Loss: 0.6141\n",
            "  [Batch 600] Loss: 0.6909\n",
            "Epoch 11 Train Loss: 0.7858 | Train Acc: 72.84%\n",
            "===== Epoch 12/50 =====\n",
            "  [Batch 200] Loss: 0.8218\n",
            "  [Batch 400] Loss: 0.6994\n",
            "  [Batch 600] Loss: 0.7658\n",
            "Epoch 12 Train Loss: 0.7713 | Train Acc: 73.34%\n",
            "===== Epoch 13/50 =====\n",
            "  [Batch 200] Loss: 0.8969\n",
            "  [Batch 400] Loss: 0.9166\n",
            "  [Batch 600] Loss: 0.5258\n",
            "Epoch 13 Train Loss: 0.7534 | Train Acc: 73.83%\n",
            "===== Epoch 14/50 =====\n",
            "  [Batch 200] Loss: 0.9425\n",
            "  [Batch 400] Loss: 0.6412\n",
            "  [Batch 600] Loss: 0.9419\n",
            "Epoch 14 Train Loss: 0.7392 | Train Acc: 74.51%\n",
            "===== Epoch 15/50 =====\n",
            "  [Batch 200] Loss: 0.7343\n",
            "  [Batch 400] Loss: 0.5792\n",
            "  [Batch 600] Loss: 0.5800\n",
            "Epoch 15 Train Loss: 0.7311 | Train Acc: 74.62%\n",
            "===== Epoch 16/50 =====\n",
            "  [Batch 200] Loss: 0.5127\n",
            "  [Batch 400] Loss: 0.7358\n",
            "  [Batch 600] Loss: 0.7849\n",
            "Epoch 16 Train Loss: 0.7173 | Train Acc: 75.14%\n",
            "===== Epoch 17/50 =====\n",
            "  [Batch 200] Loss: 0.6581\n",
            "  [Batch 400] Loss: 0.6714\n",
            "  [Batch 600] Loss: 0.9165\n",
            "Epoch 17 Train Loss: 0.7066 | Train Acc: 75.58%\n",
            "===== Epoch 18/50 =====\n",
            "  [Batch 200] Loss: 0.6357\n",
            "  [Batch 400] Loss: 0.6196\n",
            "  [Batch 600] Loss: 0.5790\n",
            "Epoch 18 Train Loss: 0.6948 | Train Acc: 75.92%\n",
            "===== Epoch 19/50 =====\n",
            "  [Batch 200] Loss: 0.6782\n",
            "  [Batch 400] Loss: 0.7482\n",
            "  [Batch 600] Loss: 0.5534\n",
            "Epoch 19 Train Loss: 0.6885 | Train Acc: 76.26%\n",
            "===== Epoch 20/50 =====\n",
            "  [Batch 200] Loss: 0.7148\n",
            "  [Batch 400] Loss: 0.8044\n",
            "  [Batch 600] Loss: 0.7146\n",
            "Epoch 20 Train Loss: 0.6822 | Train Acc: 76.41%\n",
            "===== Epoch 21/50 =====\n",
            "  [Batch 200] Loss: 0.5330\n",
            "  [Batch 400] Loss: 0.6887\n",
            "  [Batch 600] Loss: 0.9871\n",
            "Epoch 21 Train Loss: 0.6731 | Train Acc: 76.59%\n",
            "===== Epoch 22/50 =====\n",
            "  [Batch 200] Loss: 0.7714\n",
            "  [Batch 400] Loss: 0.9011\n",
            "  [Batch 600] Loss: 0.6188\n",
            "Epoch 22 Train Loss: 0.6678 | Train Acc: 76.82%\n",
            "===== Epoch 23/50 =====\n",
            "  [Batch 200] Loss: 0.5773\n",
            "  [Batch 400] Loss: 0.7406\n",
            "  [Batch 600] Loss: 0.4829\n",
            "Epoch 23 Train Loss: 0.6592 | Train Acc: 77.15%\n",
            "===== Epoch 24/50 =====\n",
            "  [Batch 200] Loss: 0.4047\n",
            "  [Batch 400] Loss: 0.5615\n",
            "  [Batch 600] Loss: 0.7400\n",
            "Epoch 24 Train Loss: 0.6504 | Train Acc: 77.56%\n",
            "===== Epoch 25/50 =====\n",
            "  [Batch 200] Loss: 0.7239\n",
            "  [Batch 400] Loss: 0.6063\n",
            "  [Batch 600] Loss: 0.6505\n",
            "Epoch 25 Train Loss: 0.6476 | Train Acc: 77.51%\n",
            "===== Epoch 26/50 =====\n",
            "  [Batch 200] Loss: 0.7243\n",
            "  [Batch 400] Loss: 0.9978\n",
            "  [Batch 600] Loss: 0.6152\n",
            "Epoch 26 Train Loss: 0.6421 | Train Acc: 77.66%\n",
            "===== Epoch 27/50 =====\n",
            "  [Batch 200] Loss: 0.7078\n",
            "  [Batch 400] Loss: 0.7532\n",
            "  [Batch 600] Loss: 0.6928\n",
            "Epoch 27 Train Loss: 0.6337 | Train Acc: 77.77%\n",
            "===== Epoch 28/50 =====\n",
            "  [Batch 200] Loss: 0.6923\n",
            "  [Batch 400] Loss: 0.4921\n",
            "  [Batch 600] Loss: 0.7078\n",
            "Epoch 28 Train Loss: 0.6278 | Train Acc: 78.14%\n",
            "===== Epoch 29/50 =====\n",
            "  [Batch 200] Loss: 0.7371\n",
            "  [Batch 400] Loss: 0.6532\n",
            "  [Batch 600] Loss: 0.5783\n",
            "Epoch 29 Train Loss: 0.6212 | Train Acc: 78.44%\n",
            "===== Epoch 30/50 =====\n",
            "  [Batch 200] Loss: 0.5550\n",
            "  [Batch 400] Loss: 0.7685\n",
            "  [Batch 600] Loss: 0.6316\n",
            "Epoch 30 Train Loss: 0.6201 | Train Acc: 78.22%\n",
            "===== Epoch 31/50 =====\n",
            "  [Batch 200] Loss: 0.6278\n",
            "  [Batch 400] Loss: 0.5912\n",
            "  [Batch 600] Loss: 0.5250\n",
            "Epoch 31 Train Loss: 0.6143 | Train Acc: 78.68%\n",
            "===== Epoch 32/50 =====\n",
            "  [Batch 200] Loss: 0.6384\n",
            "  [Batch 400] Loss: 0.4409\n",
            "  [Batch 600] Loss: 0.8771\n",
            "Epoch 32 Train Loss: 0.6057 | Train Acc: 78.82%\n",
            "===== Epoch 33/50 =====\n",
            "  [Batch 200] Loss: 0.5496\n",
            "  [Batch 400] Loss: 0.5824\n",
            "  [Batch 600] Loss: 0.5357\n",
            "Epoch 33 Train Loss: 0.6064 | Train Acc: 78.75%\n",
            "===== Epoch 34/50 =====\n",
            "  [Batch 200] Loss: 0.5376\n",
            "  [Batch 400] Loss: 0.7790\n",
            "  [Batch 600] Loss: 0.7250\n",
            "Epoch 34 Train Loss: 0.5971 | Train Acc: 79.14%\n",
            "===== Epoch 35/50 =====\n",
            "  [Batch 200] Loss: 0.6986\n",
            "  [Batch 400] Loss: 0.6192\n",
            "  [Batch 600] Loss: 0.5133\n",
            "Epoch 35 Train Loss: 0.5940 | Train Acc: 79.25%\n",
            "===== Epoch 36/50 =====\n",
            "  [Batch 200] Loss: 0.5552\n",
            "  [Batch 400] Loss: 0.6682\n",
            "  [Batch 600] Loss: 0.3860\n",
            "Epoch 36 Train Loss: 0.5899 | Train Acc: 79.35%\n",
            "===== Epoch 37/50 =====\n",
            "  [Batch 200] Loss: 0.7395\n",
            "  [Batch 400] Loss: 0.5658\n",
            "  [Batch 600] Loss: 0.7328\n",
            "Epoch 37 Train Loss: 0.5895 | Train Acc: 79.31%\n",
            "===== Epoch 38/50 =====\n",
            "  [Batch 200] Loss: 0.6795\n",
            "  [Batch 400] Loss: 0.5684\n",
            "  [Batch 600] Loss: 0.7203\n",
            "Epoch 38 Train Loss: 0.5805 | Train Acc: 79.77%\n",
            "===== Epoch 39/50 =====\n",
            "  [Batch 200] Loss: 0.5262\n",
            "  [Batch 400] Loss: 0.6669\n",
            "  [Batch 600] Loss: 0.4950\n",
            "Epoch 39 Train Loss: 0.5810 | Train Acc: 79.75%\n",
            "===== Epoch 40/50 =====\n",
            "  [Batch 200] Loss: 0.7199\n",
            "  [Batch 400] Loss: 0.7269\n",
            "  [Batch 600] Loss: 0.7213\n",
            "Epoch 40 Train Loss: 0.5707 | Train Acc: 80.01%\n",
            "===== Epoch 41/50 =====\n",
            "  [Batch 200] Loss: 0.5139\n",
            "  [Batch 400] Loss: 0.5114\n",
            "  [Batch 600] Loss: 0.4027\n",
            "Epoch 41 Train Loss: 0.5691 | Train Acc: 80.10%\n",
            "===== Epoch 42/50 =====\n",
            "  [Batch 200] Loss: 0.5643\n",
            "  [Batch 400] Loss: 0.6863\n",
            "  [Batch 600] Loss: 0.3606\n",
            "Epoch 42 Train Loss: 0.5664 | Train Acc: 80.41%\n",
            "===== Epoch 43/50 =====\n",
            "  [Batch 200] Loss: 0.4737\n",
            "  [Batch 400] Loss: 0.6297\n",
            "  [Batch 600] Loss: 0.6880\n",
            "Epoch 43 Train Loss: 0.5580 | Train Acc: 80.35%\n",
            "===== Epoch 44/50 =====\n",
            "  [Batch 200] Loss: 0.8677\n",
            "  [Batch 400] Loss: 0.4159\n",
            "  [Batch 600] Loss: 0.4192\n",
            "Epoch 44 Train Loss: 0.5575 | Train Acc: 80.34%\n",
            "===== Epoch 45/50 =====\n",
            "  [Batch 200] Loss: 0.3124\n",
            "  [Batch 400] Loss: 0.6216\n",
            "  [Batch 600] Loss: 0.2968\n",
            "Epoch 45 Train Loss: 0.5557 | Train Acc: 80.60%\n",
            "===== Epoch 46/50 =====\n",
            "  [Batch 200] Loss: 0.4090\n",
            "  [Batch 400] Loss: 0.3886\n",
            "  [Batch 600] Loss: 0.5149\n",
            "Epoch 46 Train Loss: 0.5503 | Train Acc: 80.67%\n",
            "===== Epoch 47/50 =====\n",
            "  [Batch 200] Loss: 0.4551\n",
            "  [Batch 400] Loss: 0.6108\n",
            "  [Batch 600] Loss: 0.6647\n",
            "Epoch 47 Train Loss: 0.5473 | Train Acc: 80.65%\n",
            "===== Epoch 48/50 =====\n",
            "  [Batch 200] Loss: 0.4854\n",
            "  [Batch 400] Loss: 0.3063\n",
            "  [Batch 600] Loss: 0.7774\n",
            "Epoch 48 Train Loss: 0.5458 | Train Acc: 80.94%\n",
            "===== Epoch 49/50 =====\n",
            "  [Batch 200] Loss: 0.5064\n",
            "  [Batch 400] Loss: 0.4350\n",
            "  [Batch 600] Loss: 0.4996\n",
            "Epoch 49 Train Loss: 0.5408 | Train Acc: 80.95%\n",
            "===== Epoch 50/50 =====\n",
            "  [Batch 200] Loss: 0.4493\n",
            "  [Batch 400] Loss: 0.5638\n",
            "  [Batch 600] Loss: 0.5608\n",
            "Epoch 50 Train Loss: 0.5400 | Train Acc: 81.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "params = count_parameters(model)\n",
        "print(f\"This model has {params:,} trainable parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kprvADESQZh6",
        "outputId": "9ccc3dc6-bdb6-4323-bf03-445da6947d90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This model has 22,058 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. Evaluation on Test Set\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    class_correct = [0] * 10\n",
        "    class_total = [0] * 10\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            for i in range(labels.size(0)):\n",
        "                label = labels[i].item()\n",
        "                class_total[label] += 1\n",
        "                class_correct[label] += (predicted[i].item() == label)\n",
        "\n",
        "    overall_acc = correct / total\n",
        "    print(f\"\\nTest Accuracy: {overall_acc * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\nPer-class accuracy:\")\n",
        "    for i, cls in enumerate(classes):\n",
        "        if class_total[i] > 0:\n",
        "            acc = 100.0 * class_correct[i] / class_total[i]\n",
        "            print(f\"  {cls:5s}: {acc:5.2f}%\")\n",
        "        else:\n",
        "            print(f\"  {cls:5s}: N/A\")\n",
        "\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8nFxWYm5ZOn",
        "outputId": "191e4fed-4c3e-4653-c3b8-bce526aecba6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 68.31%\n",
            "\n",
            "Per-class accuracy:\n",
            "  plane: 68.60%\n",
            "  car  : 81.00%\n",
            "  bird : 57.80%\n",
            "  cat  : 55.40%\n",
            "  deer : 53.20%\n",
            "  dog  : 58.90%\n",
            "  frog : 74.50%\n",
            "  horse: 74.70%\n",
            "  ship : 81.10%\n",
            "  truck: 77.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Step 1: Load dataset without normalization\n",
        "transform = transforms.ToTensor()  # Only convert to tensor, no normalization yet\n",
        "\n",
        "dataset = datasets.CIFAR10(root=SAVE_DIR, train=True, download=True,\n",
        "                          transform=transform)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Step 2: Calculate mean\n",
        "mean = torch.zeros(3)\n",
        "std = torch.zeros(3)\n",
        "\n",
        "for images, _ in loader:\n",
        "    # images shape: (batch_size, 3, 32, 32)\n",
        "    mean += images.mean(dim=[0, 2, 3])\n",
        "\n",
        "mean /= len(loader)\n",
        "\n",
        "# Step 3: Calculate standard deviation\n",
        "for images, _ in loader:\n",
        "    std += ((images - mean.view(1, 3, 1, 1)) ** 2).mean(dim=[0, 2, 3])\n",
        "\n",
        "std = torch.sqrt(std / len(loader))\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")"
      ],
      "metadata": {
        "id": "gTOY-gQf6SEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694efe14-8cf1-485d-f254-0a9dcc32c521"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: tensor([0.4914, 0.4822, 0.4465])\n",
            "Std: tensor([0.2470, 0.2435, 0.2616])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxb49LSu5v4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}