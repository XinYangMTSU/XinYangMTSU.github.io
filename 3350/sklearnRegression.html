<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Quick Guide: LinearRegression, PolynomialFeatures, Ridge, Lasso & ElasticNet (scikit‑learn)</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6;
      padding: 20px;
    }

    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
      max-width: 950px;
      margin-left: auto;
      margin-right: auto;
    }

    h2 {
      margin: 0 0 10px 0;
      color: #ffb3de;
      font-weight: 700;
    }

    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }

    a.anchor {
      text-decoration: none;
      margin-left: .5rem;
      opacity: .6;
      color: #ffb3de;
    }
    a.anchor:hover { opacity: 1; }

    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
      position: relative;
    }

    pre .copy {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      background: #21111b;
      border: 1px solid #ff79c6;
      border-radius: 6px;
      color: #ffb3de;
      font-size: 0.8em;
      cursor: pointer;
    }
    pre .copy:hover {
      background: #ff79c6;
      color: #fff;
    }

    ul.toc {
      padding-left: 18px;
      margin-top: 8px;
    }
    ul.toc li { margin: 4px 0; }
    ul.toc a { color: #ffb3de; }

    .note { color:#ffd7f0; opacity:.9; }
  </style>
</head>

<body>

<section id="top">
  <center>
    <h1>Quick Guide: LinearRegression, PolynomialFeatures, Ridge, Lasso & ElasticNet (scikit‑learn)</h1>
    <div class="note">scikit‑learn usage with pipelines • scaling • cross‑validation • plotting</div>
  </center>
</section>

<section id="linear">
  <h2>1) LinearRegression (OLS) </h2>
  <h4>Minimal usage</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import LinearRegression

ols = LinearRegression()
ols.fit(X_train, y_train)

y_pred = ols.predict(X_test)
</code></pre>
  
  <div class="note">Params to discuss: <code>fit_intercept</code>. 
      <br>
    Attributes: <code>coef_</code>, <code>intercept_</code>.</div>


   <h4>Vocabulary: parameters vs attributes</h4>
  
    <ul>
      <li><strong>Parameters</strong> — settings you pass to the estimator <em>before training</em> that control how it fits.
        Examples for <code>LinearRegression</code>: <code>fit_intercept</code>, <code>positive</code>, <code>copy_X</code>, <code>n_jobs</code>.
        These are chosen by you (or via CV), not learned from the data.</li>
      <li><strong>Attributes</strong> — fields that exist <em>after</em> <code>.fit()</code>; they are results of training or data‑dependent metadata.
        Examples: <code>coef_</code>, <code>intercept_</code>, <code>rank_</code>, <code>singular_</code>, <code>n_features_in_</code>, <code>feature_names_in_</code>.
      </li>
      <li><strong>Coefficients</strong> — the learned weights \( \beta \) of the linear model; in scikit‑learn they are stored in the <em>attribute</em> <code>coef_</code>.
        For a single target: shape <code>(n_features,)</code>; for multi‑output: shape <code>(n_targets, n_features)</code>.
        The intercept \( \beta_0 \) is stored in <code>intercept_</code>.</li>
    </ul>
  
    <h4>Parameters to know</h4>
    <ul>
      <li><strong>fit_intercept</strong> (default=True): Add an intercept term. If your design matrix <code>X</code> already contains a bias column (e.g., from <code>PolynomialFeatures(include_bias=True)</code>), set <code>fit_intercept=False</code> to avoid a duplicated constant.
        <pre><button class="copy">Copy</button><code># If X already has a column of ones
ols_noint = LinearRegression(fit_intercept=False).fit(X_train, y_train)</code></pre>
      </li>
      <li><strong>positive</strong> (default=False) : When set to True, forces the coefficients to be positive. This option is only supported for dense arrays. Useful when coefficients must be non‑negative by domain knowledge (e.g., additive parts). 
        May be slower and can reduce accuracy if the true relationship has negative effects.
      </li>
      <li><strong>copy_X</strong> (default=True) : If True, X will be copied; else, it may be overwritten. Keep <code>True</code> unless memory is tight.
      When copy_X=True (the default), the copied data is stored in the model's internal memory RAM -  it only exists in your program's memory while the model object exists.
      </li>
      <li><strong>n_jobs</strong> (default=None):  is a parameter that controls parallel processing - how many CPU cores to use simultaneously for computation to speed up the process.
      <b>None</b> Use 1 processor (default). <b>-1</b> Use ALL available processors.
      Set n_jobs=-1 when doing computationally expensive tasks like grid search or cross-validation to use all your CPU cores and speed things up!
      </li>

    </ul>

    <h4>Attributes to know</h4>
    <ul>
      <li><strong>intercept_</strong> : Independent term in the linear model. float or array of shape (n_targets,)</li>
      <li><strong>coef_</strong> : Estimated coefficients for the linear regression problem. coef_array of shape (n_features, ) or (n_targets, n_features)</li>
      <li><strong>n_features_in_</strong> : Number of features seen during fit. </li>
      <li><strong>feature_names_in_</strong> : Names of features seen during fit. 
        Defined only when X has feature names that are all strings.</li>
      <li><strong>rank_</strong> : Rank of matrix X. Only available when X is dense.
      Matrix rank = the number of linearly independent columns (features) in your data matrix.</li>
      <li><strong>singular_</strong> : Singular values of X. Only available when X is dense.</li>
    </ul>


    <h4>What you usually set (90% of the time)</h4>
    <ul>
      <li><strong>Nothing</strong> — defaults are fine for most uses.</li>
      <li><code>fit_intercept=False</code> <em>only</em> if your <code>X</code> already has a bias column 
        (e.g., a column of ones or <code>PolynomialFeatures(include_bias=True)</code>).</li>
    </ul>

</section>

<section id="poly">
  <h2>2) PolynomialFeatures + LinearRegression</h2>
  <h4>Pipeline with degree‑2 features</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

poly2_ols = Pipeline([
    ("poly", PolynomialFeatures(degree=2, include_bias=False)),
    ("ols", LinearRegression())
])

poly2_ols.fit(X_train, y_train)
print("Poly‑2 OLS Test R^2:", poly2_ols.score(X_test, y_test))
  </code></pre>
  <div class="note">Combinatorics: output features grow as <code>C(n + d, d) - 1</code> when <code>include_bias=False</code>. Consider regularization and scaling for higher degrees.</div>
</section>

<section id="ridge">
  <h2>3) Ridge (L2 Regularization) </h2>
  <h4>Pipeline with scaling</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline

ridge = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=1.0, random_state=RANDOM_STATE))
])

ridge.fit(X_train, y_train)
print("Ridge Test R^2:", ridge.score(X_test, y_test))
  </code></pre>
  <div class="note">Tune <code>alpha</code>; larger ⇒ more shrinkage. Scaling makes the penalty fair across features.</div>
</section>

<section id="lasso">
  <h2>4) Lasso (L1 Regularization)</h2>
  <h4>Sparse solutions & feature selection</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import Lasso
from sklearn.pipeline import Pipeline

lasso = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=0.05, max_iter=20000, random_state=RANDOM_STATE))
])

lasso.fit(X_train, y_train)
print("Lasso non‑zero features:", np.count_nonzero(lasso[-1].coef_))
print("Lasso Test R^2:", lasso.score(X_test, y_test))
  </code></pre>
  <div class="note">Increase <code>max_iter</code> if you hit convergence warnings. Zeroed coefficients ≠ causal truth.</div>
</section>

<section id="enet">
  <h2>5) ElasticNet (L1 + L2)</h2>
  <h4>Balance sparsity & stability</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import ElasticNet
from sklearn.pipeline import Pipeline

enet = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=20000, random_state=RANDOM_STATE))
])

enet.fit(X_train, y_train)
print("ElasticNet Test R^2:", enet.score(X_test, y_test))
  </code></pre>
  <div class="note"><code>l1_ratio</code>: 0 → Ridge‑like, 1 → Lasso. Useful with correlated predictors when you still want sparsity.</div>
</section>

<section id="cv">
  <h2>6) Cross‑Validation Shortcuts</h2>
  <h4>RidgeCV, LassoCV, ElasticNetCV</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV

alphas = np.logspace(-3, 3, 25)

ridge_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", RidgeCV(alphas=alphas))
])
ridge_cv.fit(X_train, y_train)
print("RidgeCV best alpha:", ridge_cv[-1].alpha_)
print("RidgeCV Test R^2:", ridge_cv.score(X_test, y_test))

lasso_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", LassoCV(alphas=alphas, max_iter=20000, cv=5, random_state=RANDOM_STATE))
])
lasso_cv.fit(X_train, y_train)
print("LassoCV best alpha:", lasso_cv[-1].alpha_)
print("LassoCV Test R^2:", lasso_cv.score(X_test, y_test))

enet_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNetCV(l1_ratio=[.2,.5,.8,1.0], alphas=alphas, max_iter=20000, cv=5, random_state=RANDOM_STATE))
])
enet_cv.fit(X_train, y_train)
print("ElasticNetCV best alpha:", enet_cv[-1].alpha_, "; best l1_ratio:", enet_cv[-1].l1_ratio_)
print("ElasticNetCV Test R^2:", enet_cv.score(X_test, y_test))
  </code></pre>
</section>

<section id="plot">
  <h2>7) Quick Plot — Predicted vs Actual </h2>
  <h4>Swap <code>model</code> to compare OLS / Ridge / Lasso / ENet / Poly pipelines</h4>
  <pre><button class="copy">Copy</button><code>
# Choose a fitted model object, e.g. ridge, lasso, enet, poly2_ols, ridge_cv, ...
model = ridge

yhat = model.predict(X_test)
plt.figure()
plt.scatter(y_test, yhat, alpha=0.6)
plt.xlabel("Actual"); plt.ylabel("Predicted")
plt.title("Predicted vs Actual")
lim = [min(y_test.min(), yhat.min()), max(y_test.max(), yhat.max())]
plt.plot(lim, lim, linestyle='--')
plt.show()
  </code></pre>
</section>

<section id="refs">
  <h2>8) Official References</h2>
  <ul class="toc">
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">sklearn.linear_model.LinearRegression</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">sklearn.preprocessing.PolynomialFeatures</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">sklearn.linear_model.Ridge</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">sklearn.linear_model.Lasso</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">sklearn.linear_model.ElasticNet</a></li>
  </ul>
</section>

  <!-- MathJax (once per page) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
<script>
  document.querySelectorAll("pre .copy").forEach(button => {
    button.addEventListener("click", () => {
      const code = button.nextElementSibling.innerText;
      navigator.clipboard.writeText(code).then(() => {
        const old = button.textContent;
        button.textContent = "Copied!";
        setTimeout(() => button.textContent = old, 1500);
      });
    });
  });
</script>

</body>
</html>
