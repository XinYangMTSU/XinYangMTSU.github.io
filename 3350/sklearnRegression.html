<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Quick Guide: LinearRegression, PolynomialFeatures, Ridge, Lasso & ElasticNet (scikit‑learn)</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6;
      padding: 20px;
    }

    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
      max-width: 950px;
      margin-left: auto;
      margin-right: auto;
    }

    h2 {
      margin: 0 0 10px 0;
      color: #ffb3de;
      font-weight: 700;
    }

    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }

    a.anchor {
      text-decoration: none;
      margin-left: .5rem;
      opacity: .6;
      color: #ffb3de;
    }
    a.anchor:hover { opacity: 1; }

    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
      position: relative;
    }

    pre .copy {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      background: #21111b;
      border: 1px solid #ff79c6;
      border-radius: 6px;
      color: #ffb3de;
      font-size: 0.8em;
      cursor: pointer;
    }
    pre .copy:hover {
      background: #ff79c6;
      color: #fff;
    }

    ul.toc {
      padding-left: 18px;
      margin-top: 8px;
    }
    ul.toc li { margin: 4px 0; }
    ul.toc a { color: #ffb3de; }

    .note { color:#ffd7f0; opacity:.9; }
  </style>
</head>

<body>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<section id="top">
  <center>
    <h1>Quick Guide: LinearRegression, PolynomialFeatures, Ridge, Lasso & ElasticNet (scikit‑learn)</h1>
    <div class="note">scikit‑learn usage with pipelines • scaling • cross‑validation • plotting</div>
  </center>
</section>

<section id="linear">
  <h2>1) LinearRegression (OLS) </h2>
  <h4></h4>
  <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — LinearRegression
# Uses all features, simple cleaning, train/test split
# Shows BOTH training and testing metrics
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame  # features + target ('MedHouseVal') in one DataFrame

# 2) Data cleaning (check missing values and duplicates)
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

# Get X, y after cleaning
X = df_all.drop(columns=["MedHouseVal"])
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Train a Linear Regression model on the training set
# Learn the coefficients from X_train and y_train
lr = LinearRegression()
lr.fit(X_train, y_train)

# 5) Predict and compute metrics for TRAIN and TEST
# --- Train ---
y_pred_train = lr.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)
sse_train = np.sum((y_train - y_pred_train) ** 2)

# --- Test ---
y_pred_test = lr.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)
sse_test = np.sum((y_test - y_pred_test) ** 2)

# 6) Put results in a tidy DataFrame
results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

# 7) Results
print(f"any_missing: {has_missing}")
print(f"duplicates_found: {duplicates_found}")
print(results_df)

# 8) ## Learned attributes after fit
print("--- learned attributes ---")
print("coef_.shape:", lr.coef_.shape)
print("coef_ :", np.round(lr.coef_,3))
print("intercept_:", np.round(float(lr.intercept_), 3))
print("n_features_in_:", lr.n_features_in_)
print("feature_names_in_:", list(lr.feature_names_in_))

if hasattr(lr, 'rank_'):
    print("rank_:", lr.rank_)
if hasattr(lr, 'singular_'):
    # singular_ is a vector; show first few values
    print("singular_ (first 4):", np.round(lr.singular_[:4], 3))
    print(f"Length of singular_: {len(lr.singular_)}") 
</code></pre>
  
  <div class="note">Params to discuss: <code>fit_intercept</code>. 
      <br>
    Attributes: <code>coef_</code>, <code>intercept_</code>.</div>


   <h4>Vocabulary: parameters vs attributes</h4>
  
    <ul>
      <li><strong>Parameters</strong> — settings you pass to the estimator <em>before training</em> that control how it fits.
        Examples for <code>LinearRegression</code>: <code>fit_intercept</code>, <code>positive</code>, <code>copy_X</code>, <code>n_jobs</code>.
        These are chosen by you (or via CV), not learned from the data.</li>
      <li><strong>Attributes</strong> — fields that exist <em>after</em> <code>.fit()</code>; they are results of training or data‑dependent metadata.
        Examples: <code>coef_</code>, <code>intercept_</code>, <code>rank_</code>, <code>singular_</code>, <code>n_features_in_</code>, <code>feature_names_in_</code>.
      </li>
      <li><strong>Coefficients</strong> — the learned weights \( \beta_0 \) of the linear model; in scikit‑learn they are stored in the <em>attribute</em> <code>coef_</code>.
        For a single target: shape <code>(n_features,)</code>; for multi‑output: shape <code>(n_targets, n_features)</code>.
        The intercept \( \beta_0 \) is stored in <code>intercept_</code>.</li>
    </ul>
  
    <h4>LinearRegression Parameters to Know</h4>
    <ul>
      <li><strong>fit_intercept</strong> (default=True): Add an intercept term. If your design matrix <code>X</code> already contains a bias column (e.g., from <code>PolynomialFeatures(include_bias=True)</code>), set <code>fit_intercept=False</code> to avoid a duplicated constant.
        <pre><button class="copy">Copy</button><code># If X already has a column of ones
ols_noint = LinearRegression(fit_intercept=False).fit(X_train, y_train)</code></pre>
      </li>
      <li><strong>positive</strong> (default=False) : When set to True, forces the coefficients to be positive. This option is only supported for dense arrays. Useful when coefficients must be non‑negative by domain knowledge (e.g., additive parts). 
        May be slower and can reduce accuracy if the true relationship has negative effects.
      </li>
      <li><strong>copy_X</strong> (default=True) : If True, X will be copied; else, it may be overwritten. Keep <code>True</code> unless memory is tight.
      When copy_X=True (the default), the copied data is stored in the model's internal memory RAM -  it only exists in your program's memory while the model object exists.
      </li>
      <li><strong>n_jobs</strong> (default=None):  is a parameter that controls parallel processing - how many CPU cores to use simultaneously for computation to speed up the process.
      <b>None</b> Use 1 processor (default). <b>-1</b> Use ALL available processors. <b>n > 1</b> Use n processors.
      Set n_jobs=-1 when doing computationally expensive tasks like grid search or cross-validation to use all your CPU cores and speed things up!
      n_jobs=1 helps only when there’s real parallel work; otherwise, it can slow things down.
      </li>

    </ul>

    <h4>LinearRegression Attributes to Know</h4>
    <ul>
      <li><strong>intercept_</strong> : Independent term in the linear model. float or array of shape (n_targets,)</li>
      <li><strong>coef_</strong> : Estimated coefficients for the linear regression problem. coef_array of shape (n_features, ) or (n_targets, n_features)</li>
      <li><strong>n_features_in_</strong> : Number of features seen during fit. </li>
      <li><strong>feature_names_in_</strong> : Names of features seen during fit. 
        Defined only when X has feature names that are all strings.</li>
      <li><strong>rank_</strong> : Rank of matrix X. Only available when X is dense.
      Matrix rank = the number of linearly independent columns (features) in your data matrix.</li>
      <li><strong>singular_</strong> : Singular values of X. Only available when X is dense.</li>
    </ul>


    <h4>What you usually set</h4>
    <ul>
      <li><strong>Nothing</strong> — defaults are fine for most uses.</li>
      <li><code>fit_intercept=False</code> <em>only</em> if your <code>X</code> already has a bias column 
        (e.g., a column of ones or <code>PolynomialFeatures(include_bias=True)</code>).</li>
    </ul>

</section>


<section id="polynomial">
  
        <h2>2) Polynomial Regression = PolynomialFeatures + LinearRegression </h2>
         
        <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — Polynomial Regression
# Uses PolynomialFeatures + LinearRegression pipeline
# Shows BOTH training and testing metrics
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame  # features + target ('MedHouseVal') in one DataFrame

# 2) Data cleaning (check missing values and duplicates)
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

# Get X, y after cleaning
X = df_all.drop(columns=["MedHouseVal"])
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Create Polynomial Regression pipeline
# degree=2: adds x^2, x1*x2, etc. terms
poly_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ("scaler", StandardScaler()),
    ('linear', LinearRegression())
])

# Fit the pipeline
poly_reg.fit(X_train, y_train)

# 5) Predict and compute metrics for TRAIN and TEST
# --- Train ---
y_pred_train = poly_reg.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)
sse_train = np.sum((y_train - y_pred_train) ** 2)

# --- Test ---
y_pred_test = poly_reg.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)
sse_test = np.sum((y_test - y_pred_test) ** 2)

# 6) Put results in a tidy DataFrame
results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

# 7) Results
print(f"any_missing: {has_missing}")
print(f"duplicates_found: {duplicates_found}")
print(results_df)

# 8) ## Pipeline and learned attributes
print("--- Pipeline Information ---")
print("Pipeline steps:", poly_reg.steps)

# Get the polynomial transformer and linear regression
poly_features = poly_reg.named_steps['poly']
linear_reg = poly_reg.named_steps['linear']

print("--- PolynomialFeatures attributes ---")
print("degree:", poly_features.degree)
print("Original features(n_features_in_):", poly_features.n_features_in_)
print("Polynomial features(n_output_features_):", poly_features.n_output_features_)

print("--- LinearRegression attributes (after polynomial transformation) ---")
print("coef_.shape:", linear_reg.coef_.shape)
print("coef_ (first 10):", np.round(linear_reg.coef_[:10],3))
print("intercept_:", np.round(float(linear_reg.intercept_), 3))
print("n_features_in_:", linear_reg.n_features_in_)

if hasattr(linear_reg, 'rank_'):
    print("rank_:", linear_reg.rank_)
if hasattr(linear_reg, 'singular_'):
    print("singular_ (first 5):", np.round(linear_reg.singular_[:5], 3))
    print(f"Length of singular_: {len(linear_reg.singular_)}")
        </code></pre>

        <div class="note">Key components: <code>PolynomialFeatures</code> for feature expansion, <code>Pipeline</code> for combining steps.
            <br>
            Parameters to discuss: <code>degree</code>, <code>include_bias</code>, <code>interaction_only</code>.
        </div>

        <h4>Vocabulary: Polynomial Features</h4>
        
        <ul>
            <li><strong>Polynomial Features</strong> — transforms original features into polynomial combinations.
                For degree=2 with features [x1, x2]: creates [x1, x2, x1², x2², x1*x2].
                This allows linear regression to capture non-linear relationships.</li>
              <br>
            <li><strong>Pipeline</strong> — chains preprocessing and model steps together.
                Ensures transformations are applied consistently to train/test data and prevents data leakage.</li>
              <br>
            <li><strong>Feature Explosion</strong> — polynomial features can create many new features quickly.
                With n original features and degree d: creates C(n+d, d) features.
                Example: 8 features, degree 2 → 44 features; degree 3 → 164 features!</li>

           <br>
            <b style="text-decoration: underline;">Polynomial Features Breakdown: 8 Original Features, degree 2 --> 44 Total Features</b>

        <ul class="poly-expanded-list">
<li><strong>Original features (8):</strong>
<div>\( x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8 \)</div>
</li>
<li><strong>With degree = 2, new features:</strong>
<ul>
<li><strong>1. Squared terms (8 new features):</strong>
<div>\( x_1^2, x_2^2, x_3^2, x_4^2, x_5^2, x_6^2, x_7^2, x_8^2 \)</div>
</li>
<li><strong>2. Interaction terms (28 new features):</strong>
<ul>
<li>\( x_1 \times x_2, x_1 \times x_3, x_1 \times x_4, x_1 \times x_5, x_1 \times x_6, x_1 \times x_7, x_1 \times x_8 \) <b>(7 terms)</b></li>
<li>\( x_2 \times x_3, x_2 \times x_4, x_2 \times x_5, x_2 \times x_6, x_2 \times x_7, x_2 \times x_8 \) <b>(6 terms)</b></li>
<li>\( x_3 \times x_4, x_3 \times x_5, x_3 \times x_6, x_3 \times x_7, x_3 \times x_8 \) <b>(5 terms)</b></li>
<li>\( x_4 \times x_5, x_4 \times x_6, x_4 \times x_7, x_4 \times x_8 \) <b>(4 terms)</b></li>
<li>\( x_5 \times x_6, x_5 \times x_7, x_5 \times x_8 \) <b>(3 terms)</b></li>
<li>\( x_6 \times x_7, x_6 \times x_8 \) <b>(2 terms)</b></li>
<li>\( x_7 \times x_8 \) <b>(1 term)</b></li>
</ul>
</li>
</ul>
</li>
<li><strong>Total:</strong> 8 original + 8 squared + 28 interactions = <strong>44 features</strong></li>
</ul>
          <br>
          <b style="text-decoration: underline;"> The mathematical formula:</b>
          <ul>
              <li>For <strong>n</strong> original features with degree <strong>d</strong>, the total number of polynomial features is:</li>
               \(
                   C(n + d, d) - 1 = \frac{(n + d)!}{d! \times n!} - 1
               \)
              <li>For n=8, d=2:</li>
               \(
                C(8 + 2, 2) - 1 = C(10, 2) - 1 = \frac{10!}{2! \times 8!} -1 = \frac{10 \times 9 \times 8!}{2! \times 8!} -1 = \frac{10 \times 9}{2 \times 1} - 1 = 45 - 1 = 44
                \)
          </ul>
          
          <br>
          
        </ul>

        <h4>PolynomialFeatures Parameters to Know</h4>
        <ul>
            <li><strong>degree</strong> (default=2): The degree of polynomial features.
                Higher degrees capture more complex relationships but risk overfitting.
                <pre><button class="copy">Copy</button><code># Linear: y = a + b*x1 + c*x2
poly_1 = PolynomialFeatures(degree=1)  # Just original features

# Quadratic: y = a + b*x1 + c*x2 + d*x1² + e*x2² + f*x1*x2  
poly_2 = PolynomialFeatures(degree=2)  # Adds squared and interaction terms

# Cubic: adds x1³, x2³, x1²*x2, x1*x2², etc.
poly_3 = PolynomialFeatures(degree=3)</code></pre>
            </li>
          
            <li><strong>include_bias</strong> (default=True): Whether to include a bias column (column of ones).
                Set to False when using with LinearRegression(fit_intercept=True) to avoid redundancy.
                <pre><button class="copy">Copy</button><code># Recommended: let LinearRegression handle intercept
PolynomialFeatures(degree=2, include_bias=False)
LinearRegression(fit_intercept=True)  # Default</code></pre>
            </li>
            <li><strong>interaction_only</strong> (default=False): If True, only interaction features are produced.
                No pure polynomial terms (like x1², x2²), only products between different features (x1*x2).
                <pre><button class="copy">Copy</button><code># With [x1, x2] and degree=2:
# interaction_only=False → [x1, x2, x1², x2², x1*x2] 
# interaction_only=True  → [x1, x2, x1*x2]  # No x1², x2²</code></pre>
            </li>
            <li><strong>order</strong> (default='C'): order='C' (row-major, default): rows are stored contiguously in memory. 
              Most scikit-learn estimators expect/convert to this, so it’s the safe default.
              order='F' (column-major / Fortran): columns are contiguous.</li>
        </ul>

        <h4>PolynomialFeatures Attributes to Know</h4>
        <ul>
            <li><strong>n_features_in_</strong> :  Number of features seen during fit.</li>
            <li><strong>feature_names_in_</strong> : Names of features seen during fit.</li>
            <li><strong>n_output_features_</strong> : The total number of polynomial output features.
            The number of output features is computed by iterating over all suitably sized combinations of input features.</li>
        </ul>
  
        <h4>Pipeline Benefits</h4>
        <ul>
            <li><strong>Prevents Data Leakage</strong>: Ensures preprocessing is fitted only on training data,
                then applied to both train and test sets.</li>
          
            <li><strong>Cleaner Code</strong>: Single object to fit, predict, and cross-validate.</li>
          
            <li><strong>Consistent Workflow</strong>: Same preprocessing applied automatically during prediction.</li>
          
            <li><strong>Grid Search Friendly</strong>: Can tune both preprocessing and model parameters together.
                <pre><button class="copy">Copy</button><code>from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import Ridge
import numpy as np

pipe = Pipeline([
    ("poly", PolynomialFeatures(include_bias=False)),
    ("scaler", StandardScaler()),
    ("ridge", Ridge())
])

param_grid = {
    "poly__degree": [1, 2, 3],
    "ridge__alpha": np.logspace(-3, 3, 30),
}

search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1, scoring="r2")
search.fit(X_train, y_train)
print(search.best_params_)
</code></pre>
            </li>
          
            <li><strong>named_steps</strong>: named_steps is a handy attribute on a scikit-learn Pipeline.
              It's a dict-like that maps the step names you gave in the pipeline to the actual fitted objects (transformers/estimator).
            </li>
          <br>
          In the polynomial pipeline:
          <pre><button class="copy">Copy</button><code>poly_lr = Pipeline([
              ("poly", PolynomialFeatures(degree=2, include_bias=False)),
              ("scaler", StandardScaler()),
              ("lr", LinearRegression())
])</code></pre>
          You can access each step like this:
           <pre><button class="copy">Copy</button><code>poly   = poly_lr.named_steps["poly"]     # the PolynomialFeatures transformer
scaler = poly_lr.named_steps["scaler"]   # the StandardScaler
lr     = poly_lr.named_steps["lr"]       # the LinearRegression estimator</code></pre>
        </ul>

  
      <h4>What you usually set</h4>
        <ul>
            <li><strong>Standard Setup</strong>: 
                <code>PolynomialFeatures(degree=2, include_bias=False) + LinearRegression()</code></li>
            <li><strong>With Regularization</strong>: Use Ridge/Lasso instead of LinearRegression to handle
                the many features created by polynomial expansion.</li>
            <li><strong>Feature Scaling</strong>: Consider adding StandardScaler before PolynomialFeatures
                since polynomial terms can have very different scales.</li>
        </ul>
    </section>


<section id="ridge">
  <h2>3) Ridge (L2 Regularization) </h2>
  <h4></h4>
  <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — Ridge (L2 Regularization)
# Standardizes features, train/test split
# Shows BOTH training and testing metrics
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame  # features + target ('MedHouseVal')

# 2) Data cleaning (check missing values and duplicates)
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

# Get X, y after cleaning
X = df_all.drop(columns=["MedHouseVal"])  # DataFrame (names → for display)
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Train a Ridge model in a scaled pipeline (recommended)
ridge = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=1.0, random_state=RANDOM_STATE))
])

ridge.fit(X_train, y_train)

# 5) Predict and compute metrics for TRAIN and TEST
# --- Train ---
y_pred_train = ridge.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)
sse_train = np.sum((y_train - y_pred_train) ** 2)

# --- Test ---
y_pred_test = ridge.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)
sse_test = np.sum((y_test - y_pred_test) ** 2)

# 6) Put results in a tidy DataFrame
results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

# 7) Results
print(f"any_missing: {has_missing}")
print(f"duplicates_found: {duplicates_found}")
print(results_df)

# 8) ## Learned attributes after fit (on the estimator step)
print("--- learned attributes (Ridge in pipeline) ---")
r = ridge.named_steps["ridge"]
print("coef_.shape:", r.coef_.shape)
print("coef_:", np.round(r.coef_, 3))
print("intercept_:", np.round(float(r.intercept_), 3))
print("n_features_in_:", r.n_features_in_)
# feature_names_in_ is typically unavailable after a StandardScaler step; show original names instead
if hasattr(r, 'feature_names_in_'):
    print("feature_names_in_:", list(r.feature_names_in_))
else:
    print("feature_names (from original X):", list(X.columns))
print("alpha:", r.alpha)
  </code></pre>
  
  <div class="note">Params to discuss: <code>alpha</code>, <code>fit_intercept</code>, <code>solver</code>, <code>max_iter</code>, <code>tol</code>. <br>
      <strong>Scale your features</strong> (as shown) so the L2 penalty treats features fairly. Use <code>RidgeCV</code> or <code>GridSearchCV</code> to select a good <code>alpha</code>.</div>

  <h4>Ridge Parameters to Know</h4>
  <ul>
    <li><strong>alpha</strong> (default=1.0): 
      Constant that multiplies the L2 term, controlling regularization strength. alpha must be a non-negative float i.e. in [0, inf).
      L2 strength. Larger → more shrinkage. Tune via <code>RidgeCV</code> or <code>GridSearchCV</code>.</li>
    <li><strong>fit_intercept</strong> (default=True): Intercept term; keep True unless your design already includes a bias column.</li>
    <li><strong>solver</strong> (default='auto'): Solver to use in the computational routines. 'auto' chooses the solver automatically based on the type of data. You can also try <code>'sag'</code>/<code>'saga'</code> for large-scale, or <code>'lsqr'</code>/<code>'cholesky'</code>/<code>'svd'</code> in specific cases.</li>
    <li><strong>max_iter</strong> (default=None) : Maximum number of iterations for conjugate gradient solver.  (e.g., <code>'sag'</code>/<code>'saga'</code>); increase <code>max_iter</code> if you see convergence warnings.</li>
    <li><strong>tol</strong> (default=1e-4): The precision of the solution (coef_) is determined by tol which specifies a different convergence criterion for each solver.</li>
    <li><strong>random_state</strong> (default=None): Controls the shuffling of data when using solvers that rely on stochastic (randomized) optimization, specifically sag (Stochastic Average Gradient) and saga (a variant with L1 support).</li>
    When you set solver="auto"
    <ul>
        <li>solver="auto" means scikit-learn will pick the best solver based on your data (size, penalty, etc.).</li>
        <li>If the automatically chosen solver is deterministic (lbfgs, newton-cg, liblinear, etc.), the random_state parameter has no effect.</li>
        <li>If the automatically chosen solver is stochastic (sag or saga), then random_state does matter, because those solvers shuffle the training data.</li>
    </ul>
    
  </ul>

  <h4>Ridge Attributes to Know</h4>
  <ul>
    <li><strong>intercept_</strong> : Independent term in the linear model.</li>
    <li><strong>coef_</strong> : Estimated coefficients after L2 shrinkage.</li>
    <li><strong>n_features_in_</strong> : Number of features seen during fit.</li>
    <li><strong>feature_names_in_</strong> : Present only if the estimator received a DataFrame directly; with a <code>Pipeline</code> that outputs NumPy arrays, names are not propagated by default.</li>
    <li><strong>n_iter_</strong> : Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.</li>
  </ul>

  <h4>What you usually set</h4>
  <ul>
    <li><strong>alpha</strong> (via CV).</li>
    <li><strong>Scaling</strong>: include <code>StandardScaler()</code> in a pipeline.</li>
    <li><strong>max_iter</strong> only if using iterative solvers and you see convergence warnings.</li>
  </ul>
</section>

<section id="lasso">
  <h2>4) Lasso (L1) Regression</h2>
  <h4></h4>

  <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — Lasso (L1) Regression
# Standardizes features (important for L1), train/test split
# Shows BOTH training and testing metrics + sparsity (# of nonzero coeffs)
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame

# 2) Data cleaning
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

X = df_all.drop(columns=["MedHouseVal"])
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Pipeline: Standardize -> Lasso
lasso_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=0.01, max_iter=10_000, tol=1e-4))
])

lasso_pipe.fit(X_train, y_train)

# 5) Predict and compute metrics
y_pred_train = lasso_pipe.predict(X_train)
y_pred_test  = lasso_pipe.predict(X_test)

# Metrics
def eval_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    sse = np.sum((y_true - y_pred) ** 2)
    return r2, rmse, mse, sse

r2_train, rmse_train, mse_train, sse_train = eval_metrics(y_train, y_pred_train)
r2_test,  rmse_test,  mse_test,  sse_test  = eval_metrics(y_test,  y_pred_test)

results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

print(results_df)

lasso = lasso_pipe.named_steps["lasso"]
print("--- learned attributes ---")
print("coef_.shape:", lasso.coef_.shape)
print("coef_:", np.round(lasso.coef_, 3))
print("intercept_:", np.round(float(lasso.intercept_), 3))
print("n_features_in_:", lasso.n_features_in_)
print("nonzero coefficients:", np.count_nonzero(lasso.coef_))
</code></pre>

  <div class="note">
    Params to discuss: <code>alpha</code>, <code>max_iter</code>, <code>tol</code>, <code>fit_intercept</code>, <code>selection</code>.
    <br>
    Attributes: <code>coef_</code>, <code>intercept_</code>, <code>n_iter_</code>, <code>n_features_in_</code>.
  </div>

  <h4>Vocabulary: Sparsity</h4>
  <ul>
    <li><strong>Sparsity</strong> — L1 drives many coefficients to zero, performing implicit feature selection.</li>
  </ul>

  <h4>Lasso Parameters to Know</h4>
  <ul>
    <li><strong>alpha</strong> (default=1.0): Regularization strength. Higher → more shrinkage and sparsity.</li>
    <li><strong>max_iter</strong> (default=1000): Max optimization iterations. Raise if you see convergence warnings.</li>
    <li><strong>tol</strong> (default=1e-4): Optimization tolerance. Smaller → stricter convergence.</li>
    <li><strong>fit_intercept</strong> (default=True): Whether to add an intercept term.</li>
    <li><strong>selection</strong> (default="cyclic"): Coordinate descent order. "random" can converge faster sometimes.</li>
  </ul>

  <h4>Lasso Attributes to Know</h4>
  <ul>
    <li><strong>intercept_</strong>: Independent bias term of the model.</li>
    <li><strong>coef_</strong>: Learned feature weights (many will be exactly 0).</li>
    <li><strong>n_features_in_</strong>: Number of input features seen during fit.</li>
    <li><strong>feature_names_in_</strong>: Names of input features (if provided).</li>
    <li><strong>n_iter_</strong>: Actual number of iterations run until convergence.</li>
  </ul>

  <h4>What you usually set</h4>
  <ul>
    <li><strong>alpha</strong> — tune with <code>LassoCV</code> for best generalization.</li>
    <li><strong>max_iter</strong> — increase if you hit convergence warnings.</li>
    <li><strong>fit_intercept=False</strong> only if <code>X</code> already has a bias column.</li>
  </ul>
</section>


<section id="elasticnet">
  <h2>5) ElasticNet (L1 + L2)</h2>
  <h4></h4>

  <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — ElasticNet (L1 + L2)
# Standardizes features, train/test split
# Shows BOTH training and testing metrics + sparsity
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame  # features + target ('MedHouseVal') in one DataFrame

# 2) Data cleaning (check missing values and duplicates)
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

# Get X, y after cleaning
X = df_all.drop(columns=["MedHouseVal"])
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Pipeline: Standardize -> ElasticNet
# L1 + L2 penalty is scale-sensitive, so standardize first.
enet_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNet(alpha=0.02, l1_ratio=0.5, max_iter=20_000, tol=1e-4, random_state=RANDOM_STATE))
])

# Fit on training data
enet_pipe.fit(X_train, y_train)

# 5) Predict and compute metrics for TRAIN and TEST
def eval_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    sse = np.sum((y_true - y_pred) ** 2)
    return r2, rmse, mse, sse

# --- Train ---
y_pred_train = enet_pipe.predict(X_train)
r2_train, rmse_train, mse_train, sse_train = eval_metrics(y_train, y_pred_train)

# --- Test ---
y_pred_test = enet_pipe.predict(X_test)
r2_test, rmse_test, mse_test, sse_test = eval_metrics(y_test, y_pred_test)

# 6) Put results in a tidy DataFrame
results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

# 7) Results
print(f"any_missing: {has_missing}")
print(f"duplicates_found: {duplicates_found}")
print(results_df)

# 8) ## Learned attributes after fit
# Extract the trained ElasticNet from the pipeline
enet = enet_pipe.named_steps["enet"]

print("--- learned attributes (on standardized features) ---")
print("coef_.shape:", enet.coef_.shape)
print("coef_ :", np.round(enet.coef_, 3))
print("intercept_:", np.round(float(enet.intercept_), 3))
print("n_features_in_:", enet.n_features_in_)

# Sparsity summary (L1 part can zero some coefficients)
nonzero = int(np.count_nonzero(enet.coef_))
print(f"nonzero coefficients: {nonzero} / {enet.coef_.size}")

# Optional: feature names for reference (from original X)
print("feature_names_in_:", list(X.columns))

# Optional (tune hyperparameters with CV):
# from sklearn.linear_model import ElasticNetCV
# cv_pipe = Pipeline([
#     ("scaler", StandardScaler()),
#     ("enet", ElasticNetCV(
#         l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],
#         alphas=np.logspace(-3, 1, 50),
#         cv=5, max_iter=50_000, random_state=RANDOM_STATE))
# ])
# cv_pipe.fit(X_train, y_train)
# print("Best alpha:", cv_pipe.named_steps["enet"].alpha_)
# print("Best l1_ratio:", cv_pipe.named_steps["enet"].l1_ratio_)
</code></pre>

  <div class="note">
    Params to discuss:
    <code>alpha</code>, <code>l1_ratio</code>, <code>max_iter</code>, <code>tol</code>, <code>fit_intercept</code>.
    <br>
    Attributes: <code>coef_</code>, <code>intercept_</code>, <code>n_iter_</code> (if exposed), <code>n_features_in_</code>.
    <br>
    ElasticNet blends L1 (sparsity/feature selection) and L2 (stability/shrinkage).
  </div>

  <h4>Vocabulary: Blended penalty</h4>
  <ul>
    
    <li><strong>Blended penalty</strong> — L1 encourages zeros (feature selection), L2 stabilizes and shares weight across correlated predictors.</li>
  </ul>

  <h4>ElasticNet Parameters to Know</h4>
  <ul>
    <li><strong>alpha</strong> (default=1.0): Overall regularization strength. Tune via <code>ElasticNetCV</code> for best generalization.</li>
    <li><strong>l1_ratio</strong> (default=0.5): Balance between L1 and L2. 0 → pure Ridge; 1 → pure Lasso.</li>
    <li><strong>Scaling required</strong>: Standardize features so the penalties treat features fairly.</li>
    <li><strong>max_iter</strong> / <strong>tol</strong>: Increase <code>max_iter</code> or relax <code>tol</code> if convergence warnings appear.</li>
    <li><strong>fit_intercept=False</strong> only if your <code>X</code> already includes a bias column.</li>
  </ul>

  <h4>What you usually set</h4>
  <ul>
    <li><strong>alpha</strong> and <strong>l1_ratio</strong> via <code>ElasticNetCV</code>.</li>
    <li><strong>max_iter</strong> if you hit convergence warnings.</li>
  </ul>
</section>


<section id="cv">
  <h2>6) Cross‑Validation Shortcuts</h2>
  <h4>RidgeCV, LassoCV, ElasticNetCV</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV

alphas = np.logspace(-3, 3, 25)

ridge_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", RidgeCV(alphas=alphas))
])
ridge_cv.fit(X_train, y_train)
print("RidgeCV best alpha:", ridge_cv[-1].alpha_)
print("RidgeCV Test R^2:", ridge_cv.score(X_test, y_test))

lasso_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", LassoCV(alphas=alphas, max_iter=20000, cv=5, random_state=RANDOM_STATE))
])
lasso_cv.fit(X_train, y_train)
print("LassoCV best alpha:", lasso_cv[-1].alpha_)
print("LassoCV Test R^2:", lasso_cv.score(X_test, y_test))

enet_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNetCV(l1_ratio=[.2,.5,.8,1.0], alphas=alphas, max_iter=20000, cv=5, random_state=RANDOM_STATE))
])
enet_cv.fit(X_train, y_train)
print("ElasticNetCV best alpha:", enet_cv[-1].alpha_, "; best l1_ratio:", enet_cv[-1].l1_ratio_)
print("ElasticNetCV Test R^2:", enet_cv.score(X_test, y_test))
  </code></pre>
</section>


<section id="refs">
  <h2>7) Official References</h2>
  <ul class="toc">
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">sklearn.linear_model.LinearRegression</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">sklearn.preprocessing.PolynomialFeatures</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">sklearn.linear_model.Ridge</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">sklearn.linear_model.Lasso</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">sklearn.linear_model.ElasticNet</a></li>
  </ul>
</section>
  
<script>
  document.querySelectorAll("pre .copy").forEach(button => {
    button.addEventListener("click", () => {
      const code = button.nextElementSibling.innerText;
      navigator.clipboard.writeText(code).then(() => {
        const old = button.textContent;
        button.textContent = "Copied!";
        setTimeout(() => button.textContent = old, 1500);
      });
    });
  });
</script>

</body>
</html>
