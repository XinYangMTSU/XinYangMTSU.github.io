<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Quick Guide: LinearRegression, PolynomialFeatures, Ridge, Lasso & ElasticNet (scikit‑learn)</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6;
      padding: 20px;
    }

    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
      max-width: 950px;
      margin-left: auto;
      margin-right: auto;
    }

    h2 {
      margin: 0 0 10px 0;
      color: #ffb3de;
      font-weight: 700;
    }

    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }

    a.anchor {
      text-decoration: none;
      margin-left: .5rem;
      opacity: .6;
      color: #ffb3de;
    }
    a.anchor:hover { opacity: 1; }

    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
      position: relative;
    }

    pre .copy {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      background: #21111b;
      border: 1px solid #ff79c6;
      border-radius: 6px;
      color: #ffb3de;
      font-size: 0.8em;
      cursor: pointer;
    }
    pre .copy:hover {
      background: #ff79c6;
      color: #fff;
    }

    ul.toc {
      padding-left: 18px;
      margin-top: 8px;
    }
    ul.toc li { margin: 4px 0; }
    ul.toc a { color: #ffb3de; }

    .note { color:#ffd7f0; opacity:.9; }
  </style>
</head>

<body>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<section id="top">
  <center>
    <h1>Quick Guide: LinearRegression, PolynomialFeatures, Ridge, Lasso & ElasticNet (scikit‑learn)</h1>
    <div class="note">scikit‑learn usage with pipelines • scaling • cross‑validation • plotting</div>
  </center>
</section>

<section id="linear">
  <h2>1) LinearRegression (OLS) </h2>
  <h4></h4>
  <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — LinearRegression
# Uses all features, simple cleaning, train/test split
# Shows BOTH training and testing metrics
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame  # features + target ('MedHouseVal') in one DataFrame

# 2) Data cleaning (check missing values and duplicates)
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

# Get X, y after cleaning
X = df_all.drop(columns=["MedHouseVal"])
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Train a Linear Regression model on the training set
# Learn the coefficients from X_train and y_train
lr = LinearRegression()
lr.fit(X_train, y_train)

# 5) Predict and compute metrics for TRAIN and TEST
# --- Train ---
y_pred_train = lr.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)
sse_train = np.sum((y_train - y_pred_train) ** 2)

# --- Test ---
y_pred_test = lr.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)
sse_test = np.sum((y_test - y_pred_test) ** 2)

# 6) Put results in a tidy DataFrame
results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

# 7) Results
print(f"any_missing: {has_missing}")
print(f"duplicates_found: {duplicates_found}")
print(results_df)

# 8) ## Learned attributes after fit
print("--- learned attributes ---")
print("coef_.shape:", lr.coef_.shape)
print("coef_ :", np.round(lr.coef_,3))
print("intercept_:", np.round(float(lr.intercept_), 3))
print("n_features_in_:", lr.n_features_in_)
print("feature_names_in_:", list(lr.feature_names_in_))

if hasattr(lr, 'rank_'):
    print("rank_:", lr.rank_)
if hasattr(lr, 'singular_'):
    # singular_ is a vector; show first few values
    print("singular_ (first 4):", np.round(lr.singular_[:4], 3))
    print(f"Length of singular_: {len(lr.singular_)}") 
</code></pre>
  
  <div class="note">Params to discuss: <code>fit_intercept</code>. 
      <br>
    Attributes: <code>coef_</code>, <code>intercept_</code>.</div>


   <h4>Vocabulary: parameters vs attributes</h4>
  
    <ul>
      <li><strong>Parameters</strong> — settings you pass to the estimator <em>before training</em> that control how it fits.
        Examples for <code>LinearRegression</code>: <code>fit_intercept</code>, <code>positive</code>, <code>copy_X</code>, <code>n_jobs</code>.
        These are chosen by you (or via CV), not learned from the data.</li>
      <li><strong>Attributes</strong> — fields that exist <em>after</em> <code>.fit()</code>; they are results of training or data‑dependent metadata.
        Examples: <code>coef_</code>, <code>intercept_</code>, <code>rank_</code>, <code>singular_</code>, <code>n_features_in_</code>, <code>feature_names_in_</code>.
      </li>
      <li><strong>Coefficients</strong> — the learned weights \( \beta_0 \) of the linear model; in scikit‑learn they are stored in the <em>attribute</em> <code>coef_</code>.
        For a single target: shape <code>(n_features,)</code>; for multi‑output: shape <code>(n_targets, n_features)</code>.
        The intercept \( \beta_0 \) is stored in <code>intercept_</code>.</li>
    </ul>
  
    <h4>LinearRegression Parameters to Know</h4>
    <ul>
      <li><strong>fit_intercept</strong> (default=True): Add an intercept term. If your design matrix <code>X</code> already contains a bias column (e.g., from <code>PolynomialFeatures(include_bias=True)</code>), set <code>fit_intercept=False</code> to avoid a duplicated constant.
        <pre><button class="copy">Copy</button><code># If X already has a column of ones
ols_noint = LinearRegression(fit_intercept=False).fit(X_train, y_train)</code></pre>
      </li>
      <li><strong>positive</strong> (default=False) : When set to True, forces the coefficients to be positive. This option is only supported for dense arrays. Useful when coefficients must be non‑negative by domain knowledge (e.g., additive parts). 
        May be slower and can reduce accuracy if the true relationship has negative effects.
      </li>
      <li><strong>copy_X</strong> (default=True) : If True, X will be copied; else, it may be overwritten. Keep <code>True</code> unless memory is tight.
      When copy_X=True (the default), the copied data is stored in the model's internal memory RAM -  it only exists in your program's memory while the model object exists.
      </li>
      <li><strong>n_jobs</strong> (default=None):  is a parameter that controls parallel processing - how many CPU cores to use simultaneously for computation to speed up the process.
      <b>None</b> Use 1 processor (default). <b>-1</b> Use ALL available processors. <b>n > 1</b> Use n processors.
      Set n_jobs=-1 when doing computationally expensive tasks like grid search or cross-validation to use all your CPU cores and speed things up!
      n_jobs=1 helps only when there’s real parallel work; otherwise, it can slow things down.
      </li>

    </ul>

    <h4>LinearRegression Attributes to Know</h4>
    <ul>
      <li><strong>intercept_</strong> : Independent term in the linear model. float or array of shape (n_targets,)</li>
      <li><strong>coef_</strong> : Estimated coefficients for the linear regression problem. coef_array of shape (n_features, ) or (n_targets, n_features)</li>
      <li><strong>n_features_in_</strong> : Number of features seen during fit. </li>
      <li><strong>feature_names_in_</strong> : Names of features seen during fit. 
        Defined only when X has feature names that are all strings.</li>
      <li><strong>rank_</strong> : Rank of matrix X. Only available when X is dense.
      Matrix rank = the number of linearly independent columns (features) in your data matrix.</li>
      <li><strong>singular_</strong> : Singular values of X. Only available when X is dense.</li>
    </ul>


    <h4>What you usually set (90% of the time)</h4>
    <ul>
      <li><strong>Nothing</strong> — defaults are fine for most uses.</li>
      <li><code>fit_intercept=False</code> <em>only</em> if your <code>X</code> already has a bias column 
        (e.g., a column of ones or <code>PolynomialFeatures(include_bias=True)</code>).</li>
    </ul>

</section>


<section id="polynomial">
        <h2>2) Polynomial Regression = PolynomialFeatures + LinearRegression </h2>
         <div class="note">Combinatorics: output features grow as <code>C(n + d, d) - 1</code> when <code>include_bias=False</code>. Consider regularization and scaling for higher degrees.</div>
        <pre><button class="copy">Copy</button><code>
# ========================================================
# California Housing — Polynomial Regression
# Uses PolynomialFeatures + LinearRegression pipeline
# Shows BOTH training and testing metrics
# ========================================================
import numpy as np
import pandas as pd

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

RANDOM_STATE = 42

# 1) Load data
data = fetch_california_housing(as_frame=True)
df_all = data.frame  # features + target ('MedHouseVal') in one DataFrame

# 2) Data cleaning (check missing values and duplicates)
has_missing = df_all.isna().any().any()
duplicates_found = int(df_all.duplicated().sum())

if duplicates_found > 0:
    df_all = df_all.drop_duplicates().reset_index(drop=True)
if has_missing:
    df_all = df_all.fillna(df_all.mean(numeric_only=True))

# Get X, y after cleaning
X = df_all.drop(columns=["MedHouseVal"])
y = df_all["MedHouseVal"]

# 3) Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 4) Create Polynomial Regression pipeline
# degree=2: adds x^2, x1*x2, etc. terms
poly_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('linear', LinearRegression())
])

# Fit the pipeline
poly_reg.fit(X_train, y_train)

# 5) Predict and compute metrics for TRAIN and TEST
# --- Train ---
y_pred_train = poly_reg.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)
sse_train = np.sum((y_train - y_pred_train) ** 2)

# --- Test ---
y_pred_test = poly_reg.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)
sse_test = np.sum((y_test - y_pred_test) ** 2)

# 6) Put results in a tidy DataFrame
results_df = pd.DataFrame(
    [
        {"Split": "Train", "R^2": r2_train, "RMSE": rmse_train, "MSE": mse_train, "SSE": sse_train},
        {"Split": "Test",  "R^2": r2_test,  "RMSE": rmse_test,  "MSE": mse_test,  "SSE": sse_test},
    ]
).set_index("Split")

# 7) Results
print(f"any_missing: {has_missing}")
print(f"duplicates_found: {duplicates_found}")
print(results_df)

# 8) ## Pipeline and learned attributes
print("--- Pipeline Information ---")
print("Pipeline steps:", poly_reg.steps)

# Get the polynomial transformer and linear regression
poly_features = poly_reg.named_steps['poly']
linear_reg = poly_reg.named_steps['linear']

print("--- PolynomialFeatures attributes ---")
print("degree:", poly_features.degree)
print("n_features_in_:", poly_features.n_features_in_)
print("n_output_features_:", poly_features.n_output_features_)

print("--- LinearRegression attributes (after polynomial transformation) ---")
print("coef_.shape:", linear_reg.coef_.shape)
print("coef_ (first 10):", np.round(linear_reg.coef_[:10], 3))
print("intercept_:", np.round(float(linear_reg.intercept_), 3))
print("n_features_in_:", linear_reg.n_features_in_)

if hasattr(linear_reg, 'rank_'):
    print("rank_:", linear_reg.rank_)
if hasattr(linear_reg, 'singular_'):
    print("singular_ (first 5):", np.round(linear_reg.singular_[:5], 3))
    print(f"Length of singular_: {len(linear_reg.singular_)}")

print(f"Original features: {X.shape[1]}")
print(f"Polynomial features: {linear_reg.n_features_in_}")
</code></pre>

        <div class="note">Key components: <code>PolynomialFeatures</code> for feature expansion, <code>Pipeline</code> for combining steps.
            <br>
            Parameters to discuss: <code>degree</code>, <code>include_bias</code>, <code>interaction_only</code>.
        </div>

        <h4>Vocabulary: Polynomial Features</h4>
        
        <ul>
            <li><strong>Polynomial Features</strong> — transforms original features into polynomial combinations.
                For degree=2 with features [x1, x2]: creates [x1, x2, x1², x2², x1*x2].
                This allows linear regression to capture non-linear relationships.</li>
            <br>
            <b style="text-decoration: underline;">Polynomial Features Breakdown: 8 Original Features --> 44 Total Features</b>

        <ul class="poly-expanded-list">
<li><strong>Original features (8):</strong>
<div>\( x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8 \)</div>
</li>
<li><strong>With degree = 2, new features:</strong>
<ul>
<li><strong>1. Squared terms (8 new features):</strong>
<div>\( x_1^2, x_2^2, x_3^2, x_4^2, x_5^2, x_6^2, x_7^2, x_8^2 \)</div>
</li>
<li><strong>2. Interaction terms (28 new features):</strong>
<ul>
<li>\( x_1 \times x_2, x_1 \times x_3, x_1 \times x_4, x_1 \times x_5, x_1 \times x_6, x_1 \times x_7, x_1 \times x_8 \) <b>(7 terms)</b></li>
<li>\( x_2 \times x_3, x_2 \times x_4, x_2 \times x_5, x_2 \times x_6, x_2 \times x_7, x_2 \times x_8 \) <b>(6 terms)</b></li>
<li>\( x_3 \times x_4, x_3 \times x_5, x_3 \times x_6, x_3 \times x_7, x_3 \times x_8 \) <b>(5 terms)</b></li>
<li>\( x_4 \times x_5, x_4 \times x_6, x_4 \times x_7, x_4 \times x_8 \) <b>(4 terms)</b></li>
<li>\( x_5 \times x_6, x_5 \times x_7, x_5 \times x_8 \) <b>(3 terms)</b></li>
<li>\( x_6 \times x_7, x_6 \times x_8 \) <b>(2 terms)</b></li>
<li>\( x_7 \times x_8 \) <b>(1 term)</b></li>
</ul>
</li>
</ul>
</li>
<li><strong>Total:</strong> 8 original + 8 squared + 28 interactions = <strong>44 features</strong></li>
</ul>
          <br>
          
            <li><strong>Pipeline</strong> — chains preprocessing and model steps together.
                Ensures transformations are applied consistently to train/test data and prevents data leakage.</li>
            <li><strong>Feature Explosion</strong> — polynomial features can create many new features quickly.
                With n original features and degree d: creates C(n+d, d) features.
                Example: 8 features, degree 2 → 44 features; degree 3 → 164 features!</li>
        </ul>

        <h4>PolynomialFeatures Parameters to Know</h4>
        <ul>
            <li><strong>degree</strong> (default=2): The degree of polynomial features.
                Higher degrees capture more complex relationships but risk overfitting.
                <pre><button class="copy">Copy</button><code># Linear: y = a + b*x1 + c*x2
poly_1 = PolynomialFeatures(degree=1)  # Just original features

# Quadratic: y = a + b*x1 + c*x2 + d*x1² + e*x2² + f*x1*x2  
poly_2 = PolynomialFeatures(degree=2)  # Adds squared and interaction terms

# Cubic: adds x1³, x2³, x1²*x2, x1*x2², etc.
poly_3 = PolynomialFeatures(degree=3)</code></pre>
            </li>
          
            <li><strong>include_bias</strong> (default=True): Whether to include a bias column (column of ones).
                Set to False when using with LinearRegression(fit_intercept=True) to avoid redundancy.
                <pre><button class="copy">Copy</button><code># Recommended: let LinearRegression handle intercept
PolynomialFeatures(degree=2, include_bias=False)
LinearRegression(fit_intercept=True)  # Default</code></pre>
            </li>
            <li><strong>interaction_only</strong> (default=False): If True, only interaction features are produced.
                No pure polynomial terms (like x1², x2²), only products between different features (x1*x2).
                <pre><button class="copy">Copy</button><code># With [x1, x2] and degree=2:
# interaction_only=False → [x1, x2, x1², x2², x1*x2] 
# interaction_only=True  → [x1, x2, x1*x2]  # No x1², x2²</code></pre>
            </li>
            <li><strong>order</strong> (default='C'): Output array order. 'C' for C-style, 'F' for Fortran-style.
                Usually keep default unless you have specific memory layout needs.</li>
        </ul>

      
        <h4>Pipeline Benefits</h4>
        <ul>
            <li><strong>Prevents Data Leakage</strong>: Ensures preprocessing is fitted only on training data,
                then applied to both train and test sets.</li>
            <li><strong>Cleaner Code</strong>: Single object to fit, predict, and cross-validate.</li>
            <li><strong>Consistent Workflow</strong>: Same preprocessing applied automatically during prediction.</li>
            <li><strong>Grid Search Friendly</strong>: Can tune both preprocessing and model parameters together.
                <pre><button class="copy">Copy</button><code># Can tune both polynomial degree and regularization
param_grid = {
    'poly__degree': [1, 2, 3],
    'linear__fit_intercept': [True, False]
}</code></pre>
            </li>
        </ul>

      <h4>What you usually set</h4>
        <ul>
            <li><strong>Standard Setup</strong>: 
                <code>PolynomialFeatures(degree=2, include_bias=False) + LinearRegression()</code></li>
            <li><strong>With Regularization</strong>: Use Ridge/Lasso instead of LinearRegression to handle
                the many features created by polynomial expansion.</li>
            <li><strong>Feature Scaling</strong>: Consider adding StandardScaler before PolynomialFeatures
                since polynomial terms can have very different scales.</li>
        </ul>

    </section>


<section id="ridge">
  <h2>3) Ridge (L2 Regularization) </h2>
  <h4>Pipeline with scaling</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline

ridge = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=1.0, random_state=RANDOM_STATE))
])

ridge.fit(X_train, y_train)
print("Ridge Test R^2:", ridge.score(X_test, y_test))
  </code></pre>
  <div class="note">Tune <code>alpha</code>; larger ⇒ more shrinkage. Scaling makes the penalty fair across features.</div>
</section>

<section id="lasso">
  <h2>4) Lasso (L1 Regularization)</h2>
  <h4>Sparse solutions & feature selection</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import Lasso
from sklearn.pipeline import Pipeline

lasso = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=0.05, max_iter=20000, random_state=RANDOM_STATE))
])

lasso.fit(X_train, y_train)
print("Lasso non‑zero features:", np.count_nonzero(lasso[-1].coef_))
print("Lasso Test R^2:", lasso.score(X_test, y_test))
  </code></pre>
  <div class="note">Increase <code>max_iter</code> if you hit convergence warnings. Zeroed coefficients ≠ causal truth.</div>
</section>

<section id="enet">
  <h2>5) ElasticNet (L1 + L2)</h2>
  <h4>Balance sparsity & stability</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import ElasticNet
from sklearn.pipeline import Pipeline

enet = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=20000, random_state=RANDOM_STATE))
])

enet.fit(X_train, y_train)
print("ElasticNet Test R^2:", enet.score(X_test, y_test))
  </code></pre>
  <div class="note"><code>l1_ratio</code>: 0 → Ridge‑like, 1 → Lasso. Useful with correlated predictors when you still want sparsity.</div>
</section>

<section id="cv">
  <h2>6) Cross‑Validation Shortcuts</h2>
  <h4>RidgeCV, LassoCV, ElasticNetCV</h4>
  <pre><button class="copy">Copy</button><code>
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV

alphas = np.logspace(-3, 3, 25)

ridge_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", RidgeCV(alphas=alphas))
])
ridge_cv.fit(X_train, y_train)
print("RidgeCV best alpha:", ridge_cv[-1].alpha_)
print("RidgeCV Test R^2:", ridge_cv.score(X_test, y_test))

lasso_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", LassoCV(alphas=alphas, max_iter=20000, cv=5, random_state=RANDOM_STATE))
])
lasso_cv.fit(X_train, y_train)
print("LassoCV best alpha:", lasso_cv[-1].alpha_)
print("LassoCV Test R^2:", lasso_cv.score(X_test, y_test))

enet_cv = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNetCV(l1_ratio=[.2,.5,.8,1.0], alphas=alphas, max_iter=20000, cv=5, random_state=RANDOM_STATE))
])
enet_cv.fit(X_train, y_train)
print("ElasticNetCV best alpha:", enet_cv[-1].alpha_, "; best l1_ratio:", enet_cv[-1].l1_ratio_)
print("ElasticNetCV Test R^2:", enet_cv.score(X_test, y_test))
  </code></pre>
</section>

<section id="plot">
  <h2>7) Quick Plot — Predicted vs Actual </h2>
  <h4>Swap <code>model</code> to compare OLS / Ridge / Lasso / ENet / Poly pipelines</h4>
  <pre><button class="copy">Copy</button><code>
# Choose a fitted model object, e.g. ridge, lasso, enet, poly2_ols, ridge_cv, ...
model = ridge

yhat = model.predict(X_test)
plt.figure()
plt.scatter(y_test, yhat, alpha=0.6)
plt.xlabel("Actual"); plt.ylabel("Predicted")
plt.title("Predicted vs Actual")
lim = [min(y_test.min(), yhat.min()), max(y_test.max(), yhat.max())]
plt.plot(lim, lim, linestyle='--')
plt.show()
  </code></pre>
</section>

<section id="refs">
  <h2>8) Official References</h2>
  <ul class="toc">
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">sklearn.linear_model.LinearRegression</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">sklearn.preprocessing.PolynomialFeatures</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">sklearn.linear_model.Ridge</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">sklearn.linear_model.Lasso</a></li>
    <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">sklearn.linear_model.ElasticNet</a></li>
  </ul>
</section>
  
<script>
  document.querySelectorAll("pre .copy").forEach(button => {
    button.addEventListener("click", () => {
      const code = button.nextElementSibling.innerText;
      navigator.clipboard.writeText(code).then(() => {
        const old = button.textContent;
        button.textContent = "Copied!";
        setTimeout(() => button.textContent = old, 1500);
      });
    });
  });
</script>

</body>
</html>
