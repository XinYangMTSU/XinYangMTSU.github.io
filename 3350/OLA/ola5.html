
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OLA5 – Unsupervised Learning: Clustering & Dimensionality Reduction (MNIST Only)</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 950px; margin: 0 auto; padding: 20px; background-color: #17191a; color: #ff79c6; }
    .container { background-color: #222025; padding: 30px; border-radius: 8px; box-shadow: 0 2px 12px rgba(255,121,198,0.15); }
    h1 { text-align: center; font-size: 26px; color: #ff79c6; margin-bottom: 6px; }
    h2.course-title { text-align: center; font-size: 28px; color: #ffb3de; margin-bottom: 18px; }
    h3.deadline { text-align: center; font-size: 15px; color: #ffb3de; margin-top: 0; margin-bottom: 20px; }
    h3 { color: #ff79c6; font-size: 22px; margin-bottom: 12px; border-bottom: 3px solid #ff79c6; padding-bottom: 6px; }
    h4 { color: #ffb3de; font-size: 18px; margin-top: 20px; margin-bottom: 8px; border-bottom: 1px solid #ffb3de; padding-bottom: 3px; }
    pre { background: #19121a; color: #ffb3de; padding: 15px; border-radius: 6px; font-size: 0.95em; overflow-x: auto; }
    code { font-family: Consolas, monospace; }
    ul, ol, p, li { color: #ffb3de; }
    li { margin-bottom: 8px; }
    table { width: 70%; margin: 15px auto; border-collapse: collapse; font-size: 0.9em; color: #ffb3de; }
    th, td { border: 1px solid #ff79c6; padding: 6px 10px; text-align: left; }
    th { background: #19121a; color: #ff79c6; }
    tr:nth-child(even) { background: #2b2b2b; }
    .highlight { background-color: #3d2940; padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
    #backToTop { position: fixed; bottom: 24px; right: 24px; background-color: #ff79c6; color: #fff; border: none; border-radius: 50%; width: 48px; height: 48px; font-size: 1.3em; cursor: pointer; display: flex; align-items: center; justify-content: center; box-shadow: 0 4px 12px rgba(0,0,0,0.25); transition: background 0.2s; }
    #backToTop:hover { background-color: #ffb3de; color: #17191a; }
    footer { text-align: center; margin-top: 30px; font-size: 0.9em; color: #ffb3de; }
  </style>
</head>
<body>

<div class="container">
  <h2 class="course-title">CSCI 3350 -- Machine Learning</h2>
  <h1>OLA5 – Unsupervised Learning: Clustering & Dimensionality Reduction (MNIST)</h1>
  <h3 class="deadline"><font color="red">Deadline:</font> November 19th, 2025 (Wednesday)</h3>

  <h3>Objective</h3>
  <p>
    Explore <b>unsupervised learning</b> on a single dataset, <b>MNIST</b>, using both <b>clustering</b> and <b>dimensionality reduction</b>.
    You will (1) find natural groupings with <b>K-Means</b> and <b>Hierarchical Clustering</b> (evaluated by the <b>Elbow Method</b>),
    and (2) reduce feature complexity with <b>PCA</b> and <b>t-SNE</b> to visualize structure. Labels are used only for coloring plots.
  </p>

  <h4>Key Concepts</h4>
  <ul>
    <li><b>K-Means</b>: centroid-based clustering; choose k via <span class="highlight">Elbow Method (inertia)</span></li>
    <li><b>Hierarchical (Agglomerative)</b>: linkages (<code>single</code>, <code>complete</code>, <code>average</code>); dendrograms on a subsample</li>
    <li><b>PCA</b>: linear dimensionality reduction; explained variance; 2D projections</li>
    <li><b>t-SNE</b>: nonlinear visualization emphasizing local neighborhoods; perplexity exploration</li>
    <li><b>Standardization</b> before clustering/dendrogram is essential</li>
  </ul>

  <h3>Requirements (MNIST Dataset)</h3>
  <ol>
    <li><b>Step 1: Load & Preprocess MNIST</b>
      <ul>
        <li>Load MNIST (e.g., first <b>5000</b> samples) via <code>keras.datasets.mnist</code> or <code>fetch_openml('mnist_784')</code></li>
        <li>Create a DataFrame (features) and store labels separately (do <b>not</b> use for clustering)</li>
        <li><b>EDA</b>:
          <ul>
            <li>Dataset shape; basic stats (mean, std, min, max)</li>
            <li>Histogram of digit counts (0–9)</li>
            <li>Grid of <b>10</b> sample 28×28 digit images</li>
          </ul>
        </li>
        <li>Standardize features with <code>StandardScaler</code></li>
      </ul>
    </li>

    <li><b>Step 2: K-Means (Elbow Method)</b>
      <ul>
        <li>For k = 2…10, fit <code>KMeans(random_state=42, n_init=10)</code>, record <b>inertia</b></li>
        <li>Plot <b>Elbow Curve</b> (k vs inertia) and pick the elbow k</li>
      </ul>
    </li>

    <li><b>Step 3: K-Means (Optimal k) & Hierarchical Clustering</b>
      <ul>
        <li><b>K-Means:</b>
          <ul>
            <li>Fit with the elbow-chosen k; obtain labels & centroids</li>
          </ul>
        </li>
        <li><b>Hierarchical (Agglomerative):</b>
          <ul>
            <li>Try linkages: <code>'single'</code>, <code>'complete'</code>, <code>'average'</code></li>
            <li>For the elbow k, fit AgglomerativeClustering for each linkage</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><b>Step 4: Dendrograms (Subsample) & Comparison</b>
      <ul>
        <li>Randomly sample ~<b>300–500</b> points (full dendrogram on 5000 is impractical)</li>
        <li>For the two best-looking linkages from Step 3:
          <ul>
            <li>Compute <code>scipy.cluster.hierarchy.linkage()</code> on the subsample</li>
            <li>Plot dendrograms with a horizontal cut line indicating k</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><b>Step 5: PCA on MNIST</b>
      <ul>
        <li>Fit <code>PCA(n_components=None)</code> on standardized data to analyze full variance</li>
        <li>Create a 2D scatter (PC1 vs PC2) colored by <b>true digit</b> (for visualization only)</li>
      </ul>
    </li>

    <li><b>Step 6: t-SNE on MNIST</b>
      <ul>
        <!--
        <li>First reduce to ~<b>50 PCs</b> (e.g., <code>PCA(n_components=50)</code>) to speed/stabilize t-SNE</li>
        -->
        <li>Run t-SNE with perplexities <b>[5, 30, 50]</b>; use <code>TSNE(n_components=2, learning_rate="auto", random_state=42, n_iter=1000)</code></li>
        <li>Make a 1×3 subplot of the three perplexities, colored by <b>true digit</b></li>
        <li>Select the best perplexity and show a final high-quality 2D plot (optional 3D t-SNE)</li>
      </ul>
    </li>

    <li><b>Step 7: Write-Up (8–10 sentences)</b>
      <ul>
        <li>Elbow-based k and what it suggests about digit structure</li>
        <li>Visual differences between K-Means and Hierarchical results</li>
        <li>How many PCs for ~95% variance; trade-offs of dimensionality</li>
        <li>What t-SNE reveals vs PCA; effect of perplexity</li>
        <li>When to use clustering vs dimensionality reduction in practice</li>
      </ul>
    </li>
  </ol>

  <h3>Expected Output</h3>
  <ul>
    <li>10 sample digit images and digit count histogram</li>
    <li>Elbow plot (k vs inertia) + chosen k</li>
    <li>Two dendrograms (best linkages) on subsample with cut line</li>
    <li>PCA-2D colored by digit</li>
    <li>t-SNE perplexity exploration (1×3) + final best t-SNE</li>
    <li>5–10 sentence analysis</li>
  </ul>

  <h3>Grading Distribution (Total: 100 points)</h3>
  <table>
    <tr><th>Task</th><th>Points</th></tr>
    <tr><td>MNIST preprocessing & EDA (Step 1)</td><td>12</td></tr>
    <tr><td>Elbow method (plot & choice) (Step 2)</td><td>12</td></tr>
    <tr><td>K-Means viz & Hierarchical linkages (Step 3)</td><td>16</td></tr>
    <tr><td>Dendrograms (subsample) & comparison (Step 4)</td><td>12</td></tr>
    <tr><td>PCA variance analysis & PCA-2D (Step 5)</td><td>16</td></tr>
    <tr><td>t-SNE exploration & final plot (Step 6)</td><td>16</td></tr>
    <tr><td>PCA vs t-SNE comparison (Step 7)</td><td>8</td></tr>
    <tr><td>Write-up (Step 8)</td><td>8</td></tr>
    <tr><th>Total</th><th>100</th></tr>
  </table>

  <h3>Starter Code Skeleton (MNIST Only)</h3>
  <pre><code># --- OLA5: Unsupervised Learning on MNIST (Clustering + DR) ---

from sklearn.datasets import fetch_openml

SEED = 42
np.random.seed(SEED)

# ===== Step 1: Load & Preprocess MNIST =====

    
# ===== Step 2: K-Means Elbow =====


# ===== Step 3: K-Means @ k_opt & Hierarchical =====


# ===== Step 4: Dendrograms on Subsample =====
# Subsample 400 points


# ===== Step 5: PCA Variance & PCA-2D by True Digit =====


# ===== Step 6: t-SNE  =====


  </code></pre>

  <h3>Hints</h3>
  <ul>
    <li>Use labels only for coloring plots and qualitative comparison—not for clustering.</li>
    <li>Elbow selection is a judgment call: look for where inertia reduction noticeably tapers.</li>
    <li>Dendrograms: always subsample for readability and performance.</li>
  </ul>

  <footer>
    © Dr. Xin Yang <br>
    Department of Computer & Information Sciences <br>
    Fisk University
  </footer>
</div>

<button id="backToTop" title="Go to top">↑</button>
<script>
  document.getElementById("backToTop").addEventListener("click", function() {
    window.scrollTo({ top: 0, behavior: "smooth" });
  });
</script>
</body>
</html>
