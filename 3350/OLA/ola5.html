<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OLA5 – Unsupervised Learning: Clustering & Dimensionality Reduction (MNIST Only)</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 950px; margin: 0 auto; padding: 20px; background-color: #17191a; color: #ff79c6; }
    .container { background-color: #222025; padding: 30px; border-radius: 8px; box-shadow: 0 2px 12px rgba(255,121,198,0.15); }
    h1 { text-align: center; font-size: 26px; color: #ff79c6; margin-bottom: 6px; }
    h2.course-title { text-align: center; font-size: 28px; color: #ffb3de; margin-bottom: 18px; }
    h3.deadline { text-align: center; font-size: 15px; color: #ffb3de; margin-top: 0; margin-bottom: 20px; }
    h3 { color: #ff79c6; font-size: 22px; margin-bottom: 12px; border-bottom: 3px solid #ff79c6; padding-bottom: 6px; }
    h4 { color: #ffb3de; font-size: 18px; margin-top: 20px; margin-bottom: 8px; border-bottom: 1px solid #ffb3de; padding-bottom: 3px; }
    pre { background: #19121a; color: #ffb3de; padding: 15px; border-radius: 6px; font-size: 0.95em; overflow-x: auto; }
    code { font-family: Consolas, monospace; }
    ul, ol, p, li { color: #ffb3de; }
    li { margin-bottom: 8px; }
    table { width: 70%; margin: 15px auto; border-collapse: collapse; font-size: 0.9em; color: #ffb3de; }
    th, td { border: 1px solid #ff79c6; padding: 6px 10px; text-align: left; }
    th { background: #19121a; color: #ff79c6; }
    tr:nth-child(even) { background: #2b2b2b; }
    .highlight { background-color: #3d2940; padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
    #backToTop { position: fixed; bottom: 24px; right: 24px; background-color: #ff79c6; color: #fff; border: none; border-radius: 50%; width: 48px; height: 48px; font-size: 1.3em; cursor: pointer; display: flex; align-items: center; justify-content: center; box-shadow: 0 4px 12px rgba(0,0,0,0.25); transition: background 0.2s; }
    #backToTop:hover { background-color: #ffb3de; color: #17191a; }
    footer { text-align: center; margin-top: 30px; font-size: 0.9em; color: #ffb3de; }
  </style>
</head>
<body>

<div class="container">
  <h2 class="course-title">CSCI 3350 -- Machine Learning</h2>
  <h1>OLA5 – Unsupervised Learning: Clustering & Dimensionality Reduction (MNIST)</h1>
  <h3 class="deadline"><font color="red">Deadline:</font> November 23rd, 2025 (Sunday)</h3>

  <h3>Objective</h3>
  <p>
    Explore <b>unsupervised learning</b> on a single dataset, <b>MNIST</b>, using both <b>clustering</b> and <b>dimensionality reduction</b>.
    You will (1) find natural groupings with <b>K-Means</b> and <b>Hierarchical Clustering</b> (evaluated by the <b>Elbow Method</b>),
    and (2) reduce feature complexity with <b>PCA</b> and <b>t-SNE</b> to visualize structure. Labels are used only for coloring plots.
  </p>

  <h4>Key Concepts</h4>
  <ul>
    <li><b>K-Means</b>: centroid-based clustering; choose k via <span class="highlight">Elbow Method (inertia)</span></li>
    <li><b>Hierarchical (Agglomerative)</b>: linkages (<code>single</code>, <code>complete</code>, <code>average</code>); dendrograms on a subsample</li>
    <li><b>PCA</b>: linear dimensionality reduction; explained variance; 2D projections</li>
    <li><b>t-SNE</b>: nonlinear visualization emphasizing local neighborhoods; perplexity exploration</li>
    <li><b>Standardization</b> before clustering/dendrogram is essential</li>
  </ul>

  <h3>Requirements (MNIST Dataset)</h3>
  <ol>
    <li><b>Step 1: Load & Preprocess MNIST</b>
      <ul>
        <li>Load MNIST (e.g., first <b>5000</b> samples) via <code>keras.datasets.mnist</code> or <code>fetch_openml('mnist_784')</code></li>
        <li>Create a DataFrame (features) and store labels separately (do <b>not</b> use for clustering)</li>
        <li><b>EDA</b>:
          <ul>
            <li>Dataset shape; basic stats (mean, std, min, max)</li>
            <li>Histogram of digit counts (0–9)</li>
            <li>Grid of <b>10</b> sample 28×28 digit images</li>
          </ul>
        </li>
        <li>Standardize features with <code>StandardScaler</code></li>
      </ul>
    </li>

    <li><b>Step 2: K-Means (Elbow Method)</b>
      <ul>
        <li>For k = 2…10, fit <code>KMeans(random_state=42, n_init=10)</code>, record <b>inertia</b></li>
        <li>Plot <b>Elbow Curve</b> (k vs inertia) and pick the elbow k</li>
      </ul>
    </li>

    <li><b>Step 3: K-Means (Optimal k) & Hierarchical Clustering</b>
      <ul>
        <li><b>K-Means:</b>
          <ul>
            <li>Fit with the elbow-chosen k; obtain labels & centroids</li>
          </ul>
        </li>
        <li><b>Hierarchical (Agglomerative):</b>
          <ul>
            <li>Try linkages: <code>'single'</code>, <code>'complete'</code>, <code>'average'</code></li>
            <li>For the elbow k, fit AgglomerativeClustering for each linkage</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><b>Step 4: Dendrograms (Subsample)</b>
      <ul>
        <li>Display dendrograms with ~<b>30–100</b> points (full dendrogram on 5000 is impractical)</li>
      </ul>
    </li>

    <li><b>Step 5: PCA on MNIST</b>
      <ul>
        <li>Fit <code>PCA(n_components=None)</code> on standardized data to analyze full variance</li>
        <li>Create a 2D scatter (PC1 vs PC2) colored by <b>true digit</b> (for visualization only)</li>
      </ul>
    </li>

    <li><b>Step 6: t-SNE on MNIST</b>
      <ul>
        <!--
        <li>First reduce to ~<b>50 PCs</b> (e.g., <code>PCA(n_components=50)</code>) to speed/stabilize t-SNE</li>
        -->
        <li>Run t-SNE with perplexities <b>[5, 30, 50]</b>; use <code>TSNE(n_components=2, learning_rate="auto", random_state=42, n_iter=1000)</code></li>
        <li>Make a 1×3 subplot of the three perplexities, colored by <b>true digit</b></li>
        <li>Select the best perplexity and show a final high-quality 2D plot</li>
      </ul>
    </li>

    <li><b>Step 7: Write-Up (5–10 sentences)</b>
      <ul>
        <li>Elbow-based k and what it suggests about digit structure</li>
        <li>Visual differences between K-Means and Hierarchical results</li>
        <li>What t-SNE reveals vs PCA; effect of perplexity</li>
      </ul>
    </li>
  </ol>

  <h3>Expected Output</h3>
  <ul>
    <li>10 sample digit images and digit count histogram</li>
    <li>Elbow plot (k vs inertia) + chosen k</li>
    <li>Dendrograms (best linkages) on subsample</li>
    <li>PCA-2D colored by digit</li>
    <li>t-SNE perplexity exploration (1×3) + final best t-SNE</li>
    <li>5–10 sentence analysis</li>
  </ul>

  <h3>Grading Distribution (Total: 100 points)</h3>
  <table>
    <tr><th>Task</th><th>Points</th></tr>
    <tr><td>MNIST preprocessing & EDA (Step 1)</td><td>15</td></tr>
    <tr><td>Elbow method (plot & choice) (Step 2)</td><td>15</td></tr>
    <tr><td>K-Means viz & Hierarchical linkages (Step 3)</td><td>15</td></tr>
    <tr><td>Dendrograms (subsample) & comparison (Step 4)</td><td>15</td></tr>
    <tr><td>PCA variance analysis & PCA-2D (Step 5)</td><td>15</td></tr>
    <tr><td>t-SNE exploration & final plot (Step 6)</td><td>15</td></tr>
    <tr><td>Write-up (Step 7)</td><td>10</td></tr>
    <tr><th>Total</th><th>100</th></tr>
  </table>

  <h3>Starter Code Skeleton (MNIST Only)</h3>
  <pre><code># --- OLA5: Unsupervised Learning on MNIST (Clustering + DR) ---

from sklearn.datasets import fetch_openml

SEED = 42
np.random.seed(SEED)

# ===== Step 1: Load & Preprocess MNIST =====

# 1. Load MNIST (70,000 samples, 784 features)
X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)
y = y.astype(int)
# You can take a subset (for faster demo)
X, y = X[:5000], y[:5000]

# If you need DataFrame to these: Dataset shape; basic stats (mean, std, min, max)
# You can Create DataFrame for features
df_X = pd.DataFrame(X)
    
    
# Grid of 10 sample 28×28 digit images
# Check Demo from https://xinyangmtsu.github.io/3350/Nov-5.pdf


    
# Standardize features
# Please use StandardScaler() to scale your features X
    
    
# ===== Step 2: K-Means Elbow =====

# Reference: https://xinyangmtsu.github.io/3350/oct-27.pdf
    
# ---- Choose k based on the elbow plot ----
# For MNIST, a natural choice is often k = 10

    
# ===== Step 3: K-Means @ k_opt & Hierarchical =====

# --- K-Means with chosen k = 10 ---
#Hint here: Use k = 10 when creating kmeans_opt here.

    
#Get Lables and Centroids
kmeans_labels = kmeans_opt.fit_predict(X_scaled) #Labels
centroids = kmeans_opt.cluster_centers_ #Centroids

print("K-Means done. Centroids shape:", centroids.shape)

# --- Hierarchical (Agglomerative) clustering for different linkages ---
linkages_list = ["single", "complete", "average"]
#check demo from here: https://xinyangmtsu.github.io/3350/oct-31.pdf
    
# ===== Step 4: Dendrograms on Subsample (p=100) =====

#Reference: https://xinyangmtsu.github.io/3350/oct-31.pdf
dendrogram(Z, truncate_mode='lastp', p = 100, leaf_rotation = 90, leaf_font_size = 12)


    
# ===== Step 5: PCA Variance & PCA-2D by True Digit =====

pca_full = PCA(n_components=None)   # keep all components
X_pca_full = pca_full.fit_transform(X_scaled)
explained_var = pca_full.explained_variance_ratio_
cum_var = np.cumsum(explained_var)

print("Number of components:", len(explained_var))
    
# PCA-2D scatter colored by true digit
plt.figure(figsize=(6, 5))
plt.scatter(
    X_pca_full[:, 0], X_pca_full[:, 1],
    c=y, s=5, cmap="tab10"
)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA 2D Projection Colored by True Digit")
plt.tight_layout()
plt.show()
    
# ===== Step 6: t-SNE  =====

perplexities = [5, 30, 50]
tsne_results = {}

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for i, per in enumerate(perplexities):
    print(f"Running t-SNE with perplexity={per}...")
    #Please continue your code here
    #Reference: https://xinyangmtsu.github.io/3350/Nov-5.pdf

    
  </code></pre>

  <h3>Hints</h3>
  <ul>
    <li>Use labels only for coloring plots and qualitative comparison—not for clustering.</li>
    <li>Elbow selection is a judgment call: look for where inertia reduction noticeably tapers.</li>
    <li>Dendrograms: always subsample for readability and performance.</li>
  </ul>

  <footer>
    © Dr. Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennesse State University
  </footer>
</div>

<button id="backToTop" title="Go to top">↑</button>
<script>
  document.getElementById("backToTop").addEventListener("click", function() {
    window.scrollTo({ top: 0, behavior: "smooth" });
  });
</script>
</body>
</html>
