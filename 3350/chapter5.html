
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 5: Unsupervised Learning - Clustering</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
   body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      display: flex;
      background: #17191a;
      min-height: 100vh;
      color: #ff79c6; /* MAIN PINK FONT COLOR */
    }
    /* Sidebar */
    #sidebar {
      position: fixed;
      width: 250px;
      height: 100vh;
      overflow-y: auto;
      background: linear-gradient(135deg, #2a1e28 80%, #47223c 100%);
      color: #ffb3de;
      padding: 38px 24px 24px 28px;
      box-shadow: 2px 0 24px rgba(255,121,198,.12);
      z-index: 10;
    }
    #sidebar h2 {
      margin-top: 0;
      font-size: 1.1em;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #ffb3de;
      margin-bottom: 1.7em;
      text-align: left;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar .subsections {
      margin-left: 1.5em;
      font-size: 0.94em;
    }
    #sidebar .subsections a {
      font-size: 0.94em;
      margin: 8px 0 8px 12px;
      padding-left: 16px;
      color: #ffb3de;
      border-left: 2px solid transparent;
      font-weight: 400;
      box-shadow: none;
      background: none;
      gap: 0.6em;
    }
    #sidebar .subsections a:hover, #sidebar .subsections a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 2px solid #ff79c6;
      font-weight: 500;
    }
    #sidebar a {
      display: flex;
      align-items: center;
      color: #ffb3de;
      text-decoration: none;
      margin: 16px 0 16px 10px;
      font-weight: 500;
      font-size: 1.08em;
      border-left: 3px solid transparent;
      padding-left: 14px;
      transition: background .19s, color .19s, border .19s;
      border-radius: 8px 0 0 8px;
      gap: 0.7em;
    }
    #sidebar a:hover, #sidebar a.active {
      color: #ff79c6;
      background: #21111b;
      border-left: 3px solid #ff79c6;
      font-weight: 700;
      box-shadow: 1px 2px 8px 1px #2d3436;
    }
    /* Main content */
    #content {
      margin-left: 270px;
      padding: 56px 6vw 56px 6vw;
      max-width: 900px;
      width: 100vw;
      background: #1a1d1f;
    }
    h1 {
      color: #ff79c6;
      font-size: 2.5em;
      font-weight: 800;
      margin-bottom: 0.4em;
      letter-spacing: -1px;
      text-shadow: 0 2px 16px rgba(255,121,198,.18);
    }
    section {
      margin-bottom: 55px;
      background: #222025;
      border-radius: 18px;
      box-shadow: 0 4px 30px rgba(255,121,198,.09);
      padding: 36px 32px 22px 32px;
      transition: box-shadow .24s;
      border-left: 7px solid #ff79c6;
      position: relative;
    }
    section:hover {
      box-shadow: 0 10px 34px 3px rgba(255,121,198,0.13);
      border-left: 7px solid #ffb3de;
    }
    section h3 {
      border-bottom: 2px solid #ff79c6;
      padding-bottom: 8px;
      margin-top: 0;
      color: #ff79c6;
      font-size: 1.5em;
      font-weight: 700;
      letter-spacing: 0.5px;
      margin-bottom: 14px;
    }
    section h4 {
      font-size: 1.18em;
      color: #ffb3de;
      margin-bottom: 5px;
      margin-top: 30px;
      font-weight: 600;
      border-bottom: 1px solid #ffb3de;
      padding-bottom: 2px;
    }
    pre {
      background: #19121a;
      color: #ffb3de;
      padding: 15px 18px;
      border-radius: 8px;
      font-size: 1.04em;
      line-height: 1.7;
      overflow-x: auto;
      box-shadow: 0 2px 10px rgba(255,121,198,0.11);
    }
    ul {
      margin-left: 2.1em;
      margin-bottom: 0;
    }
    footer {
      margin-top: 46px;
      text-align: center;
      color: #ffb3de;
      font-size: 1.09em;
      letter-spacing: 1px;
      padding: 22px 0 14px 0;
      border-top: 1px solid #402138;
      background: none;
      font-family: 'Roboto', Arial, sans-serif;
      font-weight: 500;
    }
    #sidebar::-webkit-scrollbar {
      width: 7px;
      background: #47223c;
    }
    #sidebar::-webkit-scrollbar-thumb {
      background: #ff79c6;
      border-radius: 6px;
    }
    @media (max-width: 950px) {
      #sidebar {
        display: none;
      }
      #content {
        margin-left: 0;
        padding: 18px 4vw 30px 4vw;
      }
    }

    /* Links */
    a, a:visited {
      color: #ff79c6;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover, a:focus {
      color: #fff52e;
      background: #19121a;
      text-decoration: underline;
    }
    section#references a, section#references a:visited {
      color: #ff79c6;
      font-weight: 600;
    }
    section#references a:hover, section#references a:focus {
      color: #fff52e;
      background: #19121a;
    }
  </style>

  <!-- MathJax (once per page) -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-unsupervised"><i class="fas fa-robot"></i>1. What is Unsupervised Learning?</a></li>
    
    <li>
      <a href="#clustering"><i class="fas fa-layer-group"></i>2. Clustering</a>
      <div class="subsections">
        <a href="#kmeans">2.1 K-Means Clustering</a>
        <a href="#hierarchical">2.2 Hierarchical Clustering</a>
      </div>
    </li>
    
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">
  <h1>Chapter 5: Unsupervised Learning — Clustering</h1>

  <section id="what-is-unsupervised">
    <h2>1. What is Unsupervised Learning?</h2>
    <p>
      Unsupervised learning discovers <b><font color="red">patterns and structure</font></b> in data without labeled outputs.
      The algorithm only sees the features \( \mathbf{x} \) and tries to organize, compress, or represent them.
    </p>

    <h4>The goal:</h4>
    <ul>
      <li>Group similar examples (clustering), reduce dimensionality (PCA), or learn density/structure.</li>
      <li>Provide insights when labels are unavailable or expensive to obtain.</li>
    </ul>

    <h4>Analogy:</h4>
    <p>
      Imagine sorting a box of mixed LEGO pieces by shape and color without knowing what set they belong to.
      You create groups based on similarity — that’s clustering.
    </p>

    <h4>Main families:</h4>
    <ul>
       <li><b>Clustering</b> (K-Means, Hierarchical)</li>
       <li><b>Dimensionality Reduction</b> (PCA, ICA)</li>
    </ul>
  </section>

  <section id="clustering">
    <h2>2. Clustering</h2>
    <p>
      <b>Clustering</b> is a fundamental unsupervised learning task where the goal is to group similar data points 
      together without having labeled training data. Unlike supervised learning, where we predict a target variable, 
      clustering discovers inherent patterns and structures in data.
    </p>
    
    <p>
      Clustering methods partition data into groups (clusters) so that <b><font color="red">items in the same cluster are more similar</font></b>
      to each other than to items in other clusters. Similarity is defined via a distance or a density notion.
    </p>
    
    <p>
      Clustering is the task of partitioning a dataset into groups (called <b>clusters</b>) such that:
    </p>
    <ul>
      <li><b>Within-cluster similarity:</b> Points in the same cluster are similar to each other</li>
      <li><b>Between-cluster dissimilarity:</b> Points in different clusters are dissimilar</li>
    </ul>

    <h4>Key Differences from Classification</h4>
    <ul>
      <li><b>No Labels:</b> Clustering doesn't use labeled training data; classification does</li>
      <li><b>Discovery:</b> Clustering discovers unknown patterns; classification uses known categories</li>
      <li><b>Evaluation:</b> Clustering is harder to evaluate objectively; classification uses accuracy on labeled test data</li>
    </ul>

    <h4>Two common methods:</h4>
    
    <ul>
      <li><strong>Prototype-based:</strong> <font color="red">K-Means Clustering</font> minimizes within-cluster variance around centroids.</li>
      <li><strong>Hierarchy-based:</strong> <font color="red">Hierarchical clustering</font> builds a tree (dendrogram) of merges/splits.</li>
    </ul>
  </section>

  <section id="kmeans">
    <h4>2.1 K-Means Clustering</h4>
    <br>

    <b>Idea:</b>
    <p>
      Partition \(m\) points into \(k\) clusters, each represented by a centroid.
      Assign each point to its nearest centroid, then recompute centroids as the mean of their assigned points.
      Repeat until assignments stop changing. Goal: <b><font color="red">tight clusters, far apart</font></b>.
    </p>

    <b>Objective: Within-Cluster Sum of Squares (WCSS)</b>
    
    <p style="text-align:center;">
      $$
      \min_{\{\mathcal{C}_1,\ldots,\mathcal{C}_k\}} \;\sum_{j=1}^{k}\; \sum_{\mathbf{x}_i \in \mathcal{C}_j}
      \|\mathbf{x}_i - \boldsymbol{\mu}_j\|_2^2,
      \quad \text{with } \boldsymbol{\mu}_j = \frac{1}{|\mathcal{C}_j|} \sum_{\mathbf{x}_i \in \mathcal{C}_j} \mathbf{x}_i.
      $$
    </p>

    <p><strong>Where:</strong></p>
    <ul>
      <li>\(\mathcal{C}_j\) = set of points in cluster \(j\); \(\boldsymbol{\mu}_j\) = centroid of cluster \(j\).</li>
      <li>Distance is usually Euclidean; features should be on comparable scales.</li>
    </ul>

    <br>

    <h4>How K-Means Works</h4>
    <ol>
      <li><strong>Choose \(k\)</strong> (number of clusters) — use domain knowledge or diagnostics (Elbow Method).</li>
      <li><strong>Initialize centroids</strong> - (Randomly picks \(k\) data points as initial centroids.
       (<b>k-means++</b> for smarter seeding)).</li>
      <li><strong>Assignment step</strong> — assign each point to the nearest centroid.</li>
      <li><strong>Update step</strong> — recompute each centroid as the mean of its assigned points.</li>
      <li><strong>Repeat</strong> steps 3–4 until convergence (assignments stabilize or objective change is tiny).</li>
    </ol>

    <h4>Worked Example (2D)</h4>
    <p>
      Points: \((1,1), (1.5,2), (3,4), (5,7), (3.5,5), (4.5,5), (3.5,4.5)\). Let \(k=2\). Initialize centroids at \((1,1)\) and \((5,7)\).
    </p>
    <ol>
      <li><b>Assign:</b> \((1,1),(1.5,2)\) → cluster A; others → cluster B.</li>
      <li><b>Update:</b> \(\mu_A=\big(\tfrac{1+1.5}{2}, \tfrac{1+2}{2}\big)=(1.25,1.5)\);
        \(\mu_B=\big(\tfrac{3+5+3.5+4.5+3.5}{5}, \tfrac{4+7+5+5+4.5}{5}\big)=(3.9,5.1)\).</li>
      <li><b>Reassign:</b> memberships don’t change → <b>converged</b>.</li>
    </ol>

    <h4>Choosing \(k\)</h4>
    <ul>
      <li><b>Elbow:</b> plot within-cluster SSE vs \(k\); look for the “elbow”.</li>
      <!--
      <li><b>Silhouette:</b> \( s(i)=\dfrac{b(i)-a(i)}{\max\{a(i),b(i)\}} \), where \(a(i)\) is mean intra-cluster distance and \(b(i)\) is the best alternative cluster distance.</li>
      -->
    </ul>

    <h4>Interpretation &amp; Tips</h4>
    <ul>
      <li><b>Needs Feature Scaling:</b> K-Means uses Euclidean distance, which is sensitive to feature scale. Without scaling, features with larger ranges dominate the distance calculation.
      </li>
      <li><b>Shape assumption:</b> best for roughly spherical, equal-variance clusters.</li>
    </ul>
    
  </section>


  <section id="hierarchical">
    <h4>2.2 Hierarchical Clustering</h4>

    <b>Idea:</b>
    <p>

      <b>Hierarchical Clustering</b> is an <b>unsupervised</b> machine learning algorithm that 
    groups data points into a hierarchy of clusters — from individual points up to one large cluster, or vice versa.
    <br>

      It doesn’t require you to specify the number of clusters in advance (unlike K-Means) and produces a tree-like 
      structure called a <b>dendrogram</b>.
      <br>
      
      Goal: <b><font color="red">reveal data's hierarchical structure</font></b>.
    </p>
    <b>Objective: Minimize Linkage Distance</b>
    <p style="text-align:center;">
      $$
      d(\mathcal{C}_i, \mathcal{C}_j) = \begin{cases}
        \min_{\mathbf{x} \in \mathcal{C}_i, \mathbf{y} \in \mathcal{C}_j} \|\mathbf{x} - \mathbf{y}\|_2 & \text{(Single)} \\
        \max_{\mathbf{x} \in \mathcal{C}_i, \mathbf{y} \in \mathcal{C}_j} \|\mathbf{x} - \mathbf{y}\|_2 & \text{(Complete)} \\
        \frac{1}{|\mathcal{C}_i||\mathcal{C}_j|} \sum_{\mathbf{x} \in \mathcal{C}_i} \sum_{\mathbf{y} \in \mathcal{C}_j} \|\mathbf{x} - \mathbf{y}\|_2 & \text{(Average)}
      \end{cases}
      $$
    </p>
    <p><strong>Where:</strong></p>
    <ul>
      <li>\(\mathcal{C}_i, \mathcal{C}_j\) = two clusters being compared; \(d(\cdot,\cdot)\) = distance between clusters.</li>
      <li>Ward linkage minimizes within-cluster variance increase when merging.</li>
    </ul>
    <br>

    <h4>Types of Hierarchical Clustering</h4>
    <p>
      Hierarchical clustering comes in <b>two main types</b> — distinguished by the
      <b>direction</b> of building the cluster hierarchy:
    </p>

  <ul>
      <li>
        <b>1. Agglomerative (Bottom-Up):</b>
        <ul>
          <li>Starts with each data point as its own cluster.</li>
          <li>At each step, <b>merges the two closest clusters</b> based on a chosen linkage method (single, complete, average, or Ward’s).</li>
          <li>Continues until all data points form one big cluster.</li>
          <li>Produces a <b>dendrogram</b> that shows the hierarchy of merges.</li>
        </ul>
      </li>

      <li>
        <b>2. Divisive (Top-Down):</b>
        <ul>
          <li>Starts with all data points in a single cluster.</li>
          <li>At each step, <b>splits the cluster</b> with the largest internal dissimilarity into smaller sub-clusters.</li>
          <li>Continues until each data point becomes its own cluster or a stopping condition is met.</li>
          <li>More computationally expensive; used less often than agglomerative.</li>
        </ul>
      </li>
  </ul>

    
    <h4>How Hierarchical Clustering Works</h4>
    <ol>
      <li><strong>Initialize</strong> — each point starts as its own cluster.</li>
      <li><strong>Compute pairwise distances</strong> — calculate distances between all cluster pairs using chosen linkage.</li>
      <li><strong>Merge closest pair</strong> — combine the two clusters with minimum distance.</li>
      <li><strong>Update distance matrix</strong> — recalculate distances to the new merged cluster.</li>
      <li><strong>Repeat</strong> steps 2–4 until only one cluster remains, building the dendrogram.</li>
    </ol>

    
    <h4>Choosing Number of Clusters</h4>
    <ul>
      <li><b>Dendrogram cut:</b> visualize the tree and cut at height with longest vertical lines.</li>
      <li><b>Inconsistency:</b> look for large jumps in merge distances.</li>
    </ul>
    <h4>Interpretation &amp; Tips</h4>
    <ul>
      <li><b>No need to specify \(k\):</b> Unlike K-Means, hierarchical clustering reveals structure at all scales through the dendrogram.</li>
      <li><b>Linkage choice matters:</b> Single linkage can create elongated clusters; complete linkage prefers compact clusters; Ward minimizes within-cluster variance.</li>
    </ul>
</section>

  
  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://scikit-learn.org/stable/modules/clustering.html" target="_blank">
          scikit-learn: Clustering
        </a>
      </li>
      <li>
        <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank">
          The Elements of Statistical Learning (Hastie, Tibshirani, Friedman)
        </a>
      </li>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://cran.r-project.org/web/views/Cluster.html" target="_blank">
          CRAN Task View: Cluster Analysis &amp; Finite Mixture Models
        </a>
      </li>
      <li>Scholar GPT</li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
