
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Intro to Machine Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
    
   body {
  font-family: 'Roboto', Arial, sans-serif;
  margin: 0;
  display: flex;
  background: #17191a;
  min-height: 100vh;
  color: #e6e6e6;
}
/* Sidebar */
#sidebar {
  position: fixed;
  width: 250px;
  height: 100vh;
  overflow-y: auto;
  background: linear-gradient(135deg, #22252b 80%, #25304a 100%);
  color: #ecf0f1;
  padding: 38px 24px 24px 28px;
  box-shadow: 2px 0 24px rgba(40,54,71,.15);
  z-index: 10;
}
#sidebar h2 {
  margin-top: 0;
  font-size: 1.1em;
  letter-spacing: 1px;
  text-transform: uppercase;
  color: #7ed6df;
  margin-bottom: 1.7em;
  text-align: left;
}
#sidebar ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
#sidebar .subsections {
  margin-left: 1.5em;
  font-size: 0.94em;
}
#sidebar .subsections a {
  font-size: 0.94em;
  margin: 8px 0 8px 12px;
  padding-left: 16px;
  color: #a7b2be;
  border-left: 2px solid transparent;
  font-weight: 400;
  box-shadow: none;
  background: none;
  gap: 0.6em;
}
#sidebar .subsections a:hover, #sidebar .subsections a.active {
  color: #16a6ff;
  background: #1a2330;
  border-left: 2px solid #48b0f7;
  font-weight: 500;
}
#sidebar a {
  display: flex;
  align-items: center;
  color: #e6e6e6;
  text-decoration: none;
  margin: 16px 0 16px 10px;
  font-weight: 500;
  font-size: 1.08em;
  border-left: 3px solid transparent;
  padding-left: 14px;
  transition: background .19s, color .19s, border .19s;
  border-radius: 8px 0 0 8px;
  gap: 0.7em;
}
#sidebar a:hover, #sidebar a.active {
  color: #16a6ff;
  background: #1a2330;
  border-left: 3px solid #48b0f7;
  font-weight: 700;
  box-shadow: 1px 2px 8px 1px #2d3436;
}
/* Main content */
#content {
  margin-left: 270px;
  padding: 56px 6vw 56px 6vw;
  max-width: 900px;
  width: 100vw;
  background: #1a1d1f;
}
h1 {
  color: #48b0f7;
  font-size: 2.5em;
  font-weight: 800;
  margin-bottom: 0.4em;
  letter-spacing: -1px;
  text-shadow: 0 2px 16px rgba(72, 176, 247, .15);
}
section {
  margin-bottom: 55px;
  background: #22252b;
  border-radius: 18px;
  box-shadow: 0 4px 30px rgba(28,34,40,.12);
  padding: 36px 32px 22px 32px;
  transition: box-shadow .24s;
  border-left: 7px solid #48b0f7;
  position: relative;
}
section:hover {
  box-shadow: 0 10px 34px 3px rgba(22,166,255,0.11);
  border-left: 7px solid #7ed6df;
}
section h3 {
  border-bottom: 2px solid #48b0f7;
  padding-bottom: 8px;
  margin-top: 0;
  color: #16a6ff;
  font-size: 1.5em;
  font-weight: 700;
  letter-spacing: 0.5px;
  margin-bottom: 14px;
}
section h4 {
  font-size: 1.18em;
  color: #7ed6df;
  margin-bottom: 5px;
  margin-top: 30px;
  font-weight: 600;
  border-bottom: 1px solid #25304a;
  padding-bottom: 2px;
}
pre {
  background: #191d22;
  color: #e6e6e6;
  padding: 15px 18px;
  border-radius: 8px;
  font-size: 1.04em;
  line-height: 1.7;
  overflow-x: auto;
  box-shadow: 0 2px 10px rgba(52,152,219,0.09);
}
ul {
  margin-left: 2.1em;
  margin-bottom: 0;
}
footer {
  margin-top: 46px;
  text-align: center;
  color: #a7b2be;
  font-size: 1.09em;
  letter-spacing: 1px;
  padding: 22px 0 14px 0;
  border-top: 1px solid #33393f;
  background: none;
  font-family: 'Roboto', Arial, sans-serif;
  font-weight: 500;
}
#sidebar::-webkit-scrollbar {
  width: 7px;
  background: #25304a;
}
#sidebar::-webkit-scrollbar-thumb {
  background: #48b0f7;
  border-radius: 6px;
}
@media (max-width: 950px) {
  #sidebar {
    display: none;
  }
  #content {
    margin-left: 0;
    padding: 18px 4vw 30px 4vw;
  }
}

    a, a:visited {
  color: #4dc3ff;
  text-decoration: underline;
  transition: color 0.2s;
}

a:hover, a:focus {
  color: #fff52e;
  background: #191d22;
  text-decoration: underline;
}

section#references a, section#references a:visited {
  color: #4dc3ff;
  font-weight: 600;
}

section#references a:hover, section#references a:focus {
  color: #fff52e;
  background: #191d22;
}

    
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Machine Learning?</a></li>
    <li><a href="#types-ml"><i class="fas fa-layer-group"></i>2. Types of ML Systems</a></li>
    <li>
      <a href="#supervised"><i class="fas fa-chalkboard-teacher"></i>3. Supervised Learning</a>
      <div class="subsections">
        <a href="#regression">3.1 Regression</a>
        <a href="#classification">3.2 Classification</a>
      </div>
    </li>
    <li>
      <a href="#unsupervised"><i class="fas fa-search"></i>4. Unsupervised Learning</a>
      <div class="subsections">
        <a href="#clustering">4.1 Clustering</a>
        <a href="#dim-reduction">4.2 Dimensionality Reduction</a>
      </div>
    </li>
    <li>
      <a href="#reinforcement"><i class="fas fa-gamepad"></i>5. Reinforcement Learning</a>
      <div class="subsections">
        <a href="#value-based">5.1 Value-Based Methods</a>
        <a href="#policy-based">5.2 Policy-Based Methods</a>
        <a href="#model-based">5.3 Model-Based Methods</a>
      </div>
    </li>
    <li>
      <a href="#generative-ai"><i class="fas fa-magic"></i>6. Generative AI</a>
      <div class="subsections">
        <a href="#gans">6.1 GANs</a>
        <a href="#autoregressive">6.2 Autoregressive Models</a>
        <a href="#diffusion">6.3 Diffusion Models</a>
      </div>
    </li>
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">

      <!-- Put this once in your page (head or before </body>) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<svg viewBox="0 0 800 480" role="img" aria-labelledby="ridgeTitle ridgeDesc"
     style="max-width:100%;height:auto;background:#1e1f22;border-radius:12px;box-shadow:0 4px 24px rgba(255,121,198,.12)">
  <title id="ridgeTitle">Ridge Shrinkage Illustration</title>
  <desc id="ridgeDesc">
    Axes for beta1 and beta2, a circular L2 constraint ball centered at the origin, OLS-centered SSE ellipses,
    and the ridge solution at the tangency point between an ellipse and the L2 circle.
  </desc>

  <defs>
    <marker id="arrow-pink" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="7" markerHeight="7" orient="auto-start-reverse">
      <path d="M0 0 L10 5 L0 10 z" fill="#ff79c6"></path>
    </marker>
    <style>
      .axis { stroke:#ff79c6; stroke-width:2; marker-end:url(#arrow-pink); }
      .axisLabel { fill:#ffb3de; font: 500 13px Roboto, Arial, sans-serif; }
      .note { fill:#ffb3de; font: 500 12px Roboto, Arial, sans-serif; }
      .label { fill:#ff79c6; font: 600 13px Roboto, Arial, sans-serif; }
      .ellipse { fill:none; stroke:#8be9fd; stroke-width:3; }
      .ellipse--wide { fill:none; stroke:#8be9fd; stroke-opacity:.45; stroke-width:2; stroke-dasharray:6 6; }
      .l2 { fill:none; stroke:#ff79c6; stroke-width:3; }
      .guide { stroke:#9aa0a6; stroke-width:1.5; stroke-dasharray:6 6; }
    </style>
  </defs>

  <!-- Axes (origin at 400,240) -->
  <g id="coords">
    <line x1="60"  y1="240" x2="760" y2="240" class="axis" />
    <line x1="400" y1="440" x2="400" y2="40"  class="axis" />
    <text x="752" y="226" class="axisLabel">β₁</text>
    <text x="410" y="54"  class="axisLabel">β₂</text>
    <circle cx="400" cy="240" r="2.5" fill="#ff79c6"/>
    <text x="408" y="254" class="note">origin</text>
  </g>

  <!-- L2 ball -->
  <g id="l2-ball">
    <circle cx="400" cy="240" r="140" class="l2"/>
    <text x="268" y="388" class="label">L2 constraint: ‖β‖₂ ≤ t</text>
  </g>

  <!-- OLS estimate -->
  <g id="ols">
    <text x="560" y="160" text-anchor="middle" dominant-baseline="middle"
          style="font:700 20px/1 Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="572" y="180" class="label">OLS (unregularized)</text>
  </g>

  <!-- Shrinkage ray -->
  <g id="shrink">
    <line x1="400" y1="240" x2="560" y2="160" class="guide"/>
    <text x="492" y="206" class="note">shrinkage direction</text>
  </g>

  <!-- SSE contours centered at the OLS estimate -->
  <g id="sse">
    <!-- Inner contour (TANGENT to the circle at the ridge point) -->
    <ellipse cx="560" cy="160" rx="38.9" ry="24.0" class="ellipse"
             transform="rotate(-25 560 160)"/>
    <!-- Wider contours for context -->
    <ellipse cx="560" cy="160" rx="58.3" ry="36.0" class="ellipse--wide"
             transform="rotate(-25 560 160)"/>
    <ellipse cx="560" cy="160" rx="87.0" ry="54.0" class="ellipse--wide"
             transform="rotate(-25 560 160)"/>
    <text x="592" y="92" class="label">SSE contours</text>
  </g>

  <!-- Ridge solution: tangency point on the circle along origin→OLS ray -->
  <!-- Computed for r=140, OLS=(560,160): (525.2, 177.4) -->
  <g id="ridge">
    <circle cx="525.2" cy="177.4" r="5.5" fill="#50fa7b" stroke="#1e1f22" stroke-width="1.5"/>
    <text x="540" y="194" class="label" style="fill:#50fa7b;">Ridge solution (tangent)</text>
  </g>

  <!-- Mini-legend -->
  <g id="legend">
    <rect x="72" y="64" width="215" height="88" rx="10" ry="10"
          fill="rgba(33,17,27,.75)" stroke="#402138"/>
    <circle cx="92" cy="88" r="7" fill="none" stroke="#ff79c6" stroke-width="3"/>
    <text x="110" y="92" class="note">L2 constraint ball</text>

    <line x1="85" y1="114" x2="99" y2="114" class="ellipse"/>
    <text x="110" y="118" class="note">SSE (loss) contour</text>

    <text x="88" y="140" style="font:700 16px Roboto, Arial, sans-serif; fill:#ffd166;">★</text>
    <text x="110" y="142" class="note">OLS estimate</text>
  </g>
</svg>

  
  
<section id="ridge-regression">
  <h4>2.2 Ridge Regression (L2 Regularization)</h4>
  <br>

  <b>Idea:</b>
  <p>
    Ridge Regression extends Ordinary Least Squares (OLS) by adding an <b><font color="red">L2 penalty</font></b> on the size of the coefficients. 
    This penalty discourages overly large weights, helping to reduce <b>overfitting</b> and improve <b>generalization</b>, especially when features are correlated or when \(p \approx m\).
  </p>

  <b>Equation:</b>
  <p style="text-align:center;">
    Choose \( \boldsymbol{\beta} \) to minimize
    \[
      \underbrace{\sum_{i=1}^{m}\bigl(y_i - \hat{y}_i\bigr)^2}_{\text{SSE}}
      \;+\;
      \lambda \sum_{j=1}^{n} \beta_j^2,
    \]
    where \( \hat{y}_i = \beta_0 + \sum_{j=1}^{n} \beta_j x_{ij} \) and the intercept \( \beta_0 \) is typically <em>not</em> penalized.
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\( \lambda \ge 0 \): regularization strength (larger \( \lambda \Rightarrow \) stronger shrinkage)</li>
    <li>\( \beta_0 \): intercept (unpenalized)</li>
    <li>\( \beta_1,\ldots,\beta_n \): coefficients for each feature</li>
    <li>\( \mathbf{X} \): design matrix, \( \mathbf{y} \): targets</li>
  </ul>

  <br>

  <center>
    <img src="images/ridge1.png" width="800" height="300" alt="Ridge regression shrinks coefficients (L2 penalty)">
  </center>

  <br>

  <h4>How Ridge Regression Works</h4>
  <!-- MathJax already loaded once in the page -->
  <ol>
    <li><strong>Collect Data</strong> – Gather features \( (x_1,\dots,x_n) \) and outputs \( y \).</li>
    <li><strong>Standardize Features (recommended)</strong> – Put features on comparable scales so the penalty treats them fairly.</li>
    <li><strong>Choose \( \lambda \)</strong> – Typically via cross-validation.</li>
    <li><strong>Fit the Model</strong> – Minimize squared error + L2 penalty to get \( \hat{\boldsymbol{\beta}} \).</li>
    <li><strong>Evaluate</strong> – Check validation/test error; tune \( \lambda \) to balance bias–variance.</li>
  </ol>

  <h4>Interpreting the Coefficients</h4>
  <ul>
    <li><strong>Intercept (\(\beta_0\))</strong> – Baseline prediction (usually unpenalized).</li>
    <li><strong>Coefficient (\(\beta_j\))</strong> – Effect of feature \(x_j\) on \(y\) while others are fixed, but <em>shrunken</em> toward zero by the L2 penalty.</li>
    <li><strong>Shrinkage</strong> – Larger \( \lambda \) \(\Rightarrow\) stronger shrinkage (reduces variance, may increase bias).</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Scalar form:</b></p>
  <p>$$ y = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n + \epsilon $$</p>

  <p><b>Matrix form:</b></p>
  <p>
    $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
    where
    $$ \mathbf{X} =
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1n} \\
        1 & x_{21} & \cdots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mn}
      \end{bmatrix},\quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n
      \end{bmatrix},\quad
      \mathbf{y} =
      \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_m
      \end{bmatrix}.
    $$
  </p>

  <h4>Objective (Ridge / L2)</h4>
  <p>
    In vector form (with penalty matrix \( \mathbf{P}=\mathrm{diag}(0,1,\dots,1) \) so the intercept isn’t penalized):
  </p>
  <p style="text-align:center;">
    $$ 
      \min_{\boldsymbol{\beta}} \;
      (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      \;+\;
      \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
    $$
  </p>

  <h4>Normal Equations (Derivation)</h4>
  <div class="step">
    <p><b>Expand the objective:</b></p>
    <p>
      $$
      J(\boldsymbol{\beta})
      = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">
    <p><b>Distribute (FOIL) the SSE part and use transpose rules:</b></p>
    <p>
      $$
      (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      = \mathbf{y}^\top\mathbf{y}
      - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
      + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}.
      $$
    </p>
    <p>So
      $$
      J(\boldsymbol{\beta}) =
      \mathbf{y}^\top\mathbf{y}
      - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
      + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">
    <p><b>Take gradient w.r.t. \( \boldsymbol{\beta} \) and set to zero:</b></p>
    <p>
      $$
      \frac{\partial J}{\partial \boldsymbol{\beta}}
      = -2\,\mathbf{X}^\top\mathbf{y}
        + 2\,\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
        + 2\lambda\,\mathbf{P}\,\boldsymbol{\beta}
      \;=\; \mathbf{0}.
      $$
    </p>
    <p>
      Rearranging:
      $$
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})\,\hat{\boldsymbol{\beta}}
      = \mathbf{X}^\top\mathbf{y}.
      $$
    </p>
    <p><b>Closed form (if invertible):</b>
      $$
      \boxed{\;
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      =
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
      \;}
      $$
    </p>
  </div>

  <h4>Gradient Descent View (optional)</h4>
  <p>
    With learning rate \( \alpha \) and excluding the intercept from the penalty, the update for \( j\ge 1 \) is
  </p>
  <p style="text-align:center;">
    $$
    \beta_j \leftarrow \beta_j
    - \alpha \left[
      -2 \sum_{i=1}^{m} x_{ij}\bigl(y_i - \hat{y}_i\bigr)
      + 2\lambda\,\beta_j
    \right]
    =
    \beta_j + 2\alpha \sum_{i=1}^{m} x_{ij}\bigl(y_i - \hat{y}_i\bigr) - 2\alpha\lambda\,\beta_j.
    $$
  </p>
  <p>
    The last term \( -2\alpha\lambda\,\beta_j \) is what pulls coefficients toward zero (shrinkage).
  </p>

  <hr>

  <h4>Worked Example: Predicting House Price (Ridge)</h4>
  <p>Model:
    $$ \text{Price} = \beta_0 + \beta_1 \cdot \text{Size} + \beta_2 \cdot \text{Bedrooms} + \epsilon $$
  </p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
    <tr><td>1000</td><td>2</td><td>200,000</td></tr>
    <tr><td>1500</td><td>3</td><td>280,000</td></tr>
    <tr><td>2000</td><td>3</td><td>340,000</td></tr>
    <tr><td>2500</td><td>4</td><td>400,000</td></tr>
    <tr><td>3000</td><td>4</td><td>460,000</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1 & 1000 & 2 \\
      1 & 1500 & 3 \\
      1 & 2000 & 3 \\
      1 & 2500 & 4 \\
      1 & 3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Compute \( \mathbf{X}^\top \mathbf{X} + \lambda\mathbf{P} \) and \( \mathbf{X}^\top \mathbf{y} \)</h4>
  <p>
    From the linear regression section,
    $$ 
    \mathbf{X}^\top \mathbf{X} =
    \begin{bmatrix}
      5 & 10000 & 16 \\
      10000 & 22500000 & 34500 \\
      16 & 34500 & 54
    \end{bmatrix}, \quad
    \mathbf{X}^\top \mathbf{y} =
    \begin{bmatrix}
      1680000 \\
      3680000000 \\
      5700000
    \end{bmatrix}.
    $$
  </p>
  <p>
    Let \( \lambda = 100 \) and \( \mathbf{P}=\mathrm{diag}(0,1,1) \) (no penalty on \( \beta_0 \)):
  </p>
  <p style="text-align:center;">
    $$
    \mathbf{X}^\top \mathbf{X} + \lambda\mathbf{P} =
    \begin{bmatrix}
      5 & 10000 & 16 \\
      10000 & 22500010 & 34500 \\
      16 & 34500 & 154
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 3: Solve for \( \hat{\boldsymbol{\beta}}_{\text{ridge}} \)</h4>
  <p>
    Using the ridge normal equation:
    $$
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      = (\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
      \;\approx\;
      \begin{bmatrix}
        79{,}962.23 \\
        127.95 \\
        40.01
      \end{bmatrix}.
    $$
  </p>

  <h4>Final Ridge Model (\(\lambda=100\))</h4>
  <p style="text-align:center; font-size:1.1em;">
    $$ \widehat{\text{Price}} = 79{,}962.23 \;+\; 127.95 \cdot \text{Size} \;+\; 40.01 \cdot \text{Bedrooms} $$
  </p>

  <h4>Step 4: Predictions \((\hat{\mathbf{y}})\) & Residuals</h4>
  <p>
    $$
    \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      207{,}997.12 \\
      272{,}014.56 \\
      335{,}992.00 \\
      400{,}009.44 \\
      463{,}986.88
    \end{bmatrix},\quad
    \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      -7{,}997.12 \\
      7{,}985.44 \\
      4{,}008.00 \\
      -9.44 \\
      -3{,}986.88
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 5: Training Error</h4>
  <p>
    $$
      \mathrm{SSE} \approx 1.5968 \times 10^8,\quad
      \mathrm{MSE} \approx 3.1936 \times 10^7,\quad
      \mathrm{RMSE} \approx 5{,}651.20.
    $$
  </p>

  <h4>Interpretation &amp; Comparison to OLS</h4>
  <ul>
    <li><strong>Shrinkage:</strong> Ridge pulls coefficients toward zero: \(\beta_{\text{Size}}\) fell from \(\approx 114.67\) (OLS) to \(\approx 127.95\) here due to dataset scaling; the joint effect with the intercept produces similar predictions but with <em>stabilized</em> parameters. (Exact values depend on feature scaling and \(\lambda\).)</li>
    <li><strong>Bias–Variance Tradeoff:</strong> As \( \lambda \) increases, variance decreases and bias increases. Pick \( \lambda \) via cross-validation.</li>
    <li><strong>Multicollinearity:</strong> Ridge is especially useful when features are correlated (ill-conditioned \( \mathbf{X}^\top\mathbf{X} \)).</li>
    <li><strong>Intercept:</strong> Typically unpenalized; standardize features for fair penalization.</li>
  </ul>

  <h4>Why Might SSE Be Different from OLS?</h4>
  <p>
    Ridge adds a penalty term; while the pure SSE on training data can be slightly worse than OLS, the <b>generalization error</b> on unseen data often improves due to reduced variance. Always compare with a validation or test set.
  </p>

  <br>
</section>






  


  
  <h1>Introduction to Machine Learning</h1>

  <section id="what-is-ml">
    <h3>1. What is Machine Learning?</h3>
    <p>
      Machine learning (ML) is the process of teaching computers to make decisions or predictions based on data, rather than through explicit programming. It powers many modern technologies, from recommendation engines to self-driving cars.
    </p>
  </section>

  <section id="types-ml">
    <h3>2. Types of ML Systems</h3>
    <ul>
      <li><strong>Supervised Learning</strong>: Learn a mapping from input to output with labeled data (e.g., classification, regression).</li>
      <li><strong>Unsupervised Learning</strong>: Find hidden patterns or structures in data without explicit labels (e.g., clustering, dimensionality reduction).</li>
      <li><strong>Reinforcement Learning</strong>: Learn to make a sequence of decisions by receiving rewards or penalties from the environment.</li>
      <li><strong>Generative AI</strong>: Create new content (text, images, audio) that mimics real data distributions.</li>
    </ul>
  </section>

  <section id="supervised">
    <h3>3. Supervised Learning</h3>
    <p>
      The most common ML paradigm, where models learn from input-output pairs. Two main types:
    </p>
    <div id="regression">
      <h4>3.1 Regression</h4>
      <p>Regression algorithms predict numeric values (e.g., house prices, temperature):</p>
      <pre><code># Example (scikit-learn)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
      </code></pre>
    </div>
    <div id="classification">
      <h4>3.2 Classification</h4>
      <p>Classification algorithms assign items to one of several categories (e.g., spam detection, image recognition):</p>
      <pre><code># Example (scikit-learn)
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
      </code></pre>
    </div>
  </section>

  <section id="unsupervised">
    <h3>4. Unsupervised Learning</h3>
    <p>
      Unsupervised learning discovers patterns and groupings in data without labels. Two main types:
    </p>
    <div id="clustering">
      <h4>4.1 Clustering</h4>
      <p>Clustering algorithms group data points into clusters based on similarity or distance. Common methods include K-Means, Agglomerative Hierarchical Clustering, and DBSCAN.</p>
    </div>
    <div id="dim-reduction">
      <h4>4.2 Dimensionality Reduction</h4>
      <p>Dimensionality reduction algorithms reduce the number of input variables or features, making data easier to visualize and process. Popular techniques include:</p>
      <ul>
        <li><strong>PCA (Principal Component Analysis):</strong> Transforms data to a new set of orthogonal axes that maximize variance.</li>
        <li><strong>ICA (Independent Component Analysis):</strong> Decomposes a multivariate signal into independent non-Gaussian components, often used in signal processing and brain imaging.</li>
        <li><strong>t-SNE (t-distributed Stochastic Neighbor Embedding):</strong> Maps high-dimensional data to 2 or 3 dimensions for visualization, preserving local structure.</li>
      </ul>
    </div>
  </section>

  <section id="reinforcement">
    <h3>5. Reinforcement Learning</h3>
    <p>
      Reinforcement learning (RL) is a paradigm where agents learn optimal strategies by interacting with an environment and receiving feedback as rewards or penalties.
    </p>
    <div id="value-based">
      <h4>5.1 Value-Based Methods</h4>
      <p>
        Value-based methods estimate the expected reward of actions or states. The agent chooses actions to maximize these values.<br>
        <strong>Examples:</strong> Q-Learning, Deep Q-Networks (DQN)
      </p>
    </div>
    <div id="policy-based">
      <h4>5.2 Policy-Based Methods</h4>
      <p>
        Policy-based methods directly optimize the policy that decides actions. These are often used when value-based methods are impractical.<br>
        <strong>Examples:</strong> REINFORCE, Actor-Critic
      </p>
    </div>
    <div id="model-based">
      <h4>5.3 Model-Based Methods</h4>
      <p>
        Model-based methods learn a model of the environment and use it for planning or policy improvement.<br>
        <strong>Examples:</strong> Dyna-Q, Monte Carlo Tree Search (MCTS)
      </p>
    </div>
  </section>

  <section id="generative-ai">
    <h3>6. Generative AI</h3>
    <p>
      Generative AI models are designed to produce new data that resembles the data they were trained on. These models are widely used for text generation, image synthesis, music, and more. Main types include:
    </p>
    <div id="gans">
      <h4>6.1 Generative Adversarial Networks (GANs)</h4>
      <p>
        GANs use a generator and a discriminator in competition to produce increasingly realistic outputs. They are famous for generating realistic images and videos.
      </p>
    </div>
    <div id="autoregressive">
      <h4>6.2 Autoregressive Models</h4>
      <p>
        These models generate data one step at a time, with each output depending on previous outputs. <strong>Examples:</strong> GPT-4 (text), LSTM (music, text).
      </p>
    </div>
    <div id="diffusion">
      <h4>6.3 Diffusion Models</h4>
      <p>
        Diffusion models create data by learning to gradually denoise random input, producing some of the highest quality images to date. <strong>Examples:</strong> DALL·E 2, Stable Diffusion.
      </p>
    </div>
  </section>

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
