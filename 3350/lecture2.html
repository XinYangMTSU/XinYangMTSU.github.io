
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Intro to Machine Learning</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <style>
    
   body {
  font-family: 'Roboto', Arial, sans-serif;
  margin: 0;
  display: flex;
  background: #17191a;
  min-height: 100vh;
  color: #e6e6e6;
}
/* Sidebar */
#sidebar {
  position: fixed;
  width: 250px;
  height: 100vh;
  overflow-y: auto;
  background: linear-gradient(135deg, #22252b 80%, #25304a 100%);
  color: #ecf0f1;
  padding: 38px 24px 24px 28px;
  box-shadow: 2px 0 24px rgba(40,54,71,.15);
  z-index: 10;
}
#sidebar h2 {
  margin-top: 0;
  font-size: 1.1em;
  letter-spacing: 1px;
  text-transform: uppercase;
  color: #7ed6df;
  margin-bottom: 1.7em;
  text-align: left;
}
#sidebar ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
#sidebar .subsections {
  margin-left: 1.5em;
  font-size: 0.94em;
}
#sidebar .subsections a {
  font-size: 0.94em;
  margin: 8px 0 8px 12px;
  padding-left: 16px;
  color: #a7b2be;
  border-left: 2px solid transparent;
  font-weight: 400;
  box-shadow: none;
  background: none;
  gap: 0.6em;
}
#sidebar .subsections a:hover, #sidebar .subsections a.active {
  color: #16a6ff;
  background: #1a2330;
  border-left: 2px solid #48b0f7;
  font-weight: 500;
}
#sidebar a {
  display: flex;
  align-items: center;
  color: #e6e6e6;
  text-decoration: none;
  margin: 16px 0 16px 10px;
  font-weight: 500;
  font-size: 1.08em;
  border-left: 3px solid transparent;
  padding-left: 14px;
  transition: background .19s, color .19s, border .19s;
  border-radius: 8px 0 0 8px;
  gap: 0.7em;
}
#sidebar a:hover, #sidebar a.active {
  color: #16a6ff;
  background: #1a2330;
  border-left: 3px solid #48b0f7;
  font-weight: 700;
  box-shadow: 1px 2px 8px 1px #2d3436;
}
/* Main content */
#content {
  margin-left: 270px;
  padding: 56px 6vw 56px 6vw;
  max-width: 900px;
  width: 100vw;
  background: #1a1d1f;
}
h1 {
  color: #48b0f7;
  font-size: 2.5em;
  font-weight: 800;
  margin-bottom: 0.4em;
  letter-spacing: -1px;
  text-shadow: 0 2px 16px rgba(72, 176, 247, .15);
}
section {
  margin-bottom: 55px;
  background: #22252b;
  border-radius: 18px;
  box-shadow: 0 4px 30px rgba(28,34,40,.12);
  padding: 36px 32px 22px 32px;
  transition: box-shadow .24s;
  border-left: 7px solid #48b0f7;
  position: relative;
}
section:hover {
  box-shadow: 0 10px 34px 3px rgba(22,166,255,0.11);
  border-left: 7px solid #7ed6df;
}
section h3 {
  border-bottom: 2px solid #48b0f7;
  padding-bottom: 8px;
  margin-top: 0;
  color: #16a6ff;
  font-size: 1.5em;
  font-weight: 700;
  letter-spacing: 0.5px;
  margin-bottom: 14px;
}
section h4 {
  font-size: 1.18em;
  color: #7ed6df;
  margin-bottom: 5px;
  margin-top: 30px;
  font-weight: 600;
  border-bottom: 1px solid #25304a;
  padding-bottom: 2px;
}
pre {
  background: #191d22;
  color: #e6e6e6;
  padding: 15px 18px;
  border-radius: 8px;
  font-size: 1.04em;
  line-height: 1.7;
  overflow-x: auto;
  box-shadow: 0 2px 10px rgba(52,152,219,0.09);
}
ul {
  margin-left: 2.1em;
  margin-bottom: 0;
}
footer {
  margin-top: 46px;
  text-align: center;
  color: #a7b2be;
  font-size: 1.09em;
  letter-spacing: 1px;
  padding: 22px 0 14px 0;
  border-top: 1px solid #33393f;
  background: none;
  font-family: 'Roboto', Arial, sans-serif;
  font-weight: 500;
}
#sidebar::-webkit-scrollbar {
  width: 7px;
  background: #25304a;
}
#sidebar::-webkit-scrollbar-thumb {
  background: #48b0f7;
  border-radius: 6px;
}
@media (max-width: 950px) {
  #sidebar {
    display: none;
  }
  #content {
    margin-left: 0;
    padding: 18px 4vw 30px 4vw;
  }
}

    a, a:visited {
  color: #4dc3ff;
  text-decoration: underline;
  transition: color 0.2s;
}

a:hover, a:focus {
  color: #fff52e;
  background: #191d22;
  text-decoration: underline;
}

section#references a, section#references a:visited {
  color: #4dc3ff;
  font-weight: 600;
}

section#references a:hover, section#references a:focus {
  color: #fff52e;
  background: #191d22;
}

    
  </style>
</head>
<body>
<nav id="sidebar">
  <h2><i class="fas fa-book"></i> Contents</h2>
  <ul>
    <li><a href="#what-is-ml"><i class="fas fa-robot"></i>1. What is Machine Learning?</a></li>
    <li><a href="#types-ml"><i class="fas fa-layer-group"></i>2. Types of ML Systems</a></li>
    <li>
      <a href="#supervised"><i class="fas fa-chalkboard-teacher"></i>3. Supervised Learning</a>
      <div class="subsections">
        <a href="#regression">3.1 Regression</a>
        <a href="#classification">3.2 Classification</a>
      </div>
    </li>
    <li>
      <a href="#unsupervised"><i class="fas fa-search"></i>4. Unsupervised Learning</a>
      <div class="subsections">
        <a href="#clustering">4.1 Clustering</a>
        <a href="#dim-reduction">4.2 Dimensionality Reduction</a>
      </div>
    </li>
    <li>
      <a href="#reinforcement"><i class="fas fa-gamepad"></i>5. Reinforcement Learning</a>
      <div class="subsections">
        <a href="#value-based">5.1 Value-Based Methods</a>
        <a href="#policy-based">5.2 Policy-Based Methods</a>
        <a href="#model-based">5.3 Model-Based Methods</a>
      </div>
    </li>
    <li>
      <a href="#generative-ai"><i class="fas fa-magic"></i>6. Generative AI</a>
      <div class="subsections">
        <a href="#gans">6.1 GANs</a>
        <a href="#autoregressive">6.2 Autoregressive Models</a>
        <a href="#diffusion">6.3 Diffusion Models</a>
      </div>
    </li>
    <li><a href="#references"><i class="fas fa-link"></i>References</a></li>
  </ul>
</nav>

<main id="content">

      <!-- Put this once in your page (head or before </body>) -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


  <section id="logistic-regression">
    
    <h4>2.1 Logistic Regression</h4>

    <!-- 1. Motivation -->
    <h3>1. Motivation</h3>
    <div class="card">
      <p><strong>Problem with Linear Regression for classification:</strong></p>
      <ul>
        <li>Linear Regression can predict any real number (e.g., −3.2 or 2.7).</li>
        <li>But classification tasks need <em>Yes/No</em> outputs (Admit/Reject, Spam/Not Spam, Disease/No Disease).</li>
        <li>We need outputs between <strong>0 and 1</strong> so we can interpret them as <strong>probabilities</strong>.</li>
      </ul>
    </div>

    <!-- 2. Idea -->
    <h3>2. Idea</h3>
    <p>Logistic Regression takes the linear model from Linear Regression:</p>
    <p class="eq">
      $$ z \;=\; \beta_0 \;+\; \beta_1 x_1 \;+\; \cdots \;+\; \beta_p x_p $$
    </p>
    <p>and passes it through the <em>sigmoid</em> (logistic) function:</p>
    <p class="eq">
      $$ \sigma(z) \;=\; \frac{1}{1 + e^{-z}} $$
    </p>
    <ul>
      <li>Output is always in \((0,1)\).</li>
      <li>Can be interpreted as the probability of class \(1\).</li>
    </ul>

    <!-- 3. Equation -->
    <h3>3. Equation</h3>
    <p class="eq">
      $$ P(y=1 \mid x) \;=\; \frac{1}{\,1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}\,} $$
    </p>
    <p class="center small">If the probability \(\ge 0.5\) → predict \(1\); otherwise \(0\).</p>

    <!-- 4. Interpretation -->
    <h3>4. Interpretation of Coefficients</h3>
    <p>Each coefficient \(\beta_j\) changes the <strong>log-odds</strong> of the outcome:</p>
    <p class="eq">
      $$ \log\!\left(\frac{p}{1-p}\right) \;=\; \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$
    </p>
    <p>If you exponentiate \(\beta_j\), you get an <strong>odds ratio</strong>:</p>
    <ul>
      <li>\(e^{\beta_j} &gt; 1\): increases chance of the event.</li>
      <li>\(e^{\beta_j} &lt; 1\): decreases chance of the event.</li>
    </ul>

    <!-- 5. Objective -->
    <h3>5. Objective Function (How the Model Learns)</h3>
    <p>Instead of minimizing squared error, Logistic Regression uses <strong>Maximum Likelihood Estimation (MLE)</strong>.</p>
    <p class="eq">
      $$ \ell(\boldsymbol{\beta}) \;=\; \sum_{i=1}^{m} \Big( y_i \log p_i \;+\; (1 - y_i)\log(1 - p_i) \Big), \quad p_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}) $$
    </p>
    <p><strong>Optimization:</strong> use <em>Gradient Descent</em> (or Newton-type methods) to find the best \(\boldsymbol{\beta}\).</p>

    <!-- 6. Example -->
    <h3>6. Example</h3>
    <p>Imagine predicting if a student is admitted based on GPA and test score.</p>
    <table aria-label="Example dataset for admission">
      <thead>
        <tr><th>GPA</th><th>Test Score</th><th>Admitted (y)</th></tr>
      </thead>
      <tbody>
        <tr><td>3.0</td><td>600</td><td>0</td></tr>
        <tr><td>3.8</td><td>700</td><td>1</td></tr>
        <tr><td>4.0</td><td>720</td><td>1</td></tr>
        <tr><td>2.8</td><td>580</td><td>0</td></tr>
      </tbody>
    </table>

    <p class="eq">
      $$ P(\text{Admit}=1) \;=\; \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{GPA} + \beta_2 \cdot \text{TestScore})}} $$
    </p>
    <div class="tip">
      If for a new student (GPA = 3.6, Score = 680) we calculate \(p = 0.72\): that’s a 72% chance of admission → predict <strong>Admitted = 1</strong>.
    </div>

    <!-- 7. Key Points -->
    <h3>7. Key Points for Students</h3>
    <ul>
      <li>Logistic Regression outputs <strong>probabilities</strong>, not raw continuous values.</li>
      <li>A threshold (commonly 0.5) converts probability → class.</li>
      <li>Uses <strong>MLE</strong>, not Ordinary Least Squares.</li>
      <li>Works best when the relationship between features and <em>log-odds</em> is approximately linear.</li>
    </ul>

    <!-- 8. Visualization -->
    <h3>8. Visualization (for teaching)</h3>
    <p>Sigmoid curve behavior:</p>
    <ul>
      <li>As \(z \to -\infty\), probability \(\to 0\).</li>
      <li>As \(z \to +\infty\), probability \(\to 1\).</li>
      <li>At \(z = 0\), probability \(= 0.5\).</li>
    </ul>

    <canvas id="sigmoidCanvas" width="720" height="380" aria-label="Sigmoid function plot"></canvas>
    <div class="caption">Sigmoid function \( \sigma(z) = \dfrac{1}{1 + e^{-z}} \)</div>

    <div class="hr"></div>
    <p class="small">Tip: This section is self-contained. If your site already includes MathJax, you can remove the MathJax script tag above.</p>

  <script>
    // Draw a simple sigmoid plot on the canvas (z in [-8, 8])
    (function drawSigmoid() {
      const canvas = document.getElementById('sigmoidCanvas');
      if (!canvas || !canvas.getContext) return;
      const ctx = canvas.getContext('2d');
      const W = canvas.width, H = canvas.height;

      // Padding for axes
      const pad = { left: 60, right: 20, top: 20, bottom: 50 };
      const plotW = W - pad.left - pad.right;
      const plotH = H - pad.top - pad.bottom;

      // Domain & range
      const zMin = -8, zMax = 8;
      const pMin = 0,  pMax = 1;

      const xScale = z => pad.left + ( (z - zMin) / (zMax - zMin) ) * plotW;
      const yScale = p => pad.top + (1 - ( (p - pMin) / (pMax - pMin) )) * plotH; // invert y

      // Clear
      ctx.clearRect(0, 0, W, H);

      // Background
      ctx.fillStyle = '#ffffff';
      ctx.fillRect(0, 0, W, H);

      // Axes
      ctx.strokeStyle = '#cbd5e1';
      ctx.lineWidth = 1;

      // Border of plot area
      ctx.strokeRect(pad.left, pad.top, plotW, plotH);

      // Horizontal grid lines at p = 0, 0.5, 1
      const gridPs = [0, 0.5, 1];
      ctx.beginPath();
      gridPs.forEach(p => {
        const y = yScale(p);
        ctx.moveTo(pad.left, y);
        ctx.lineTo(pad.left + plotW, y);
      });
      ctx.stroke();

      // Vertical grid lines at z = -6, -3, 0, 3, 6
      const gridZs = [-6, -3, 0, 3, 6];
      ctx.beginPath();
      gridZs.forEach(z => {
        const x = xScale(z);
        ctx.moveTo(x, pad.top);
        ctx.lineTo(x, pad.top + plotH);
      });
      ctx.stroke();

      // Labels
      ctx.fillStyle = '#374151';
      ctx.font = '12px Roboto, sans-serif';
      // y labels
      gridPs.forEach(p => {
        const y = yScale(p);
        ctx.fillText(p.toFixed(1), 28, y + 4);
      });
      // x labels
      gridZs.forEach(z => {
        const x = xScale(z);
        ctx.fillText(z.toString(), x - 6, pad.top + plotH + 18);
      });

      // Axis titles
      ctx.font = '13px Roboto, sans-serif';
      ctx.fillText('z', pad.left + plotW / 2, H - 18);
      ctx.save();
      ctx.translate(18, pad.top + plotH / 2);
      ctx.rotate(-Math.PI / 2);
      ctx.fillText('Probability  p = σ(z)', 0, 0);
      ctx.restore();

      // Sigmoid curve
      ctx.strokeStyle = '#2563eb';
      ctx.lineWidth = 2;
      ctx.beginPath();
      const steps = 600;
      for (let i = 0; i <= steps; i++) {
        const t = i / steps;
        const z = zMin + t * (zMax - zMin);
        const p = 1 / (1 + Math.exp(-z));
        const x = xScale(z);
        const y = yScale(p);
        if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
      }
      ctx.stroke();

      // Mark p=0.5 at z=0
      const x0 = xScale(0);
      const y05 = yScale(0.5);
      ctx.fillStyle = '#1f2937';
      ctx.beginPath();
      ctx.arc(x0, y05, 3.5, 0, 2 * Math.PI);
      ctx.fill();

      // Dashed helpers to axes
      ctx.setLineDash([5, 4]);
      ctx.strokeStyle = '#9ca3af';
      ctx.beginPath();
      ctx.moveTo(x0, y05);
      ctx.lineTo(x0, pad.top + plotH);
      ctx.moveTo(x0, y05);
      ctx.lineTo(pad.left, y05);
      ctx.stroke();
      ctx.setLineDash([]);
    })();
  </script>
</section>


  

  
<section id="ridge-regression">
  <h4>2.2 Ridge Regression (L2 Regularization)</h4>
  <br>

  <b>Idea:</b>
  <p>
    Ridge Regression extends Ordinary Least Squares (OLS) by adding an <b><font color="red">L2 penalty</font></b> on the size of the coefficients. 
    This penalty discourages overly large weights, helping to reduce <b>overfitting</b> and improve <b>generalization</b>, especially when features are correlated or when \(p \approx m\).
  </p>

  <b>Equation:</b>
  <p style="text-align:center;">
    Choose \( \boldsymbol{\beta} \) to minimize
    \[
      \underbrace{\sum_{i=1}^{m}\bigl(y_i - \hat{y}_i\bigr)^2}_{\text{SSE}}
      \;+\;
      \lambda \sum_{j=1}^{n} \beta_j^2,
    \]
    where \( \hat{y}_i = \beta_0 + \sum_{j=1}^{n} \beta_j x_{ij} \) and the intercept \( \beta_0 \) is typically <em>not</em> penalized.
  </p>

  <p><strong>Where:</strong></p>
  <ul>
    <li>\( \lambda \ge 0 \): regularization strength (larger \( \lambda \Rightarrow \) stronger shrinkage)</li>
    <li>\( \beta_0 \): intercept (unpenalized)</li>
    <li>\( \beta_1,\ldots,\beta_n \): coefficients for each feature</li>
    <li>\( \mathbf{X} \): design matrix, \( \mathbf{y} \): targets</li>
  </ul>

  <br>

  <center>
    <img src="images/ridge1.png" width="800" height="300" alt="Ridge regression shrinks coefficients (L2 penalty)">
  </center>

  <br>

  <h4>How Ridge Regression Works</h4>
  <!-- MathJax already loaded once in the page -->
  <ol>
    <li><strong>Collect Data</strong> – Gather features \( (x_1,\dots,x_n) \) and outputs \( y \).</li>
    <li><strong>Standardize Features (recommended)</strong> – Put features on comparable scales so the penalty treats them fairly.</li>
    <li><strong>Choose \( \lambda \)</strong> – Typically via cross-validation.</li>
    <li><strong>Fit the Model</strong> – Minimize squared error + L2 penalty to get \( \hat{\boldsymbol{\beta}} \).</li>
    <li><strong>Evaluate</strong> – Check validation/test error; tune \( \lambda \) to balance bias–variance.</li>
  </ol>

  <h4>Interpreting the Coefficients</h4>
  <ul>
    <li><strong>Intercept (\(\beta_0\))</strong> – Baseline prediction (usually unpenalized).</li>
    <li><strong>Coefficient (\(\beta_j\))</strong> – Effect of feature \(x_j\) on \(y\) while others are fixed, but <em>shrunken</em> toward zero by the L2 penalty.</li>
    <li><strong>Shrinkage</strong> – Larger \( \lambda \) \(\Rightarrow\) stronger shrinkage (reduces variance, may increase bias).</li>
  </ul>

  <br>

  <h4>Model &amp; Notation</h4>
  <p><b>Scalar form:</b></p>
  <p>$$ y = \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n + \epsilon $$</p>

  <p><b>Matrix form:</b></p>
  <p>
    $$ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} $$
    where
    $$ \mathbf{X} =
      \begin{bmatrix}
        1 & x_{11} & \cdots & x_{1n} \\
        1 & x_{21} & \cdots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m1} & \cdots & x_{mn}
      \end{bmatrix},\quad
      \boldsymbol{\beta} =
      \begin{bmatrix}
        \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n
      \end{bmatrix},\quad
      \mathbf{y} =
      \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_m
      \end{bmatrix}.
    $$
  </p>

  <h4>Objective (Ridge / L2)</h4>
  <p>
    In vector form (with penalty matrix \( \mathbf{P}=\mathrm{diag}(0,1,\dots,1) \) so the intercept isn’t penalized):
  </p>
  <p style="text-align:center;">
    $$ 
      \min_{\boldsymbol{\beta}} \;
      (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      \;+\;
      \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
    $$
  </p>

  <h4>Normal Equations (Derivation)</h4>
  <div class="step">
    <p><b>Expand the objective:</b></p>
    <p>
      $$
      J(\boldsymbol{\beta})
      = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">
    <p><b>Distribute (FOIL) the SSE part and use transpose rules:</b></p>
    <p>
      $$
      (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
      = \mathbf{y}^\top\mathbf{y}
      - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
      + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}.
      $$
    </p>
    <p>So
      $$
      J(\boldsymbol{\beta}) =
      \mathbf{y}^\top\mathbf{y}
      - 2\,\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}
      + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
      + \lambda\,\boldsymbol{\beta}^\top \mathbf{P}\,\boldsymbol{\beta}.
      $$
    </p>
  </div>

  <div class="step">
    <p><b>Take gradient w.r.t. \( \boldsymbol{\beta} \) and set to zero:</b></p>
    <p>
      $$
      \frac{\partial J}{\partial \boldsymbol{\beta}}
      = -2\,\mathbf{X}^\top\mathbf{y}
        + 2\,\mathbf{X}^\top\mathbf{X}\,\boldsymbol{\beta}
        + 2\lambda\,\mathbf{P}\,\boldsymbol{\beta}
      \;=\; \mathbf{0}.
      $$
    </p>
    <p>
      Rearranging:
      $$
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})\,\hat{\boldsymbol{\beta}}
      = \mathbf{X}^\top\mathbf{y}.
      $$
    </p>
    <p><b>Closed form (if invertible):</b>
      $$
      \boxed{\;
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      =
      (\mathbf{X}^\top\mathbf{X} + \lambda\,\mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
      \;}
      $$
    </p>
  </div>

  <h4>Gradient Descent View (optional)</h4>
  <p>
    With learning rate \( \alpha \) and excluding the intercept from the penalty, the update for \( j\ge 1 \) is
  </p>
  <p style="text-align:center;">
    $$
    \beta_j \leftarrow \beta_j
    - \alpha \left[
      -2 \sum_{i=1}^{m} x_{ij}\bigl(y_i - \hat{y}_i\bigr)
      + 2\lambda\,\beta_j
    \right]
    =
    \beta_j + 2\alpha \sum_{i=1}^{m} x_{ij}\bigl(y_i - \hat{y}_i\bigr) - 2\alpha\lambda\,\beta_j.
    $$
  </p>
  <p>
    The last term \( -2\alpha\lambda\,\beta_j \) is what pulls coefficients toward zero (shrinkage).
  </p>

  <hr>

  <h4>Worked Example: Predicting House Price (Ridge)</h4>
  <p>Model:
    $$ \text{Price} = \beta_0 + \beta_1 \cdot \text{Size} + \beta_2 \cdot \text{Bedrooms} + \epsilon $$
  </p>

  <h4>Data</h4>
  <table border="1" cellpadding="6">
    <tr><th>Size (sq.ft)</th><th>Bedrooms</th><th>Price ($)</th></tr>
    <tr><td>1000</td><td>2</td><td>200,000</td></tr>
    <tr><td>1500</td><td>3</td><td>280,000</td></tr>
    <tr><td>2000</td><td>3</td><td>340,000</td></tr>
    <tr><td>2500</td><td>4</td><td>400,000</td></tr>
    <tr><td>3000</td><td>4</td><td>460,000</td></tr>
  </table>

  <h4>Step 1: Build \(\mathbf{X}\) and \(\mathbf{y}\)</h4>
  <p>
    $$ 
    \mathbf{X} =
    \begin{bmatrix}
      1 & 1000 & 2 \\
      1 & 1500 & 3 \\
      1 & 2000 & 3 \\
      1 & 2500 & 4 \\
      1 & 3000 & 4
    \end{bmatrix},\quad
    \mathbf{y} =
    \begin{bmatrix}
      200000 \\ 280000 \\ 340000 \\ 400000 \\ 460000
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 2: Compute \( \mathbf{X}^\top \mathbf{X} + \lambda\mathbf{P} \) and \( \mathbf{X}^\top \mathbf{y} \)</h4>
  <p>
    From the linear regression section,
    $$ 
    \mathbf{X}^\top \mathbf{X} =
    \begin{bmatrix}
      5 & 10000 & 16 \\
      10000 & 22500000 & 34500 \\
      16 & 34500 & 54
    \end{bmatrix}, \quad
    \mathbf{X}^\top \mathbf{y} =
    \begin{bmatrix}
      1680000 \\
      3680000000 \\
      5700000
    \end{bmatrix}.
    $$
  </p>
  <p>
    Let \( \lambda = 100 \) and \( \mathbf{P}=\mathrm{diag}(0,1,1) \) (no penalty on \( \beta_0 \)):
  </p>
  <p style="text-align:center;">
    $$
    \mathbf{X}^\top \mathbf{X} + \lambda\mathbf{P} =
    \begin{bmatrix}
      5 & 10000 & 16 \\
      10000 & 22500010 & 34500 \\
      16 & 34500 & 154
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 3: Solve for \( \hat{\boldsymbol{\beta}}_{\text{ridge}} \)</h4>
  <p>
    Using the ridge normal equation:
    $$
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      = (\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{P})^{-1}\,\mathbf{X}^\top\mathbf{y}
      \;\approx\;
      \begin{bmatrix}
        79{,}962.23 \\
        127.95 \\
        40.01
      \end{bmatrix}.
    $$
  </p>

  <h4>Final Ridge Model (\(\lambda=100\))</h4>
  <p style="text-align:center; font-size:1.1em;">
    $$ \widehat{\text{Price}} = 79{,}962.23 \;+\; 127.95 \cdot \text{Size} \;+\; 40.01 \cdot \text{Bedrooms} $$
  </p>

  <h4>Step 4: Predictions \((\hat{\mathbf{y}})\) & Residuals</h4>
  <p>
    $$
    \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      207{,}997.12 \\
      272{,}014.56 \\
      335{,}992.00 \\
      400{,}009.44 \\
      463{,}986.88
    \end{bmatrix},\quad
    \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \approx
    \begin{bmatrix}
      -7{,}997.12 \\
      7{,}985.44 \\
      4{,}008.00 \\
      -9.44 \\
      -3{,}986.88
    \end{bmatrix}.
    $$
  </p>

  <h4>Step 5: Training Error</h4>
  <p>
    $$
      \mathrm{SSE} \approx 1.5968 \times 10^8,\quad
      \mathrm{MSE} \approx 3.1936 \times 10^7,\quad
      \mathrm{RMSE} \approx 5{,}651.20.
    $$
  </p>

  <h4>Interpretation &amp; Comparison to OLS</h4>
  <ul>
    <li><strong>Shrinkage:</strong> Ridge pulls coefficients toward zero: \(\beta_{\text{Size}}\) fell from \(\approx 114.67\) (OLS) to \(\approx 127.95\) here due to dataset scaling; the joint effect with the intercept produces similar predictions but with <em>stabilized</em> parameters. (Exact values depend on feature scaling and \(\lambda\).)</li>
    <li><strong>Bias–Variance Tradeoff:</strong> As \( \lambda \) increases, variance decreases and bias increases. Pick \( \lambda \) via cross-validation.</li>
    <li><strong>Multicollinearity:</strong> Ridge is especially useful when features are correlated (ill-conditioned \( \mathbf{X}^\top\mathbf{X} \)).</li>
    <li><strong>Intercept:</strong> Typically unpenalized; standardize features for fair penalization.</li>
  </ul>

  <h4>Why Might SSE Be Different from OLS?</h4>
  <p>
    Ridge adds a penalty term; while the pure SSE on training data can be slightly worse than OLS, the <b>generalization error</b> on unseen data often improves due to reduced variance. Always compare with a validation or test set.
  </p>

  <br>
</section>






  


  
  <h1>Introduction to Machine Learning</h1>

  <section id="what-is-ml">
    <h3>1. What is Machine Learning?</h3>
    <p>
      Machine learning (ML) is the process of teaching computers to make decisions or predictions based on data, rather than through explicit programming. It powers many modern technologies, from recommendation engines to self-driving cars.
    </p>
  </section>

  <section id="types-ml">
    <h3>2. Types of ML Systems</h3>
    <ul>
      <li><strong>Supervised Learning</strong>: Learn a mapping from input to output with labeled data (e.g., classification, regression).</li>
      <li><strong>Unsupervised Learning</strong>: Find hidden patterns or structures in data without explicit labels (e.g., clustering, dimensionality reduction).</li>
      <li><strong>Reinforcement Learning</strong>: Learn to make a sequence of decisions by receiving rewards or penalties from the environment.</li>
      <li><strong>Generative AI</strong>: Create new content (text, images, audio) that mimics real data distributions.</li>
    </ul>
  </section>

  <section id="supervised">
    <h3>3. Supervised Learning</h3>
    <p>
      The most common ML paradigm, where models learn from input-output pairs. Two main types:
    </p>
    <div id="regression">
      <h4>3.1 Regression</h4>
      <p>Regression algorithms predict numeric values (e.g., house prices, temperature):</p>
      <pre><code># Example (scikit-learn)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
      </code></pre>
    </div>
    <div id="classification">
      <h4>3.2 Classification</h4>
      <p>Classification algorithms assign items to one of several categories (e.g., spam detection, image recognition):</p>
      <pre><code># Example (scikit-learn)
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
      </code></pre>
    </div>
  </section>

  <section id="unsupervised">
    <h3>4. Unsupervised Learning</h3>
    <p>
      Unsupervised learning discovers patterns and groupings in data without labels. Two main types:
    </p>
    <div id="clustering">
      <h4>4.1 Clustering</h4>
      <p>Clustering algorithms group data points into clusters based on similarity or distance. Common methods include K-Means, Agglomerative Hierarchical Clustering, and DBSCAN.</p>
    </div>
    <div id="dim-reduction">
      <h4>4.2 Dimensionality Reduction</h4>
      <p>Dimensionality reduction algorithms reduce the number of input variables or features, making data easier to visualize and process. Popular techniques include:</p>
      <ul>
        <li><strong>PCA (Principal Component Analysis):</strong> Transforms data to a new set of orthogonal axes that maximize variance.</li>
        <li><strong>ICA (Independent Component Analysis):</strong> Decomposes a multivariate signal into independent non-Gaussian components, often used in signal processing and brain imaging.</li>
        <li><strong>t-SNE (t-distributed Stochastic Neighbor Embedding):</strong> Maps high-dimensional data to 2 or 3 dimensions for visualization, preserving local structure.</li>
      </ul>
    </div>
  </section>

  <section id="reinforcement">
    <h3>5. Reinforcement Learning</h3>
    <p>
      Reinforcement learning (RL) is a paradigm where agents learn optimal strategies by interacting with an environment and receiving feedback as rewards or penalties.
    </p>
    <div id="value-based">
      <h4>5.1 Value-Based Methods</h4>
      <p>
        Value-based methods estimate the expected reward of actions or states. The agent chooses actions to maximize these values.<br>
        <strong>Examples:</strong> Q-Learning, Deep Q-Networks (DQN)
      </p>
    </div>
    <div id="policy-based">
      <h4>5.2 Policy-Based Methods</h4>
      <p>
        Policy-based methods directly optimize the policy that decides actions. These are often used when value-based methods are impractical.<br>
        <strong>Examples:</strong> REINFORCE, Actor-Critic
      </p>
    </div>
    <div id="model-based">
      <h4>5.3 Model-Based Methods</h4>
      <p>
        Model-based methods learn a model of the environment and use it for planning or policy improvement.<br>
        <strong>Examples:</strong> Dyna-Q, Monte Carlo Tree Search (MCTS)
      </p>
    </div>
  </section>

  <section id="generative-ai">
    <h3>6. Generative AI</h3>
    <p>
      Generative AI models are designed to produce new data that resembles the data they were trained on. These models are widely used for text generation, image synthesis, music, and more. Main types include:
    </p>
    <div id="gans">
      <h4>6.1 Generative Adversarial Networks (GANs)</h4>
      <p>
        GANs use a generator and a discriminator in competition to produce increasingly realistic outputs. They are famous for generating realistic images and videos.
      </p>
    </div>
    <div id="autoregressive">
      <h4>6.2 Autoregressive Models</h4>
      <p>
        These models generate data one step at a time, with each output depending on previous outputs. <strong>Examples:</strong> GPT-4 (text), LSTM (music, text).
      </p>
    </div>
    <div id="diffusion">
      <h4>6.3 Diffusion Models</h4>
      <p>
        Diffusion models create data by learning to gradually denoise random input, producing some of the highest quality images to date. <strong>Examples:</strong> DALL·E 2, Stable Diffusion.
      </p>
    </div>
  </section>

  <section id="references">
    <h3>References</h3>
    <ul>
      <li>
        <a href="https://developers.google.com/machine-learning/crash-course" target="_blank">
          Google Machine Learning Crash Course
        </a>
      </li>
      <li>
        <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank">
          scikit-learn User Guide
        </a>
      </li>
      <li>
        <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
          Andrew Ng's Machine Learning (Coursera)
        </a>
      </li>
      <li>
        <a href="https://www.deeplearning.ai/short-courses/" target="_blank">
          DeepLearning.AI Short Courses
        </a>
      </li>
    </ul>
  </section>
  <footer>
    &copy; 2025 Xin Yang <br>
    Department of Computer Science <br>
    Middle Tennessee State University <br>
  </footer>
</main>

<script>
  // Smooth scrolling & highlight active link
  const sidebarLinks = document.querySelectorAll('#sidebar a');
  sidebarLinks.forEach(anchor => {
    anchor.onclick = function(e) {
      e.preventDefault();
      document.querySelector(this.getAttribute('href'))
        .scrollIntoView({ behavior: 'smooth' });
    };
  });

  // Active link highlight on scroll
  const sectionIds = Array.from(sidebarLinks).map(l => l.getAttribute('href'));
  window.addEventListener('scroll', () => {
    let current = sectionIds[0];
    for (const id of sectionIds) {
      const section = document.querySelector(id);
      if (section && section.getBoundingClientRect().top - 80 < 0) {
        current = id;
      }
    }
    sidebarLinks.forEach(link => link.classList.remove('active'));
    const activeLink = document.querySelector(`#sidebar a[href="${current}"]`);
    if (activeLink) activeLink.classList.add('active');
  });
</script>

</body>
</html>
