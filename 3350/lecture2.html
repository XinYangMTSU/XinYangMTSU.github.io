<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>OLS with Correlated Features vs Ridge</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #17191a;
      --panel: #222025;
      --ink: #ffb3de;
      --accent: #ff79c6;
      --shadow: rgba(255,121,198,.12);
      --muted: #c7b3c8;
    }
    body {
      font-family: 'Roboto', Arial, sans-serif;
      margin: 0;
      background: var(--bg);
      color: var(--ink);
      padding: 24px;
    }
    section {
      margin: 28px auto;
      background: var(--panel);
      border-radius: 18px;
      border-left: 7px solid var(--accent);
      box-shadow: 0 6px 28px var(--shadow);
      max-width: 940px;
      padding: 28px 30px;
    }
    h1, h2, h3 {
      color: var(--ink);
      margin: 0 0 10px 0;
    }
    h1 { font-size: 2.0rem; color: var(--accent); text-shadow: 0 2px 16px rgba(255,121,198,.18); }
    h2 { font-size: 1.35rem; margin-top: 18px; }
    p { line-height: 1.6; }
    .callout {
      border: 1px solid var(--accent);
      border-radius: 12px;
      padding: 14px 16px;
      background: #20161e;
      margin: 16px 0;
    }
    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 16px;
    }
    .chip {
      display: inline-block;
      padding: 6px 10px;
      font-size: .92rem;
      border: 1px solid var(--ink);
      border-radius: 999px;
      margin: 2px 6px 2px 0;
      opacity: .9;
    }
    code.inline { background:#19121a; padding:2px 6px; border-radius:6px; }
    .equation { text-align:center; margin: 12px 0; }
    .note { color: var(--muted); font-size: .96rem; }
    .legend { font-size: .92rem; color: var(--muted); }
    .kbd { background:#19121a; padding:2px 6px; border-radius:6px; font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace; }
    .list { margin-left: 1.1rem; }
    .svgwrap { background:#1e1f22; border-radius:12px; padding:10px; box-shadow:0 4px 20px var(--shadow); }
    a, a:visited { color: var(--accent); }
    .assumption {
      background: #19121a;
      border: 2px dashed var(--accent);
      padding: 16px;
      border-radius: 12px;
      margin: 20px 0;
      font-style: italic;
      color: var(--muted);
    }
  </style>
</head>
  
<body>

<section>
  <h1>Multicollinearity Problem: Correlated Features in OLS</h1>
  <p class="note">
    When features are highly correlated, OLS can produce wildly different coefficient values that yield nearly identical predictions, making the model unstable and hard to interpret.
  </p>
</section>

<section>
  <h2>The Multicollinearity Scenario</h2>
  <p>Suppose we have two features describing the same tumor size concept:</p>
  <ul class="list">
    <li>\(x_1 =\) size of tumor (cm)</li>
    <li>\(x_2 =\) diameter of tumor (cm)</li>
  </ul>
  <p>They're almost the same measurement ⇒ <b>highly correlated</b>.</p>

  <div class="equation">
    \[
      y \;=\; \beta_0 + \beta_1 x_1 + \beta_2 x_2
    \]
  </div>

  <div class="assumption">
    <b>Key Assumption:</b> Since \(x_1 \approx x_2\) (highly correlated), we can substitute \(x_2 \approx x_1\) to see how different coefficient combinations affect predictions.
  </div>

  <p>Because \(x_1 \approx x_2\), there are many coefficient pairs \((\beta_1, \beta_2)\) that yield nearly the same predictions:</p>
  <div class="grid">
    <div class="callout">
      <b>Solution A:</b> \(\beta_1 = 100,\; \beta_2 = -98\)<br/>
      <b>Prediction:</b> \(100x_1 - 98x_2 \approx 100x_1 - 98x_1 = 2x_1\)<br/>
      <span class="legend">Large, opposite signs; sensitive to tiny data changes.</span>
    </div>
    <div class="callout">
      <b>Solution B:</b> \(\beta_1 = 2,\; \beta_2 = 0\)<br/>
      <b>Prediction:</b> \(2x_1 + 0x_2 = 2x_1\)<br/>
      <span class="legend">Pushes weight onto \(x_1\), ignores \(x_2\) completely.</span>
    </div>
    <div class="callout">
      <b>Solution C:</b> \(\beta_1 = 1,\; \beta_2 = 1\)<br/>
      <b>Prediction:</b> \(1x_1 + 1x_2 \approx x_1 + x_1 = 2x_1\)<br/>
      <span class="legend">Distributes weight equally between correlated features.</span>
    </div>
  </div>

  <p><b>Key issue:</b> All three solutions produce nearly identical predictions (\(\approx 2x_1\)), but have drastically different coefficient values! OLS "doesn't care" which combination it picks, so coefficients can swing wildly with tiny changes in the data — this hurts stability and test performance.</p>
</section>

<section>
  <h2>Why This Matters</h2>
  <div class="grid">
    <div class="callout">
      <b>❌ Unstable Coefficients</b><br/>
      Small changes in training data can cause coefficients to jump between these equivalent solutions.
    </div>
    <div class="callout">
      <b>❌ Poor Interpretability</b><br/>
      Coefficients don't reflect true feature importance when they can be arbitrarily large or small.
    </div>
    <div class="callout">
      <b>❌ Reduced Generalization</b><br/>
      Extreme coefficient values often don't generalize well to new, unseen data.
    </div>
  </div>
</section>

<section>
  <h2>Ridge Regression: The Solution</h2>
  <p>Ridge adds a penalty for large coefficients, naturally preferring more balanced solutions:</p>

  <div class="equation">
    \[
      \hat{\boldsymbol{\beta}}_{\text{ridge}}
      \;=\;
      \arg\min_{\boldsymbol{\beta}}
      \Big(
        \|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2
        \;+\;
        \lambda \|\boldsymbol{\beta}\|_2^2
      \Big)
    \]
  </div>

  <div class="callout">
    <b>How Ridge Helps:</b> Among our three solutions, Ridge would heavily penalize Solution A (\(\beta_1^2 + \beta_2^2 = 100^2 + (-98)^2 = 19,604\)) compared to Solution C (\(\beta_1^2 + \beta_2^2 = 1^2 + 1^2 = 2\)). This naturally pushes toward more balanced, stable coefficient combinations.
  </div>

  <p><b>Effect with correlated features:</b> Ridge tends to <i>spread weight</i> across correlated features, shrinking coefficients toward smaller, more stable values instead of letting one blow up while the other cancels it out.</p>
</section>

<section>
  <h2>Intuition & Takeaways</h2>
  <div class="grid">
    <div class="callout">
      <b>OLS intuition</b><br/>
      "Any coefficient combination works if predictions match." ⇒ Coefficients can be extreme and unstable with correlated features.
    </div>
    <div class="callout">
      <b>Ridge intuition</b><br/>
      "Keep coefficients small and balanced." ⇒ Shares weight across correlated features, yielding stable, generalizable models.
    </div>
  </div>
  <p>
    <span class="chip">Multicollinearity</span>
    <span class="chip">Coefficient stability</span>
    <span class="chip">Generalization</span>
    <span class="chip">Bias–Variance Tradeoff</span>
  </p>
</section>

<!-- MathJax (load once per page) -->
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
